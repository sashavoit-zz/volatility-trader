{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1c04b9a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c04b9a8",
    "outputId": "61e276fe-2388-4914-fd4d-2a1fe0e3509c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tsai in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: fastai>=2.5.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (2.5.6)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (21.3)\n",
      "Requirement already satisfied: imbalanced-learn>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (0.9.0)\n",
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (22.0.3)\n",
      "Requirement already satisfied: psutil>=5.4.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (5.9.0)\n",
      "Requirement already satisfied: torch<1.11,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (1.10.2)\n",
      "Requirement already satisfied: pyts>=0.12.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (0.12.0)\n",
      "Requirement already satisfied: nbformat>=5.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tsai) (5.1.3)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (1.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (2.25.1)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (0.11.3)\n",
      "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (3.4.2)\n",
      "Requirement already satisfied: pillow>6.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (8.2.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (1.8.0)\n",
      "Requirement already satisfied: spacy<4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (3.2.4)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (6.0)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (1.0.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (0.0.5)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fastai>=2.5.3->tsai) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from imbalanced-learn>=0.8.0->tsai) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from imbalanced-learn>=0.8.0->tsai) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from imbalanced-learn>=0.8.0->tsai) (3.1.0)\n",
      "Requirement already satisfied: jupyter-core in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nbformat>=5.1.3->tsai) (4.9.2)\n",
      "Requirement already satisfied: ipython-genutils in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nbformat>=5.1.3->tsai) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nbformat>=5.1.3->tsai) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nbformat>=5.1.3->tsai) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.48.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyts>=0.12.0->tsai) (0.55.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch<1.11,>=1.7.0->tsai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging->tsai) (2.4.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=5.1.3->tsai) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=5.1.3->tsai) (0.18.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from numba>=0.48.0->pyts>=0.12.0->tsai) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from numba>=0.48.0->pyts>=0.12.0->tsai) (49.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (0.7.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (2.4.3)\n",
      "Requirement already satisfied: click<8.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (8.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (0.9.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (8.0.15)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<4->fastai>=2.5.3->tsai) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->fastai>=2.5.3->tsai) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->fastai>=2.5.3->tsai) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->fastai>=2.5.3->tsai) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->fastai>=2.5.3->tsai) (4.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->fastai>=2.5.3->tsai) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->fastai>=2.5.3->tsai) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->fastai>=2.5.3->tsai) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->fastai>=2.5.3->tsai) (2021.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->fastai>=2.5.3->tsai) (1.16.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<4->fastai>=2.5.3->tsai) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2->spacy<4->fastai>=2.5.3->tsai) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tsai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78057e88",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d6b375a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.713467</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>2.098651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.719555</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>2.068303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.718412</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>2.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.720527</td>\n",
       "      <td>0.095456</td>\n",
       "      <td>1.834030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.719023</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>1.833023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PRICE      SIZE       vol\n",
       "0 -1.713467  0.000773  2.098651\n",
       "1 -1.719555  0.036915  2.068303\n",
       "2 -1.718412  0.048620  2.006872\n",
       "3 -1.720527  0.095456  1.834030\n",
       "4 -1.719023  0.023657  1.833023"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"5sec_intervals_no_na_normed.csv\")\n",
    "df = df[['PRICE', 'SIZE', 'vol']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "855dc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[:round(0.8 * len(df))].copy()\n",
    "test_data = df[train_data.iloc[-1].name+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82053910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.713467</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>2.098651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.719555</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>2.068303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.718412</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>2.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.720527</td>\n",
       "      <td>0.095456</td>\n",
       "      <td>1.834030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.719023</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>1.833023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-1.714687</td>\n",
       "      <td>-0.006404</td>\n",
       "      <td>1.818437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-1.718210</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>1.817190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-1.716480</td>\n",
       "      <td>0.062125</td>\n",
       "      <td>1.818805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-1.717838</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>1.809160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-1.715703</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>1.787224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRICE      SIZE       vol\n",
       "0   -1.713467  0.000773  2.098651\n",
       "1   -1.719555  0.036915  2.068303\n",
       "2   -1.718412  0.048620  2.006872\n",
       "3   -1.720527  0.095456  1.834030\n",
       "4   -1.719023  0.023657  1.833023\n",
       "..        ...       ...       ...\n",
       "115 -1.714687 -0.006404  1.818437\n",
       "116 -1.718210  0.009351  1.817190\n",
       "117 -1.716480  0.062125  1.818805\n",
       "118 -1.717838 -0.020977  1.809160\n",
       "119 -1.715703  0.000706  1.787224\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4b8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_normalized = torch.FloatTensor(train_data.to_numpy().tolist())\n",
    "test_data_normalized = torch.FloatTensor(test_data.to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aba82b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6298, -0.6214, -0.6266,  ...,  0.5650,  0.5892,  0.6046])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_normalized[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "78f6ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(input_data, tw, sl):\n",
    "    \"\"\"\n",
    "    tw: train_window\n",
    "    sl: sequence_length\n",
    "    \"\"\"\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw-sl):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        # Select the volatility column\n",
    "        train_label = input_data[i+tw:i+tw+sl, 2]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "train_window = 100\n",
    "pred_seq_len = 20\n",
    "\n",
    "# Prepare sequences\n",
    "train_sequence = create_inout_sequences(train_data.to_numpy(), train_window, pred_seq_len)\n",
    "test_sequence = create_inout_sequences(test_data.to_numpy(), train_window, pred_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcfe9f",
   "metadata": {},
   "source": [
    "# Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b783be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generator that yields batches of data\n",
    "\n",
    "    Args:\n",
    "      x: input values\n",
    "      y: output values\n",
    "      batch_size: size of each batch\n",
    "    Yields:\n",
    "      batch_x: a batch of inputs of size at most batch_size\n",
    "      batch_y: a batch of outputs of size at most batch_size\n",
    "    \"\"\"\n",
    "    N = np.shape(x)[0]\n",
    "    assert N == np.shape(y)[0]\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_x = np.array(x[i : i + batch_size])\n",
    "        batch_y = np.array(y[i : i + batch_size])\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d5a895b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [t[0] for t in train_sequence]\n",
    "train_y = [t[1] for t in train_sequence]\n",
    "\n",
    "batches = get_batch(train_x, train_y, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "22aaf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_vars(xs, ys, gpu=False):\n",
    "    \"\"\"\n",
    "    Helper function to convert numpy arrays to pytorch tensors.\n",
    "    If GPU is used, move the tensors to GPU.\n",
    "\n",
    "    Args:\n",
    "      xs (float numpy tenosor): greyscale input\n",
    "      ys (int numpy tenosor): categorical labels\n",
    "      gpu (bool): whether to move pytorch tensor to GPU\n",
    "    Returns:\n",
    "      Variable(xs), Variable(ys)\n",
    "    \"\"\"\n",
    "    xs = torch.from_numpy(xs).float()\n",
    "    ys = torch.from_numpy(ys).float()\n",
    "    if gpu:\n",
    "        xs = xs.cuda()\n",
    "        ys = ys.cuda()\n",
    "    return Variable(xs), Variable(ys)\n",
    "\n",
    "\n",
    "def run_validation_step(\n",
    "    model,\n",
    "    criterion,\n",
    "    inputs,\n",
    "    labels,\n",
    "    batch_size,\n",
    "    gpu,\n",
    "    plotpath=None,\n",
    "    visualize=True,\n",
    "    downsize_input=False\n",
    "):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    losses = []\n",
    "    num_colours = np.shape(colours)[0]\n",
    "    for i, (xs, ys) in enumerate(get_batch(inputs, labels, batch_size)):\n",
    "        inputs, labels = get_torch_vars(xs, ys, gpu)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        losses.append(val_loss.data.item())\n",
    "\n",
    "        # _, predicted = torch.max(outputs.data, 1, keepdim=True)\n",
    "        total += labels.size(0)\n",
    "        # It is correct if predicted within a certain std\n",
    "        std = np.std(labels.data)\n",
    "        correct += (outputs == labels.data).sum()\n",
    "\n",
    "#     if plotpath:  # only plot if a path is provided\n",
    "#         plot(\n",
    "#             xs,\n",
    "#             ys,\n",
    "#             predicted.cpu().numpy(),\n",
    "#             colours,\n",
    "#             plotpath,\n",
    "#             visualize=visualize,\n",
    "#             compare_bilinear=downsize_input,\n",
    "#         )\n",
    "\n",
    "    val_loss = np.mean(losses)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7475e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "        \n",
    "def train(args, model=None):\n",
    "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
    "    # TODO: necessary?\n",
    "    torch.set_num_threads(5)\n",
    "    # Numpy random seed\n",
    "    npr.seed(args.seed)\n",
    "\n",
    "    # Save directory\n",
    "    save_dir = \"outputs/\" + args.experiment_name\n",
    "\n",
    "    # LOAD THE MODEL\n",
    "    if model is None:\n",
    "        # Net = globals()[args.model]\n",
    "        model = VolForecast(input_size=args.input_size, output_size=args.output_size)\n",
    "\n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learn_rate)\n",
    "\n",
    "    # DATA\n",
    "    print(\"Loading data...\")\n",
    "    train_sequence = create_inout_sequences(train_data.to_numpy(), args.sequence_len, args.output_size)\n",
    "    test_sequence = create_inout_sequences(test_data.to_numpy(), args.sequence_len, args.output_size)\n",
    "    train_x, train_y = [t[0] for t in train_sequence], [t[1] for t in train_sequence]\n",
    "    test_x, test_y = [t[0] for t in test_sequence], [t[1] for t in test_sequence]\n",
    "\n",
    "    # Create the outputs folder if not created already\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Beginning training ...\")\n",
    "    if args.gpu:\n",
    "        model.cuda()\n",
    "    start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train the Model\n",
    "        model.train()  # Change model to 'train' mode\n",
    "        losses = []\n",
    "        for i, (xs, ys) in enumerate(get_batch(train_x, train_y, args.batch_size)):\n",
    "            inputs, labels = get_torch_vars(xs, ys, args.gpu)\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.transpose(1,2)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "            \n",
    "            print(loss.data.item())\n",
    "\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "        val_loss, val_acc = run_validation_step(\n",
    "            model,\n",
    "            criterion,\n",
    "            test_x,\n",
    "            test_y,\n",
    "            args.batch_size,\n",
    "            args,gpu,\n",
    "            save_dir + \"/test_%d.png\" % epoch,\n",
    "            args.visualize,\n",
    "            args.downsize_input,\n",
    "        )\n",
    "\n",
    "        time_elapsed = time.time() - start\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_accs.append(val_acc)\n",
    "        print(\n",
    "            \"Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f\"\n",
    "            % (epoch + 1, args.epochs, val_loss, val_acc, time_elapsed)\n",
    "        )\n",
    "\n",
    "    # Plot training curve\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
    "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.savefig(save_dir + \"/training_curve.png\")\n",
    "\n",
    "    if args.checkpoint:\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), args.checkpoint)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c62e4",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5e238f44",
   "metadata": {
    "id": "5e238f44"
   },
   "outputs": [],
   "source": [
    "# TCN IMPLEMENTATION\n",
    "\n",
    "# Bai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.\n",
    "# Official TCN PyTorch implementation: https://github.com/locuslab/TCN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8988bb29",
   "metadata": {
    "id": "8988bb29"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
    "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "pbl1joAlY2sj",
   "metadata": {
    "id": "pbl1joAlY2sj"
   },
   "outputs": [],
   "source": [
    "# Model that uses TCN\n",
    "\n",
    "class VolForecast(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(VolForecast, self).__init__()\n",
    "        self.input_size = input_size  # Input size is same as sequence_len\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.tcn = TCN(input_size=input_size, output_size=25, num_channels=6*[25], kernel_size=7, dropout=0.1)\n",
    "        self.l1 = nn.Linear(25, 25)\n",
    "        self.drp1 = nn.Dropout(p=0.1)\n",
    "        self.l2 = nn.Linear(25, 25)\n",
    "        self.drp2 = nn.Dropout(p=0.1)\n",
    "        self.l3 = nn.Linear(25, output_size)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        tcn_o = F.relu(self.tcn(x))\n",
    "        l1_o = F.relu(self.l1(tcn_o))\n",
    "        drp1_o = self.drp1(l1_o)\n",
    "        l2_o = F.relu(self.l2(drp1_o))\n",
    "        drp2_o = self.drp2(l2_o)        \n",
    "        l3_o = self.l3(drp2_o)\n",
    "        return l3_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ccd72be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Beginning training ...\n",
      "2.068053722381592\n",
      "0.5693691968917847\n",
      "0.515004575252533\n",
      "0.3864291310310364\n",
      "0.22140195965766907\n",
      "0.44159063696861267\n",
      "0.3759973347187042\n",
      "0.06272967904806137\n",
      "0.03644510358572006\n",
      "0.06424359232187271\n",
      "0.06791005283594131\n",
      "0.5102248191833496\n",
      "0.6651337742805481\n",
      "0.6409245729446411\n",
      "0.9382301568984985\n",
      "1.8329172134399414\n",
      "0.9598550796508789\n",
      "0.16623437404632568\n",
      "0.283458411693573\n",
      "0.3083169758319855\n",
      "0.3117481768131256\n",
      "0.27000734210014343\n",
      "0.23115894198417664\n",
      "0.27541476488113403\n",
      "0.19235680997371674\n",
      "0.9696447253227234\n",
      "0.4257797598838806\n",
      "0.4623383581638336\n",
      "0.5989387631416321\n",
      "0.2639574706554413\n",
      "0.0848701074719429\n",
      "0.22939172387123108\n",
      "0.08986471593379974\n",
      "0.09727586805820465\n",
      "0.1403292864561081\n",
      "0.11510743200778961\n",
      "0.045904237776994705\n",
      "0.08698195219039917\n",
      "0.10285496711730957\n",
      "0.2988232970237732\n",
      "0.5488910675048828\n",
      "0.30283814668655396\n",
      "0.11040369421243668\n",
      "0.09370595216751099\n",
      "0.06153693050146103\n",
      "0.06438787281513214\n",
      "0.08469153195619583\n",
      "0.09652240574359894\n",
      "0.03342176228761673\n",
      "0.08280526101589203\n",
      "0.1869257539510727\n",
      "0.053477782756090164\n",
      "0.02556665614247322\n",
      "0.02267000451683998\n",
      "0.014379134401679039\n",
      "0.12250266969203949\n",
      "0.1340486705303192\n",
      "0.01694309152662754\n",
      "0.03059845231473446\n",
      "0.19170056283473969\n",
      "0.218828946352005\n",
      "0.043334294110536575\n",
      "0.03466487303376198\n",
      "0.056079696863889694\n",
      "0.041343025863170624\n",
      "0.012815335765480995\n",
      "0.051064539700746536\n",
      "0.04663433879613876\n",
      "0.062222421169281006\n",
      "5.99897575378418\n",
      "43.2384147644043\n",
      "1.6582530736923218\n",
      "0.4328819215297699\n",
      "0.07153131812810898\n",
      "0.8166744112968445\n",
      "0.9958885312080383\n",
      "0.07536870241165161\n",
      "0.1034967303276062\n",
      "0.15939705073833466\n",
      "0.06167031079530716\n",
      "0.018103772774338722\n",
      "0.018124723806977272\n",
      "0.027844751253724098\n",
      "0.06715761125087738\n",
      "0.03808366134762764\n",
      "0.017483050003647804\n",
      "0.019131287932395935\n",
      "0.03216206282377243\n",
      "0.027594223618507385\n",
      "0.07637763768434525\n",
      "0.17121021449565887\n",
      "0.13210934400558472\n",
      "0.3939552903175354\n",
      "0.3382002115249634\n",
      "0.14125441014766693\n",
      "0.22836855053901672\n",
      "0.26931172609329224\n",
      "0.13838547468185425\n",
      "0.05565439909696579\n",
      "0.1734827309846878\n",
      "0.19182835519313812\n",
      "0.3231956362724304\n",
      "0.26865315437316895\n",
      "0.1928970068693161\n",
      "0.2719305455684662\n",
      "0.39410191774368286\n",
      "0.5657614469528198\n",
      "0.6504095792770386\n",
      "0.5647785663604736\n",
      "0.5486921072006226\n",
      "0.6476553678512573\n",
      "0.6702603101730347\n",
      "0.5488702058792114\n",
      "0.5853667855262756\n",
      "0.5676151514053345\n",
      "0.5224318504333496\n",
      "0.43185606598854065\n",
      "0.3316606879234314\n",
      "0.4502163529396057\n",
      "0.14235959947109222\n",
      "0.07516594231128693\n",
      "0.1666628122329712\n",
      "0.2938362956047058\n",
      "0.23080265522003174\n",
      "0.18934646248817444\n",
      "0.10348590463399887\n",
      "0.39596232771873474\n",
      "0.3496311604976654\n",
      "0.3455662727355957\n",
      "0.23991553485393524\n",
      "0.18936161696910858\n",
      "0.13045784831047058\n",
      "0.28512096405029297\n",
      "0.2886733412742615\n",
      "0.11195103079080582\n",
      "0.1295502632856369\n",
      "0.18623216450214386\n",
      "0.19733071327209473\n",
      "0.24417836964130402\n",
      "0.08179578930139542\n",
      "0.08949001133441925\n",
      "0.7934702038764954\n",
      "9.884547233581543\n",
      "39.14986801147461\n",
      "12.47612190246582\n",
      "3.741428852081299\n",
      "2.3943440914154053\n",
      "1.0626428127288818\n",
      "0.45842915773391724\n",
      "0.4996069073677063\n",
      "0.2822352945804596\n",
      "0.22113189101219177\n",
      "0.3618338108062744\n",
      "0.2733915448188782\n",
      "0.2519550919532776\n",
      "0.12504363059997559\n",
      "0.038578350096940994\n",
      "0.025379275903105736\n",
      "0.035940982401371\n",
      "0.02709742821753025\n",
      "0.02228046953678131\n",
      "0.01155065931379795\n",
      "0.01224473025649786\n",
      "0.01586613990366459\n",
      "0.016804205253720284\n",
      "0.012170011177659035\n",
      "0.028661686927080154\n",
      "0.014853594824671745\n",
      "0.07424750924110413\n",
      "0.0983675867319107\n",
      "0.06097313016653061\n",
      "0.021711016073822975\n",
      "0.07484650611877441\n",
      "0.06834549456834793\n",
      "0.045199453830718994\n",
      "0.03837663307785988\n",
      "0.08580708503723145\n",
      "0.17323485016822815\n",
      "0.19935908913612366\n",
      "0.19224244356155396\n",
      "0.14691373705863953\n",
      "0.017704522237181664\n",
      "0.124875009059906\n",
      "0.17762739956378937\n",
      "0.0655512660741806\n",
      "0.029377996921539307\n",
      "0.016868513077497482\n",
      "0.053166359663009644\n",
      "0.15687890350818634\n",
      "0.14732113480567932\n",
      "0.032243646681308746\n",
      "0.07380478084087372\n",
      "0.15544533729553223\n",
      "0.1407601535320282\n",
      "0.025701815262436867\n",
      "0.11185024678707123\n",
      "0.11082322895526886\n",
      "0.04592408984899521\n",
      "0.4685073792934418\n",
      "1.0516822338104248\n",
      "0.40844789147377014\n",
      "1.3636665344238281\n",
      "1.0190203189849854\n",
      "0.28903692960739136\n",
      "0.11042983829975128\n",
      "0.15418653190135956\n",
      "0.2068757265806198\n",
      "0.055198658257722855\n",
      "0.03863298147916794\n",
      "0.08186507225036621\n",
      "0.08331386744976044\n",
      "0.22060401737689972\n",
      "0.05664702504873276\n",
      "0.10788391530513763\n",
      "0.1910405158996582\n",
      "0.704613447189331\n",
      "2.6778931617736816\n",
      "4.051870346069336\n",
      "0.27454477548599243\n",
      "0.08490043133497238\n",
      "0.09051329642534256\n",
      "0.08596376329660416\n",
      "0.03735695034265518\n",
      "0.025800351053476334\n",
      "0.01719117909669876\n",
      "0.017592577263712883\n",
      "0.03198479115962982\n",
      "0.028502482920885086\n",
      "0.04313572496175766\n",
      "0.0444929301738739\n",
      "0.013905693776905537\n",
      "0.02570740319788456\n",
      "0.03598532825708389\n",
      "0.02974771521985531\n",
      "0.04093382507562637\n",
      "0.07502289861440659\n",
      "0.2853321433067322\n",
      "0.28127574920654297\n",
      "0.1681613177061081\n",
      "0.03456105291843414\n",
      "0.06535471975803375\n",
      "0.047472864389419556\n",
      "0.04019039124250412\n",
      "0.1515720784664154\n",
      "0.13597972691059113\n",
      "0.1200995072722435\n",
      "0.23721852898597717\n",
      "0.2696588635444641\n",
      "0.08119022846221924\n",
      "0.21904237568378448\n",
      "0.3879130780696869\n",
      "0.29715269804000854\n",
      "0.3180171847343445\n",
      "0.3818940818309784\n",
      "0.4895000457763672\n",
      "0.44614696502685547\n",
      "0.3853033781051636\n",
      "0.37135952711105347\n",
      "0.24005571007728577\n",
      "0.2568293511867523\n",
      "0.014813278801739216\n",
      "0.1943720132112503\n",
      "0.27435725927352905\n",
      "0.26240992546081543\n",
      "0.3418673872947693\n",
      "0.20532366633415222\n",
      "0.14825651049613953\n",
      "0.1378331482410431\n",
      "0.17835094034671783\n",
      "0.18645057082176208\n",
      "0.16951988637447357\n",
      "0.17065389454364777\n",
      "4.223296642303467\n",
      "3.5293145179748535\n",
      "0.3764064610004425\n",
      "0.31127655506134033\n",
      "0.27371299266815186\n",
      "0.24700765311717987\n",
      "0.3299235999584198\n",
      "0.3920559287071228\n",
      "0.195500910282135\n",
      "0.1905408352613449\n",
      "0.3344764709472656\n",
      "0.44567984342575073\n",
      "0.3227981925010681\n",
      "0.22457905113697052\n",
      "0.061839520931243896\n",
      "0.02867656946182251\n",
      "0.09420807659626007\n",
      "0.6984914541244507\n",
      "1.181325912475586\n",
      "0.776063084602356\n",
      "0.5434754490852356\n",
      "0.22727617621421814\n",
      "0.07566316425800323\n",
      "0.07997340708971024\n",
      "0.06965969502925873\n",
      "0.10732387006282806\n",
      "0.19341538846492767\n",
      "0.16350796818733215\n",
      "0.07215552031993866\n",
      "0.15900222957134247\n",
      "0.11460015922784805\n",
      "0.12811894714832306\n",
      "0.14612296223640442\n",
      "0.04001689329743385\n",
      "0.059025757014751434\n",
      "0.0911758542060852\n",
      "0.0218660868704319\n",
      "0.0722379982471466\n",
      "0.06709565222263336\n",
      "0.0285864919424057\n",
      "0.02676698938012123\n",
      "0.0519898347556591\n",
      "0.03272074833512306\n",
      "0.02587234415113926\n",
      "0.032580018043518066\n",
      "0.02376222424209118\n",
      "0.036199409514665604\n",
      "0.06002617999911308\n",
      "0.017432600259780884\n",
      "0.03454579785466194\n",
      "0.0602889284491539\n",
      "0.09854362905025482\n",
      "0.10310345888137817\n",
      "0.017027754336595535\n",
      "0.023202115669846535\n",
      "0.05647987127304077\n",
      "0.01737133041024208\n",
      "0.024530528113245964\n",
      "0.0206182561814785\n",
      "0.0242486409842968\n",
      "0.05695962905883789\n",
      "0.012284716591238976\n",
      "0.0779023990035057\n",
      "0.039008356630802155\n",
      "0.032301757484674454\n",
      "0.075368732213974\n",
      "0.048663102090358734\n",
      "0.021857939660549164\n",
      "0.013924039900302887\n",
      "0.009288923814892769\n",
      "0.01419706642627716\n",
      "0.015118208713829517\n",
      "0.028165411204099655\n",
      "0.0598854124546051\n",
      "0.11998746544122696\n",
      "0.036424387246370316\n",
      "0.016142860054969788\n",
      "0.03872697427868843\n",
      "0.014405143447220325\n",
      "0.011125755496323109\n",
      "0.019463172182440758\n",
      "0.023286769166588783\n",
      "0.0127273453399539\n",
      "0.012778853997588158\n",
      "0.016574282199144363\n",
      "0.025559350848197937\n",
      "0.09862564504146576\n",
      "0.07488157600164413\n",
      "0.0764814242720604\n",
      "1.1955664157867432\n",
      "1.5121830701828003\n",
      "0.4953692853450775\n",
      "0.2196509838104248\n",
      "0.29970216751098633\n",
      "0.24842779338359833\n",
      "0.16514882445335388\n",
      "0.3499005436897278\n",
      "0.5072227716445923\n",
      "0.0873337835073471\n",
      "0.175774946808815\n",
      "0.03633381053805351\n",
      "0.014601374976336956\n",
      "0.008460303768515587\n",
      "0.014868694357573986\n",
      "0.009705529548227787\n",
      "0.01041560061275959\n",
      "0.009586942382156849\n",
      "0.012054736725986004\n",
      "0.01109483651816845\n",
      "0.02038358710706234\n",
      "0.022567566484212875\n",
      "0.03947853669524193\n",
      "0.05266687273979187\n",
      "0.033044762909412384\n",
      "0.018665339797735214\n",
      "0.028996635228395462\n",
      "0.03169282525777817\n",
      "0.10571898519992828\n",
      "0.14357121288776398\n",
      "0.027966191992163658\n",
      "0.14869359135627747\n",
      "0.10753784328699112\n",
      "0.05621805787086487\n",
      "0.05076587200164795\n",
      "0.07069210708141327\n",
      "0.05711262300610542\n",
      "0.06997587531805038\n",
      "0.0626385509967804\n",
      "0.09086920320987701\n",
      "0.13634254038333893\n",
      "0.10117683559656143\n",
      "0.12299440056085587\n",
      "0.09284087270498276\n",
      "0.05493972823023796\n",
      "0.065907321870327\n",
      "0.03885453939437866\n",
      "0.033272672444581985\n",
      "0.06892694532871246\n",
      "0.12268798053264618\n",
      "0.05376995727419853\n",
      "0.014781164936721325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027914011850953102\n",
      "0.02579783834517002\n",
      "0.02054740861058235\n",
      "0.02344214916229248\n",
      "0.016709504649043083\n",
      "0.031830549240112305\n",
      "0.04262258857488632\n",
      "0.04932300001382828\n",
      "0.02911645732820034\n",
      "0.0270698219537735\n",
      "0.02889333665370941\n",
      "0.029692042618989944\n",
      "0.04948103427886963\n",
      "0.06430435180664062\n",
      "0.1105302944779396\n",
      "0.11208085715770721\n",
      "0.03153533488512039\n",
      "0.02705686166882515\n",
      "0.08352130651473999\n",
      "0.089476577937603\n",
      "0.9461216926574707\n",
      "0.3199942708015442\n",
      "0.07996408641338348\n",
      "0.041303448379039764\n",
      "0.03860215097665787\n",
      "0.11026950925588608\n",
      "0.04264059290289879\n",
      "0.0621006116271019\n",
      "0.05842796713113785\n",
      "0.04236710071563721\n",
      "0.007877830415964127\n",
      "0.031921058893203735\n",
      "0.04310157522559166\n",
      "0.015336178243160248\n",
      "0.02246442623436451\n",
      "0.05289115756750107\n",
      "0.016565244644880295\n",
      "0.01868678256869316\n",
      "0.04599735140800476\n",
      "0.023163557052612305\n",
      "0.013715775683522224\n",
      "0.014860482886433601\n",
      "0.01631961204111576\n",
      "0.045352041721343994\n",
      "0.04499158263206482\n",
      "0.019625086337327957\n",
      "0.016814645379781723\n",
      "0.08513667434453964\n",
      "0.04181363061070442\n",
      "0.1760375201702118\n",
      "0.3030050992965698\n",
      "0.08292417228221893\n",
      "0.0168917179107666\n",
      "0.033329397439956665\n",
      "0.0785447508096695\n",
      "0.08402645587921143\n",
      "0.09316569566726685\n",
      "0.09961371123790741\n",
      "0.1317644864320755\n",
      "0.1267639696598053\n",
      "0.1151948794722557\n",
      "0.026449594646692276\n",
      "0.04504082724452019\n",
      "0.053341664373874664\n",
      "0.021176759153604507\n",
      "0.025188153609633446\n",
      "0.03463151305913925\n",
      "0.027380626648664474\n",
      "0.018012842163443565\n",
      "0.036676615476608276\n",
      "0.03247261419892311\n",
      "0.045624904334545135\n",
      "0.06707970798015594\n",
      "0.037029121071100235\n",
      "0.021469075232744217\n",
      "0.02532695233821869\n",
      "0.0208846814930439\n",
      "0.02494579181075096\n",
      "0.03459428623318672\n",
      "0.023353219032287598\n",
      "0.02160891517996788\n",
      "0.022889696061611176\n",
      "0.06650284677743912\n",
      "0.0344640389084816\n",
      "0.033714283257722855\n",
      "0.0998593121767044\n",
      "0.10303997993469238\n",
      "0.10768959671258926\n",
      "0.022432580590248108\n",
      "0.016753260046243668\n",
      "0.03783489018678665\n",
      "0.06872809678316116\n",
      "1.3022212982177734\n",
      "0.546340823173523\n",
      "0.12121997773647308\n",
      "0.04526964947581291\n",
      "0.057104162871837616\n",
      "0.06109149381518364\n",
      "0.12269840389490128\n",
      "0.04319038242101669\n",
      "0.08791126310825348\n",
      "0.06531315296888351\n",
      "0.010275667533278465\n",
      "0.06659087538719177\n",
      "0.05679142475128174\n",
      "0.038238439708948135\n",
      "0.025555673986673355\n",
      "0.01382588129490614\n",
      "0.034157343208789825\n",
      "0.011642003431916237\n",
      "0.15180820226669312\n",
      "0.08560897409915924\n",
      "0.1008119136095047\n",
      "0.009099588729441166\n",
      "0.015216119587421417\n",
      "0.039300039410591125\n",
      "0.05220376327633858\n",
      "0.027564192190766335\n",
      "0.012831158936023712\n",
      "0.010332202538847923\n",
      "0.026560723781585693\n",
      "0.03197481483221054\n",
      "0.017198938876390457\n",
      "0.03174562379717827\n",
      "0.014780553989112377\n",
      "0.024376291781663895\n",
      "0.016770733520388603\n",
      "0.02728499472141266\n",
      "0.04369697719812393\n",
      "0.02381979115307331\n",
      "0.024059075862169266\n",
      "0.02210691198706627\n",
      "0.04218089580535889\n",
      "0.09286295622587204\n",
      "0.04864165186882019\n",
      "0.023938175290822983\n",
      "0.03279324620962143\n",
      "0.06836269050836563\n",
      "0.054731786251068115\n",
      "0.038527972996234894\n",
      "0.04107788950204849\n",
      "0.6600876450538635\n",
      "0.08287852257490158\n",
      "0.41349998116493225\n",
      "0.03087594173848629\n",
      "0.02302129566669464\n",
      "0.018680082634091377\n",
      "0.058676671236753464\n",
      "0.11625393480062485\n",
      "0.10443896055221558\n",
      "0.059823013842105865\n",
      "0.0572165846824646\n",
      "0.07983056455850601\n",
      "0.019629834219813347\n",
      "0.030209112912416458\n",
      "0.04746915027499199\n",
      "0.05054046958684921\n",
      "0.04235123097896576\n",
      "0.04221280291676521\n",
      "0.038378458470106125\n",
      "0.04399338364601135\n",
      "0.04819250479340553\n",
      "0.05423649400472641\n",
      "0.549262523651123\n",
      "0.6360902786254883\n",
      "0.2931371331214905\n",
      "0.11980345100164413\n",
      "0.05432598665356636\n",
      "0.028642471879720688\n",
      "0.038155242800712585\n",
      "0.02015642262995243\n",
      "0.040625691413879395\n",
      "0.014296998269855976\n",
      "0.011389384977519512\n",
      "0.024862809106707573\n",
      "0.020801175385713577\n",
      "0.04991016536951065\n",
      "0.04732031747698784\n",
      "0.0069715604186058044\n",
      "0.07281026244163513\n",
      "0.10721047222614288\n",
      "0.03278220444917679\n",
      "0.0457674115896225\n",
      "0.05341261625289917\n",
      "0.026287591084837914\n",
      "0.007102570030838251\n",
      "0.008616091683506966\n",
      "0.01280907355248928\n",
      "0.07025288045406342\n",
      "0.03416790813207626\n",
      "0.014582115225493908\n",
      "0.01365022175014019\n",
      "0.015447558835148811\n",
      "0.015177938155829906\n",
      "0.035407908260822296\n",
      "0.07998119294643402\n",
      "0.043755799531936646\n",
      "0.04249486327171326\n",
      "0.05108265206217766\n",
      "0.01438064593821764\n",
      "0.027181684970855713\n",
      "0.06914424896240234\n",
      "0.06254884600639343\n",
      "0.015840966254472733\n",
      "0.026387179270386696\n",
      "0.047855623066425323\n",
      "0.13287918269634247\n",
      "0.036807864904403687\n",
      "0.055963050574064255\n",
      "0.07652197033166885\n",
      "0.03377702459692955\n",
      "0.05009356141090393\n",
      "0.0658130794763565\n",
      "0.06488996744155884\n",
      "0.030804097652435303\n",
      "0.09304432570934296\n",
      "0.04505543038249016\n",
      "0.047605276107788086\n",
      "0.05520688369870186\n",
      "0.12325187027454376\n",
      "0.09801336377859116\n",
      "0.06350541859865189\n",
      "0.08685148507356644\n",
      "0.11518190801143646\n",
      "0.07208811491727829\n",
      "0.06323046237230301\n",
      "0.07029692828655243\n",
      "0.06218261644244194\n",
      "0.08258803933858871\n",
      "0.0816304087638855\n",
      "0.02355143241584301\n",
      "0.01168750785291195\n",
      "0.014056622982025146\n",
      "0.016466420143842697\n",
      "0.04280690848827362\n",
      "0.053420789539813995\n",
      "4.164384841918945\n",
      "0.9954274296760559\n",
      "0.2780892252922058\n",
      "0.14197850227355957\n",
      "0.09054754674434662\n",
      "0.1264585256576538\n",
      "0.529267430305481\n",
      "0.6747100353240967\n",
      "0.13166868686676025\n",
      "0.3219194710254669\n",
      "0.16262704133987427\n",
      "0.03797505423426628\n",
      "0.07786314189434052\n",
      "0.096743144094944\n",
      "0.01902366615831852\n",
      "0.01874670945107937\n",
      "0.015687376260757446\n",
      "0.02799500897526741\n",
      "0.046254713088274\n",
      "0.14211495220661163\n",
      "0.07014567404985428\n",
      "0.01700986549258232\n",
      "0.01031513512134552\n",
      "0.010946840979158878\n",
      "0.008223515935242176\n",
      "0.012266047298908234\n",
      "0.008103733882308006\n",
      "0.019238337874412537\n",
      "0.007584171835333109\n",
      "0.008595595136284828\n",
      "0.015921518206596375\n",
      "0.009292695671319962\n",
      "0.03666428104043007\n",
      "0.023679178208112717\n",
      "0.006311959121376276\n",
      "0.00815808679908514\n",
      "0.02676638960838318\n",
      "0.020061206072568893\n",
      "0.043607886880636215\n",
      "0.03481683135032654\n",
      "0.05950036644935608\n",
      "0.08476004749536514\n",
      "0.04858378693461418\n",
      "0.03455907106399536\n",
      "0.07219236344099045\n",
      "0.06728009134531021\n",
      "0.0335041806101799\n",
      "0.07780855149030685\n",
      "0.10389511287212372\n",
      "0.016244029626250267\n",
      "0.024456694722175598\n",
      "0.05318731069564819\n",
      "0.07363945990800858\n",
      "0.1140819564461708\n",
      "0.06430716812610626\n",
      "0.03471698611974716\n",
      "0.0434742197394371\n",
      "0.03546006977558136\n",
      "0.03654899075627327\n",
      "0.03910767287015915\n",
      "0.08869972079992294\n",
      "0.1489674150943756\n",
      "0.11237664520740509\n",
      "0.1087539792060852\n",
      "0.13970094919204712\n",
      "0.0649622306227684\n",
      "0.021851515397429466\n",
      "0.017390966415405273\n",
      "0.013963292352855206\n",
      "0.027816588059067726\n",
      "0.08390451967716217\n",
      "0.027821088209748268\n",
      "0.012706547975540161\n",
      "0.9675647020339966\n",
      "2.776182174682617\n",
      "0.11861550807952881\n",
      "0.08337219804525375\n",
      "0.12936249375343323\n",
      "0.05976750701665878\n",
      "0.04360393062233925\n",
      "0.07079948484897614\n",
      "0.16993655264377594\n",
      "0.15481850504875183\n",
      "0.035662032663822174\n",
      "0.06980794668197632\n",
      "0.04302364960312843\n",
      "0.046480122953653336\n",
      "0.04216563701629639\n",
      "0.03150109574198723\n",
      "0.11825201660394669\n",
      "0.03463470935821533\n",
      "0.028824929147958755\n",
      "0.025506019592285156\n",
      "0.016418013721704483\n",
      "0.024548739194869995\n",
      "0.03673643618822098\n",
      "0.024620113894343376\n",
      "0.029024679213762283\n",
      "0.031269121915102005\n",
      "0.058023035526275635\n",
      "0.06432006508111954\n",
      "0.04410844296216965\n",
      "0.07731227576732635\n",
      "0.10229841619729996\n",
      "0.020832370966672897\n",
      "0.026272455230355263\n",
      "0.01897520385682583\n",
      "0.0072265081107616425\n",
      "0.04248398542404175\n",
      "0.01415611058473587\n",
      "0.017915770411491394\n",
      "0.04967609792947769\n",
      "0.08558489382266998\n",
      "0.11683664470911026\n",
      "0.1038689836859703\n",
      "0.04784471541643143\n",
      "0.021515466272830963\n",
      "0.07159824669361115\n",
      "0.051720015704631805\n",
      "0.016239529475569725\n",
      "0.025811636820435524\n",
      "0.017764009535312653\n",
      "0.02066826820373535\n",
      "0.03270634263753891\n",
      "0.04377277195453644\n",
      "0.041672009974718094\n",
      "0.05530039221048355\n",
      "0.027012934908270836\n",
      "0.039345353841781616\n",
      "0.03192907199263573\n",
      "0.05403595045208931\n",
      "0.09558302164077759\n",
      "0.03183066472411156\n",
      "0.038098253309726715\n",
      "0.05955254286527634\n",
      "0.016818072646856308\n",
      "0.030804678797721863\n",
      "0.05289401486515999\n",
      "0.02430582046508789\n",
      "0.016338910907506943\n",
      "0.05054210498929024\n",
      "0.028994793072342873\n",
      "0.010840250179171562\n",
      "0.008017552085220814\n",
      "0.07954605668783188\n",
      "0.21710392832756042\n",
      "3.327409267425537\n",
      "0.35710451006889343\n",
      "0.19273915886878967\n",
      "0.19724485278129578\n",
      "0.22436919808387756\n",
      "0.06531838327646255\n",
      "0.06558901816606522\n",
      "0.09804916381835938\n",
      "0.04567635804414749\n",
      "0.10153453052043915\n",
      "0.08425553143024445\n",
      "0.08486638963222504\n",
      "0.05572022870182991\n",
      "0.05576888471841812\n",
      "0.1568635106086731\n",
      "0.1154639720916748\n",
      "0.060745906084775925\n",
      "0.061021722853183746\n",
      "0.09607885777950287\n",
      "0.02602658048272133\n",
      "0.027608513832092285\n",
      "0.012239612638950348\n",
      "0.011286305263638496\n",
      "0.006842734757810831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010866492986679077\n",
      "0.008657926693558693\n",
      "0.008464627899229527\n",
      "0.017107728868722916\n",
      "0.007316058035939932\n",
      "0.0074246725998818874\n",
      "0.010686175897717476\n",
      "0.03931047394871712\n",
      "0.030346721410751343\n",
      "0.028366591781377792\n",
      "0.03618747740983963\n",
      "0.007062057964503765\n",
      "0.008702153339982033\n",
      "0.028704706579446793\n",
      "0.08407436311244965\n",
      "0.1271972358226776\n",
      "0.013709080405533314\n",
      "0.035261381417512894\n",
      "0.06571371853351593\n",
      "0.03169269487261772\n",
      "0.02910258248448372\n",
      "0.07799644768238068\n",
      "0.0789998322725296\n",
      "0.011112484149634838\n",
      "0.013508734293282032\n",
      "0.01487375982105732\n",
      "0.04878506064414978\n",
      "0.04208315536379814\n",
      "0.01467945147305727\n",
      "0.00959165021777153\n",
      "0.009593483991920948\n",
      "0.030198996886610985\n",
      "0.05968993157148361\n",
      "0.011275405995547771\n",
      "0.020019929856061935\n",
      "0.011789816431701183\n",
      "0.02432519756257534\n",
      "0.015068747103214264\n",
      "0.021184246987104416\n",
      "0.027776163071393967\n",
      "0.019279692322015762\n",
      "0.023858876898884773\n",
      "0.042343657463788986\n",
      "0.02158135548233986\n",
      "0.05377255752682686\n",
      "0.09247546643018723\n",
      "0.17288748919963837\n",
      "0.11603911221027374\n",
      "0.582040548324585\n",
      "3.616265058517456\n",
      "0.48716503381729126\n",
      "0.32936984300613403\n",
      "0.09747956693172455\n",
      "0.17900638282299042\n",
      "0.19870755076408386\n",
      "0.23311424255371094\n",
      "0.18912188708782196\n",
      "0.2770432233810425\n",
      "0.08046514540910721\n",
      "0.05124039575457573\n",
      "0.019613921642303467\n",
      "0.019850993528962135\n",
      "0.017740989103913307\n",
      "0.008947737514972687\n",
      "0.006471611559391022\n",
      "0.00820382870733738\n",
      "0.008067870512604713\n",
      "0.009262407198548317\n",
      "0.007757724728435278\n",
      "0.007962733507156372\n",
      "0.015374521724879742\n",
      "0.012607179582118988\n",
      "0.008705409243702888\n",
      "0.010645228438079357\n",
      "0.008988525718450546\n",
      "0.024647051468491554\n",
      "0.009949540719389915\n",
      "0.008174719288945198\n",
      "0.02136535570025444\n",
      "0.01442476361989975\n",
      "0.008209616877138615\n",
      "0.008692411705851555\n",
      "0.027173031121492386\n",
      "0.02513737417757511\n",
      "0.010286699049174786\n",
      "0.019314564764499664\n",
      "0.007902650162577629\n",
      "0.009020110592246056\n",
      "0.007472252938896418\n",
      "0.014603731222450733\n",
      "0.01947101205587387\n",
      "0.007377258036285639\n",
      "0.05201458930969238\n",
      "0.02977076731622219\n",
      "0.029624084010720253\n",
      "0.017681052908301353\n",
      "0.061779964715242386\n",
      "0.03047773614525795\n",
      "0.017123732715845108\n",
      "0.012426964938640594\n",
      "0.011016080155968666\n",
      "0.016105996444821358\n",
      "0.012075753882527351\n",
      "0.06127055734395981\n",
      "0.05508672446012497\n",
      "0.05839814990758896\n",
      "0.0585503876209259\n",
      "0.05295949429273605\n",
      "0.025035735219717026\n",
      "0.019833886995911598\n",
      "0.029119577258825302\n",
      "0.043223775923252106\n",
      "0.0503169484436512\n",
      "0.04649292677640915\n",
      "0.04352106899023056\n",
      "0.05452809855341911\n",
      "0.04690827429294586\n",
      "0.05649891495704651\n",
      "0.05476130172610283\n",
      "0.04639524966478348\n",
      "0.40011757612228394\n",
      "0.4076286852359772\n",
      "2.408285617828369\n",
      "1.9300880432128906\n",
      "0.41559940576553345\n",
      "0.18206097185611725\n",
      "0.2005964070558548\n",
      "0.34406036138534546\n",
      "0.1468697190284729\n",
      "0.35266122221946716\n",
      "0.20027801394462585\n",
      "0.1769425868988037\n",
      "0.02040468528866768\n",
      "0.07961998879909515\n",
      "0.12169275432825089\n",
      "0.04067046195268631\n",
      "0.009695922955870628\n",
      "0.01948000118136406\n",
      "0.011354743503034115\n",
      "0.011815926060080528\n",
      "0.01554232556372881\n",
      "0.008182699792087078\n",
      "0.013803469017148018\n",
      "0.006041106767952442\n",
      "0.010884838178753853\n",
      "0.08993931859731674\n",
      "0.04976388067007065\n",
      "0.04845274239778519\n",
      "0.006940123625099659\n",
      "0.007125171832740307\n",
      "0.008763979189097881\n",
      "0.008013876155018806\n",
      "0.006065350957214832\n",
      "0.020771358162164688\n",
      "0.012823211960494518\n",
      "0.009676231071352959\n",
      "0.01275251992046833\n",
      "0.006895813159644604\n",
      "0.007589998189359903\n",
      "0.010365557856857777\n",
      "0.014553946442902088\n",
      "0.007689135614782572\n",
      "0.007895885035395622\n",
      "0.019846919924020767\n",
      "0.022396637126803398\n",
      "0.030148375779390335\n",
      "0.024561651051044464\n",
      "0.06242407113313675\n",
      "0.03983336314558983\n",
      "0.06557967513799667\n",
      "0.05691920593380928\n",
      "0.009008773602545261\n",
      "0.040899746119976044\n",
      "0.0511932447552681\n",
      "0.03662896901369095\n",
      "0.034075237810611725\n",
      "0.07414596527814865\n",
      "0.0296345017850399\n",
      "0.026005178689956665\n",
      "0.026354774832725525\n",
      "0.029205968603491783\n",
      "0.02337932214140892\n",
      "0.025509078055620193\n",
      "0.021489139646291733\n",
      "0.03273674473166466\n",
      "0.02694552019238472\n",
      "0.03993726894259453\n",
      "0.05764678865671158\n",
      "0.018697518855333328\n",
      "0.0178091861307621\n",
      "0.01598423346877098\n",
      "0.016315212473273277\n",
      "0.013947883620858192\n",
      "0.03512590378522873\n",
      "0.056789837777614594\n",
      "0.5333855152130127\n",
      "0.3029855489730835\n",
      "5.395681858062744\n",
      "4.386744022369385\n",
      "4.2526655197143555\n",
      "0.4605807363986969\n",
      "0.1952475607395172\n",
      "0.16029715538024902\n",
      "0.20162205398082733\n",
      "0.32115182280540466\n",
      "0.49916553497314453\n",
      "0.36491772532463074\n",
      "0.26444211602211\n",
      "0.10572830587625504\n",
      "0.06063740700483322\n",
      "0.43415436148643494\n",
      "0.9820230603218079\n",
      "4.6662821769714355\n",
      "8.373252868652344\n",
      "4.168591499328613\n",
      "1.1270112991333008\n",
      "0.08801643550395966\n",
      "0.07434023171663284\n",
      "0.05852218344807625\n",
      "0.28779590129852295\n",
      "0.3083510994911194\n",
      "0.27966517210006714\n",
      "0.24097895622253418\n",
      "0.06907327473163605\n",
      "0.05676388740539551\n",
      "0.06525623798370361\n",
      "0.06293487548828125\n",
      "0.1270207315683365\n",
      "0.3292933702468872\n",
      "0.25247007608413696\n",
      "0.11997967958450317\n",
      "0.06646034866571426\n",
      "0.026647889986634254\n",
      "0.022289011627435684\n",
      "0.036495279520750046\n",
      "0.012526588514447212\n",
      "0.011883629485964775\n",
      "0.029602736234664917\n",
      "0.009712226688861847\n",
      "0.017939763143658638\n",
      "0.013758385553956032\n",
      "0.0050698514096438885\n",
      "0.0216981228441\n",
      "0.02467089518904686\n",
      "0.019703656435012817\n",
      "0.013702127151191235\n",
      "0.0162816159427166\n",
      "0.007916614413261414\n",
      "0.019539620727300644\n",
      "0.008238091133534908\n",
      "0.028864238411188126\n",
      "0.006260201334953308\n",
      "0.0070326803252100945\n",
      "0.009623108431696892\n",
      "0.006661826279014349\n",
      "0.004404452629387379\n",
      "0.006421778351068497\n",
      "0.015844976529479027\n",
      "0.012090694159269333\n",
      "0.016827622428536415\n",
      "0.04351671785116196\n",
      "0.19985631108283997\n",
      "0.13949045538902283\n",
      "0.010733531787991524\n",
      "0.00616577360779047\n",
      "0.014349812641739845\n",
      "0.047839388251304626\n",
      "0.39058980345726013\n",
      "0.5393800735473633\n",
      "3.2636406421661377\n",
      "0.6825565099716187\n",
      "0.16235584020614624\n",
      "0.14984162151813507\n",
      "0.08145013451576233\n",
      "0.1528104543685913\n",
      "0.08486060053110123\n",
      "0.06416070461273193\n",
      "0.07631872594356537\n",
      "0.12459897994995117\n",
      "0.11639733612537384\n",
      "0.059192508459091187\n",
      "0.13820627331733704\n",
      "0.10122611373662949\n",
      "0.026838969439268112\n",
      "0.016321813687682152\n",
      "0.04241618141531944\n",
      "0.07053497433662415\n",
      "0.10882236808538437\n",
      "0.1537047028541565\n",
      "0.10835468769073486\n",
      "0.008326290175318718\n",
      "0.030246267095208168\n",
      "0.05744694918394089\n",
      "0.01106601394712925\n",
      "0.06981515884399414\n",
      "0.04087431728839874\n",
      "0.013909831643104553\n",
      "0.0088393185287714\n",
      "0.0123834740370512\n",
      "0.026686647906899452\n",
      "0.023388642817735672\n",
      "0.010647939518094063\n",
      "0.024440687149763107\n",
      "0.019850723445415497\n",
      "0.03442620113492012\n",
      "0.007885683327913284\n",
      "0.00823230016976595\n",
      "0.006599468644708395\n",
      "0.010850879363715649\n",
      "0.016739243641495705\n",
      "0.027631497010588646\n",
      "0.06020919233560562\n",
      "0.03756588697433472\n",
      "0.02606957033276558\n",
      "0.009750660508871078\n",
      "0.010478779673576355\n",
      "0.047268517315387726\n",
      "0.05063619464635849\n",
      "0.04312776401638985\n",
      "0.015757771208882332\n",
      "0.051028184592723846\n",
      "0.04446607083082199\n",
      "0.020220953971147537\n",
      "0.038605574518442154\n",
      "0.013265718705952168\n",
      "0.01134383212774992\n",
      "0.008271058090031147\n",
      "0.006077578756958246\n",
      "0.01294267363846302\n",
      "0.010981069877743721\n",
      "0.01925824210047722\n",
      "0.011548863723874092\n",
      "0.04505344107747078\n",
      "0.013691122643649578\n",
      "0.014423522166907787\n",
      "0.022522203624248505\n",
      "0.007908234372735023\n",
      "0.01494473498314619\n",
      "0.008110836148262024\n",
      "0.017977699637413025\n",
      "0.17763909697532654\n",
      "31.526315689086914\n",
      "15.677050590515137\n",
      "19.796199798583984\n",
      "1.4392629861831665\n",
      "2.9307045936584473\n",
      "4.852055072784424\n",
      "5.67678689956665\n",
      "38.000877380371094\n",
      "203.9720916748047\n",
      "64.42097473144531\n",
      "0.7688415050506592\n",
      "0.04808957129716873\n",
      "0.5747523307800293\n",
      "0.5426570773124695\n",
      "2.548314332962036\n",
      "0.6544224619865417\n",
      "2.1190731525421143\n",
      "5.8146138191223145\n",
      "30.41336441040039\n",
      "13.754864692687988\n",
      "0.6254821419715881\n",
      "0.8991716504096985\n",
      "1.195648193359375\n",
      "0.1975439488887787\n",
      "0.03225846588611603\n",
      "1.2284356355667114\n",
      "3.1414403915405273\n",
      "2.1991684436798096\n",
      "1.1916218996047974\n",
      "1.2205402851104736\n",
      "1.2366416454315186\n",
      "1.519524335861206\n",
      "1.0874637365341187\n",
      "0.38246458768844604\n",
      "0.6047884821891785\n",
      "0.6265801191329956\n",
      "0.24000029265880585\n",
      "0.28583207726478577\n",
      "0.24686706066131592\n",
      "0.14835935831069946\n",
      "0.2068384438753128\n",
      "0.0837847962975502\n",
      "0.13768179714679718\n",
      "0.16195984184741974\n",
      "0.0796971470117569\n",
      "0.17664039134979248\n",
      "0.2999108135700226\n",
      "0.24892833828926086\n",
      "0.103249691426754\n",
      "0.08518589287996292\n",
      "0.049588777124881744\n",
      "0.04517596960067749\n",
      "0.030271317809820175\n",
      "0.023274656385183334\n",
      "0.02713724598288536\n",
      "0.02477268874645233\n",
      "0.12918242812156677\n",
      "0.07777692377567291\n",
      "0.06039794534444809\n",
      "0.020517870783805847\n",
      "0.053140442818403244\n",
      "0.048299238085746765\n",
      "0.01967095024883747\n",
      "0.020449163392186165\n",
      "0.022156359627842903\n",
      "0.015248958952724934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018084099516272545\n",
      "0.045684974640607834\n",
      "0.03101060353219509\n",
      "0.03153520077466965\n",
      "0.037806376814842224\n",
      "0.033447153866291046\n",
      "0.0348091758787632\n",
      "0.07237911224365234\n",
      "0.07395735383033752\n",
      "0.13464462757110596\n",
      "0.3501848876476288\n",
      "0.23054027557373047\n",
      "0.08460094779729843\n",
      "0.10470856726169586\n",
      "0.10519029945135117\n",
      "0.11441679298877716\n",
      "0.41507118940353394\n",
      "0.28247886896133423\n",
      "0.12819698452949524\n",
      "0.1762998402118683\n",
      "0.28677088022232056\n",
      "0.19652816653251648\n",
      "0.23430290818214417\n",
      "0.2822086811065674\n",
      "0.19605694711208344\n",
      "0.10711655765771866\n",
      "0.12584586441516876\n",
      "0.21388161182403564\n",
      "0.30297762155532837\n",
      "0.4091445505619049\n",
      "0.37384963035583496\n",
      "0.34602850675582886\n",
      "1.845091462135315\n",
      "1.6021921634674072\n",
      "1.5702320337295532\n",
      "1.2447805404663086\n",
      "1.1299254894256592\n",
      "1.4268763065338135\n",
      "0.7702649235725403\n",
      "0.9739810228347778\n",
      "0.7571384310722351\n",
      "0.5946229100227356\n",
      "0.12926676869392395\n",
      "0.06679240614175797\n",
      "0.1663810759782791\n",
      "0.184660404920578\n",
      "0.12138569355010986\n",
      "0.07875178754329681\n",
      "0.13882236182689667\n",
      "0.12473921477794647\n",
      "0.140143021941185\n",
      "0.22093641757965088\n",
      "0.0654257982969284\n",
      "0.09083911776542664\n",
      "0.10214096307754517\n",
      "0.015286808833479881\n",
      "0.019152550026774406\n",
      "0.0802188515663147\n",
      "0.0680404081940651\n",
      "0.02365984581410885\n",
      "0.01762869395315647\n",
      "0.016722941771149635\n",
      "0.07303688675165176\n",
      "0.03183800354599953\n",
      "0.020723965018987656\n",
      "0.03088904358446598\n",
      "0.00966615043580532\n",
      "0.016793962568044662\n",
      "0.015562142245471478\n",
      "0.013686932623386383\n",
      "0.03476186469197273\n",
      "0.019252324476838112\n",
      "0.01003965176641941\n",
      "0.010048473253846169\n",
      "0.015009462833404541\n",
      "0.009795939549803734\n",
      "0.023946115747094154\n",
      "0.029512306675314903\n",
      "0.02972322143614292\n",
      "0.041254766285419464\n",
      "0.026777511462569237\n",
      "0.029331687837839127\n",
      "0.038024649024009705\n",
      "0.03307116776704788\n",
      "0.037321917712688446\n",
      "0.030828583985567093\n",
      "0.01868034154176712\n",
      "0.016253041103482246\n",
      "0.06843604147434235\n",
      "0.01825869455933571\n",
      "0.025078821927309036\n",
      "0.018868617713451385\n",
      "0.016521692276000977\n",
      "0.031709689646959305\n",
      "0.06737454235553741\n",
      "0.09671956300735474\n",
      "0.0572570338845253\n",
      "0.03086969256401062\n",
      "0.036528609693050385\n",
      "0.11543166637420654\n",
      "0.2026345282793045\n",
      "0.1516038030385971\n",
      "0.05963651090860367\n",
      "0.04572925716638565\n",
      "0.035324908792972565\n",
      "0.06310833990573883\n",
      "0.13224458694458008\n",
      "0.4330824911594391\n",
      "1.7032493352890015\n",
      "0.7863913774490356\n",
      "0.6567298173904419\n",
      "0.5543872714042664\n",
      "2.720643997192383\n",
      "2.273589611053467\n",
      "0.9851115345954895\n",
      "0.19409067928791046\n",
      "0.2997671067714691\n",
      "0.28723180294036865\n",
      "0.16311852633953094\n",
      "0.11386775970458984\n",
      "0.14598257839679718\n",
      "0.11864075809717178\n",
      "0.04492522031068802\n",
      "0.05185612291097641\n",
      "0.07177677005529404\n",
      "0.43740114569664\n",
      "0.26033681631088257\n",
      "0.5916920900344849\n",
      "0.06652456521987915\n",
      "0.039604030549526215\n",
      "0.06109923869371414\n",
      "0.0706002339720726\n",
      "0.048123542219400406\n",
      "0.07270118594169617\n",
      "0.05585939437150955\n",
      "0.06698153913021088\n",
      "0.066595159471035\n",
      "0.0512908510863781\n",
      "0.035642050206661224\n",
      "0.060305405408144\n",
      "0.012757673859596252\n",
      "0.02513887919485569\n",
      "0.029977848753333092\n",
      "0.03448965400457382\n",
      "0.06643557548522949\n",
      "0.06214524060487747\n",
      "0.04863443225622177\n",
      "0.026705706492066383\n",
      "0.0458390936255455\n",
      "0.10963044315576553\n",
      "0.0995713472366333\n",
      "0.08118157833814621\n",
      "0.0656503438949585\n",
      "0.08048652112483978\n",
      "0.05062160640954971\n",
      "0.05054238438606262\n",
      "0.060990869998931885\n",
      "0.027366602793335915\n",
      "0.037707965821027756\n",
      "0.025048743933439255\n",
      "0.021747605875134468\n",
      "0.05049426481127739\n",
      "0.023040588945150375\n",
      "0.020459908992052078\n",
      "0.02640017867088318\n",
      "0.030056800693273544\n",
      "0.016091100871562958\n",
      "0.028678715229034424\n",
      "0.007830800488591194\n",
      "0.0730355754494667\n",
      "0.03797665610909462\n",
      "0.056175924837589264\n",
      "0.036245644092559814\n",
      "0.03207682818174362\n",
      "0.05978693813085556\n",
      "0.06314539909362793\n",
      "0.039706479758024216\n",
      "0.015337169170379639\n",
      "0.03150606155395508\n",
      "0.00858717504888773\n",
      "0.024587521329522133\n",
      "0.022761842235922813\n",
      "0.04084724187850952\n",
      "0.16793908178806305\n",
      "0.1620214432477951\n",
      "33.14331817626953\n",
      "12.531309127807617\n",
      "0.39181941747665405\n",
      "0.3013083040714264\n",
      "0.22953681647777557\n",
      "0.4235299527645111\n",
      "0.27571338415145874\n",
      "0.1449691355228424\n",
      "0.17569640278816223\n",
      "0.07397781312465668\n",
      "0.05340595170855522\n",
      "0.044663455337285995\n",
      "0.223506361246109\n",
      "0.18507196009159088\n",
      "0.21355757117271423\n",
      "0.3359822630882263\n",
      "0.10603474080562592\n",
      "0.03688759356737137\n",
      "0.04114723950624466\n",
      "0.021247724071145058\n",
      "0.008252345025539398\n",
      "0.010178505443036556\n",
      "0.013610067777335644\n",
      "0.015155123546719551\n",
      "0.021512161940336227\n",
      "0.026801850646734238\n",
      "0.04017465561628342\n",
      "0.011676228605210781\n",
      "0.0070148431695997715\n",
      "0.022124091163277626\n",
      "0.009582934901118279\n",
      "0.026303377002477646\n",
      "0.03220342844724655\n",
      "0.06332588940858841\n",
      "0.11797913163900375\n",
      "0.06772150099277496\n",
      "0.024665705859661102\n",
      "0.029397618025541306\n",
      "0.014716538600623608\n",
      "0.04338777810335159\n",
      "0.004109521862119436\n",
      "0.010431870818138123\n",
      "0.04802604019641876\n",
      "0.01070777140557766\n",
      "0.0161238145083189\n",
      "0.4766716957092285\n",
      "0.23674681782722473\n",
      "0.33398061990737915\n",
      "0.01625526137650013\n",
      "0.028484702110290527\n",
      "0.0204751156270504\n",
      "0.0072854855097830296\n",
      "0.007533165160566568\n",
      "0.007302910089492798\n",
      "0.010343628004193306\n",
      "0.014378907158970833\n",
      "0.015471053309738636\n",
      "0.017691243439912796\n",
      "0.01075262762606144\n",
      "0.03896158188581467\n",
      "0.016954975202679634\n",
      "0.008134333416819572\n",
      "0.013251537457108498\n",
      "0.01145157776772976\n",
      "0.04681619256734848\n",
      "0.009559722617268562\n",
      "0.0337505005300045\n",
      "0.05627800151705742\n",
      "0.028923580422997475\n",
      "0.020080503076314926\n",
      "0.015857886523008347\n",
      "0.04611609876155853\n",
      "1.2789175510406494\n",
      "0.5090882778167725\n",
      "0.14540095627307892\n",
      "0.1698782593011856\n",
      "0.2837299704551697\n",
      "0.2257678508758545\n",
      "0.08877123892307281\n",
      "0.040881283581256866\n",
      "5.025479316711426\n",
      "1.6596981287002563\n",
      "3.0415077209472656\n",
      "0.03018290363252163\n",
      "0.021993327885866165\n",
      "0.016249805688858032\n",
      "0.07248534262180328\n",
      "0.06033216044306755\n",
      "0.08017583936452866\n",
      "0.040495146065950394\n",
      "0.12013547122478485\n",
      "0.025934439152479172\n",
      "0.0064978511072695255\n",
      "0.01661558821797371\n",
      "0.01564094051718712\n",
      "0.04574975743889809\n",
      "0.17870540916919708\n",
      "0.16144710779190063\n",
      "0.4584532678127289\n",
      "0.010691964067518711\n",
      "0.021211307495832443\n",
      "0.007424821145832539\n",
      "0.011973937042057514\n",
      "0.007130021695047617\n",
      "0.01677611470222473\n",
      "0.027591576799750328\n",
      "0.050180744379758835\n",
      "0.02613222599029541\n",
      "0.015244962647557259\n",
      "0.015775537118315697\n",
      "0.028713911771774292\n",
      "0.019361793994903564\n",
      "0.017098452895879745\n",
      "0.03213881328701973\n",
      "0.015717510133981705\n",
      "0.013620818965137005\n",
      "0.040169328451156616\n",
      "0.0244000181555748\n",
      "0.015855656936764717\n",
      "0.028111983090639114\n",
      "0.03322569280862808\n",
      "0.03147474303841591\n",
      "0.06626331061124802\n",
      "0.03414682298898697\n",
      "0.025621730834245682\n",
      "0.051531292498111725\n",
      "0.03379819542169571\n",
      "0.01107156090438366\n",
      "0.03543747588992119\n",
      "0.027015840634703636\n",
      "0.013659377582371235\n",
      "0.018952708691358566\n",
      "0.04024166613817215\n",
      "0.02236390858888626\n",
      "0.012720187194645405\n",
      "0.01572953164577484\n",
      "0.013154657557606697\n",
      "0.015182161703705788\n",
      "0.012899341993033886\n",
      "0.011393770575523376\n",
      "0.01421383023262024\n",
      "0.006441777106374502\n",
      "0.03816906735301018\n",
      "0.017036868259310722\n",
      "0.8256217241287231\n",
      "0.34851041436195374\n",
      "0.12641306221485138\n",
      "0.06572000682353973\n",
      "0.07816114276647568\n",
      "0.07552613317966461\n",
      "0.07817327976226807\n",
      "0.10376767814159393\n",
      "0.07779379189014435\n",
      "0.045447029173374176\n",
      "0.08814806491136551\n",
      "0.11075405776500702\n",
      "0.04382624849677086\n",
      "0.03605109825730324\n",
      "0.08384329080581665\n",
      "0.08784129470586777\n",
      "0.02848884090781212\n",
      "0.025641173124313354\n",
      "0.017516616731882095\n",
      "0.02779329940676689\n",
      "0.03806328773498535\n",
      "0.012800700962543488\n",
      "0.030223947018384933\n",
      "0.06075669452548027\n",
      "0.014477337710559368\n",
      "0.009797943755984306\n",
      "0.01662425883114338\n",
      "0.011723090894520283\n",
      "0.01103014126420021\n",
      "0.03267838433384895\n",
      "0.010238932445645332\n",
      "0.052353013306856155\n",
      "0.028557609766721725\n",
      "0.008260832168161869\n",
      "0.29484573006629944\n",
      "0.5150356292724609\n",
      "0.10524896532297134\n",
      "0.06598813086748123\n",
      "0.02491586282849312\n",
      "0.05058656260371208\n",
      "0.018839523196220398\n",
      "0.011098540388047695\n",
      "0.01262718252837658\n",
      "0.02485310658812523\n",
      "0.016046758741140366\n",
      "0.011724662035703659\n",
      "0.024597855284810066\n",
      "0.026746805757284164\n",
      "0.024788621813058853\n",
      "0.017000051215291023\n",
      "0.01883510686457157\n",
      "0.01479061134159565\n",
      "0.01558548491448164\n",
      "0.02073242887854576\n",
      "0.023863598704338074\n",
      "0.018289417028427124\n",
      "0.03362012654542923\n",
      "0.02938728965818882\n",
      "0.01599997654557228\n",
      "0.01491878367960453\n",
      "0.026685316115617752\n",
      "0.013572996482253075\n",
      "0.012312376871705055\n",
      "0.015931466594338417\n",
      "0.06573236733675003\n",
      "0.02483431063592434\n",
      "0.01590074971318245\n",
      "0.023109983652830124\n",
      "0.008663850836455822\n",
      "0.021183405071496964\n",
      "0.07464421540498734\n",
      "0.06396027654409409\n",
      "0.9156819581985474\n",
      "0.238468736410141\n",
      "0.1268215924501419\n",
      "0.09014682471752167\n",
      "0.07021874189376831\n",
      "0.04448064789175987\n",
      "0.03498629480600357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04289961978793144\n",
      "0.04058552533388138\n",
      "0.0274885892868042\n",
      "0.02665216289460659\n",
      "0.026383060961961746\n",
      "0.026035333052277565\n",
      "0.037983715534210205\n",
      "0.025306057184934616\n",
      "0.017046092078089714\n",
      "0.01064587663859129\n",
      "0.011430774815380573\n",
      "0.023117486387491226\n",
      "0.036350466310977936\n",
      "0.020707566291093826\n",
      "0.028402844443917274\n",
      "0.028809521347284317\n",
      "0.028488298878073692\n",
      "0.011429140344262123\n",
      "0.009570079855620861\n",
      "0.026606595143675804\n",
      "0.04472099989652634\n",
      "0.04413831979036331\n",
      "0.01501480769366026\n",
      "0.009487414732575417\n",
      "0.0172174833714962\n",
      "0.01310006808489561\n",
      "0.031708478927612305\n",
      "0.023033535107970238\n",
      "0.05204082280397415\n",
      "0.06429308652877808\n",
      "0.030120376497507095\n",
      "0.03156978636980057\n",
      "0.030749276280403137\n",
      "0.028166303411126137\n",
      "0.028734330087900162\n",
      "0.04442676529288292\n",
      "0.036137063056230545\n",
      "0.02165529504418373\n",
      "0.03065468743443489\n",
      "0.04487984627485275\n",
      "0.03718847781419754\n",
      "0.02210230380296707\n",
      "0.03628205507993698\n",
      "0.03954198583960533\n",
      "0.01759449392557144\n",
      "0.03871157020330429\n",
      "0.05553705617785454\n",
      "0.06288906186819077\n",
      "0.05343518778681755\n",
      "0.037890270352363586\n",
      "0.04068567976355553\n",
      "0.035962872207164764\n",
      "0.02171958237886429\n",
      "0.02300131693482399\n",
      "0.023673249408602715\n",
      "0.02466738596558571\n",
      "0.03190595656633377\n",
      "0.02341059222817421\n",
      "0.023103106766939163\n",
      "0.027738820761442184\n",
      "0.02010018192231655\n",
      "0.028877193108201027\n",
      "0.02145930752158165\n",
      "0.03720469027757645\n",
      "0.0845348984003067\n",
      "0.8922117352485657\n",
      "0.10378463566303253\n",
      "0.0628555417060852\n",
      "0.0282102283090353\n",
      "0.017075562849640846\n",
      "0.05684946849942207\n",
      "3.31502103805542\n",
      "1.5219122171401978\n",
      "1.8128576278686523\n",
      "0.08588080108165741\n",
      "0.10974502563476562\n",
      "0.1520501673221588\n",
      "0.014870007522404194\n",
      "0.014774009585380554\n",
      "0.014539490453898907\n",
      "0.00892709381878376\n",
      "0.007494094781577587\n",
      "0.0063733807764947414\n",
      "0.006592708174139261\n",
      "0.00933474488556385\n",
      "0.013397961854934692\n",
      "0.014878252521157265\n",
      "0.01396210491657257\n",
      "0.008012538775801659\n",
      "0.02054254524409771\n",
      "0.053413134068250656\n",
      "0.018891356885433197\n",
      "0.012113838456571102\n",
      "0.012963538989424706\n",
      "0.13695243000984192\n",
      "0.08084011822938919\n",
      "0.023428434506058693\n",
      "0.04544415697455406\n",
      "0.05048864334821701\n",
      "0.04498161002993584\n",
      "0.3586418330669403\n",
      "0.25657933950424194\n",
      "0.015640748664736748\n",
      "0.02247113734483719\n",
      "0.02501039206981659\n",
      "0.027790972962975502\n",
      "0.02445586957037449\n",
      "0.021659020334482193\n",
      "0.026072943583130836\n",
      "0.023453420028090477\n",
      "0.03551242873072624\n",
      "0.02529228664934635\n",
      "0.02235179767012596\n",
      "0.01983114518225193\n",
      "0.031890030950307846\n",
      "0.025948932394385338\n",
      "0.016442779451608658\n",
      "0.025938233360648155\n",
      "0.03167787939310074\n",
      "0.025696540251374245\n",
      "0.029558833688497543\n",
      "0.017959248274564743\n",
      "0.035474151372909546\n",
      "0.03278811275959015\n",
      "0.022993894293904305\n",
      "0.018294822424650192\n",
      "0.014968691393733025\n",
      "0.02187676914036274\n",
      "0.020873263478279114\n",
      "0.015726828947663307\n",
      "0.016352221369743347\n",
      "0.017735373228788376\n",
      "0.02564224600791931\n",
      "0.022670967504382133\n",
      "0.01292913593351841\n",
      "0.018570685759186745\n",
      "0.2957363724708557\n",
      "2.3341879844665527\n",
      "0.11111845821142197\n",
      "0.06030883640050888\n",
      "0.036040257662534714\n",
      "0.054218899458646774\n",
      "0.03464490547776222\n",
      "0.043742768466472626\n",
      "0.017098771408200264\n",
      "0.015932150185108185\n",
      "0.010909597389400005\n",
      "0.01048057246953249\n",
      "0.028089482337236404\n",
      "0.017161086201667786\n",
      "0.0064314850606024265\n",
      "0.008255867287516594\n",
      "0.006128882057964802\n",
      "0.02250426635146141\n",
      "0.008910244330763817\n",
      "0.007219627499580383\n",
      "0.03113011084496975\n",
      "0.0315946564078331\n",
      "0.018965944647789\n",
      "0.006376226432621479\n",
      "0.01356195192784071\n",
      "0.00784916989505291\n",
      "0.024668466299772263\n",
      "0.03546726703643799\n",
      "0.015815170481801033\n",
      "0.023268569260835648\n",
      "0.01576157845556736\n",
      "0.026107627898454666\n",
      "0.03400227054953575\n",
      "0.017036080360412598\n",
      "0.012567180208861828\n",
      "0.011377387680113316\n",
      "0.0071249911561608315\n",
      "0.011182083748281002\n",
      "0.015102791599929333\n",
      "0.01913098804652691\n",
      "0.044363632798194885\n",
      "0.02579134702682495\n",
      "0.04877106100320816\n",
      "0.020952342078089714\n",
      "0.01707254722714424\n",
      "0.014250168576836586\n",
      "0.015761474147439003\n",
      "0.021566186100244522\n",
      "0.01779310405254364\n",
      "0.09141399711370468\n",
      "0.029978808015584946\n",
      "0.014681289903819561\n",
      "0.045783840119838715\n",
      "0.07700648903846741\n",
      "0.03645307943224907\n",
      "0.02702636644244194\n",
      "0.025315213948488235\n",
      "0.02543242834508419\n",
      "0.01939449831843376\n",
      "0.031221140176057816\n",
      "0.032370273023843765\n",
      "0.03308733552694321\n",
      "0.027327414602041245\n",
      "0.0303304735571146\n",
      "0.03795154392719269\n",
      "0.026391979306936264\n",
      "0.016573408618569374\n",
      "0.025365334004163742\n",
      "0.024563517421483994\n",
      "0.022928861901164055\n",
      "0.02071508951485157\n",
      "0.045199476182460785\n",
      "0.7871512174606323\n",
      "0.7240664958953857\n",
      "0.4196709096431732\n",
      "0.07047603279352188\n",
      "0.013005045242607594\n",
      "0.025743568316102028\n",
      "0.019569721072912216\n",
      "0.02498817630112171\n",
      "0.037028439342975616\n",
      "0.029930662363767624\n",
      "0.020835209637880325\n",
      "0.008566584438085556\n",
      "0.010904623195528984\n",
      "0.010615961626172066\n",
      "0.026017386466264725\n",
      "0.030451660975813866\n",
      "0.02170640602707863\n",
      "0.009822973981499672\n",
      "0.01240088976919651\n",
      "0.01046665944159031\n",
      "0.007925143465399742\n",
      "0.009221585467457771\n",
      "0.014771337620913982\n",
      "0.012195311486721039\n",
      "0.024288801476359367\n",
      "0.03718437999486923\n",
      "0.01441258005797863\n",
      "0.01414605975151062\n",
      "0.021360555663704872\n",
      "0.038482408970594406\n",
      "0.028928380459547043\n",
      "0.031184319406747818\n",
      "0.04216655716300011\n",
      "0.02962617203593254\n",
      "0.03327103704214096\n",
      "0.03043311834335327\n",
      "0.01649329997599125\n",
      "0.017377573996782303\n",
      "0.023057367652654648\n",
      "0.02002151869237423\n",
      "0.038737885653972626\n",
      "0.03884092718362808\n",
      "0.032251548022031784\n",
      "0.028004014864563942\n",
      "0.01771981082856655\n",
      "0.03769027069211006\n",
      "0.024810275062918663\n",
      "0.016783982515335083\n",
      "0.02253640815615654\n",
      "0.04244520142674446\n",
      "0.05292096734046936\n",
      "0.027705714106559753\n",
      "0.02462487481534481\n",
      "0.05220410227775574\n",
      "0.07825114578008652\n",
      "0.03439231216907501\n",
      "0.032027967274188995\n",
      "0.026927996426820755\n",
      "0.03551614284515381\n",
      "0.02645236626267433\n",
      "0.03394169360399246\n",
      "0.042832598090171814\n",
      "0.021336784586310387\n",
      "0.03273404762148857\n",
      "0.02842898666858673\n",
      "0.02896622195839882\n",
      "0.06029988452792168\n",
      "4.374436378479004\n",
      "2.426121711730957\n",
      "1.9554656744003296\n",
      "0.07892350107431412\n",
      "0.05742108076810837\n",
      "0.4310677945613861\n",
      "0.1137835830450058\n",
      "0.02996406890451908\n",
      "0.03258790820837021\n",
      "0.018310461193323135\n",
      "0.010339891538023949\n",
      "0.016408249735832214\n",
      "0.01561802625656128\n",
      "0.020618030801415443\n",
      "0.38830965757369995\n",
      "0.27285489439964294\n",
      "0.16495028138160706\n",
      "0.09097366034984589\n",
      "0.10029833018779755\n",
      "0.06748165190219879\n",
      "0.03908552601933479\n",
      "0.04797070100903511\n",
      "0.026644399389624596\n",
      "0.037119753658771515\n",
      "0.030660375952720642\n",
      "0.030401239171624184\n",
      "0.0202304869890213\n",
      "0.02645929716527462\n",
      "0.01585174724459648\n",
      "0.03759533166885376\n",
      "0.01570211909711361\n",
      "0.024901816621422768\n",
      "0.018980851396918297\n",
      "0.015220182947814465\n",
      "0.02629813551902771\n",
      "0.01839541271328926\n",
      "0.012776821851730347\n",
      "0.008318683132529259\n",
      "0.00762257631868124\n",
      "0.011160466820001602\n",
      "0.007230585906654596\n",
      "0.02409900538623333\n",
      "0.02476409822702408\n",
      "0.06278107315301895\n",
      "0.027182335034012794\n",
      "0.018568363040685654\n",
      "0.05243309587240219\n",
      "0.005351448431611061\n",
      "0.06454138457775116\n",
      "0.023684190586209297\n",
      "0.014982717111706734\n",
      "0.013115939684212208\n",
      "0.008454902097582817\n",
      "0.017316507175564766\n",
      "0.01786324940621853\n",
      "0.05283766984939575\n",
      "0.03158904239535332\n",
      "0.020457662642002106\n",
      "0.020896075293421745\n",
      "0.028165215626358986\n",
      "0.04085832089185715\n",
      "0.0119897136464715\n",
      "0.01044214516878128\n",
      "0.0192414578050375\n",
      "0.015881873667240143\n",
      "0.018066147342324257\n",
      "0.01499147992581129\n",
      "0.00783471018075943\n",
      "0.012308290228247643\n",
      "0.01128316204994917\n",
      "0.008256403729319572\n",
      "0.015380119904875755\n",
      "0.011673967353999615\n",
      "0.012454183772206306\n",
      "0.01079361792653799\n",
      "0.04771798104047775\n",
      "0.011888250708580017\n",
      "0.2631627917289734\n",
      "0.07850195467472076\n",
      "0.027986060827970505\n",
      "0.02326318807899952\n",
      "0.025129955261945724\n",
      "0.024416519328951836\n",
      "0.03307159245014191\n",
      "0.010649635456502438\n",
      "0.013797986321151257\n",
      "0.01398892980068922\n",
      "0.005891459062695503\n",
      "0.005940317176282406\n",
      "0.00911751575767994\n",
      "0.012122971937060356\n",
      "0.023236507549881935\n",
      "0.013379296287894249\n",
      "0.0655597597360611\n",
      "0.06280346214771271\n",
      "0.013518303632736206\n",
      "0.01644979603588581\n",
      "0.05418530851602554\n",
      "0.06926320493221283\n",
      "0.034097179770469666\n",
      "0.031170258298516273\n",
      "0.04875882714986801\n",
      "0.04330786317586899\n",
      "0.017194997519254684\n",
      "0.017104467377066612\n",
      "0.033105701208114624\n",
      "0.011204305104911327\n",
      "0.011210146360099316\n",
      "0.13215084373950958\n",
      "0.02342222072184086\n",
      "0.040927059948444366\n",
      "0.01911098137497902\n",
      "0.028022537007927895\n",
      "0.0597720742225647\n",
      "0.01378342043608427\n",
      "0.007814863696694374\n",
      "0.015275327488780022\n",
      "0.028753096237778664\n",
      "0.014404689893126488\n",
      "0.0183778814971447\n",
      "0.04258529096841812\n",
      "0.02160903438925743\n",
      "0.02296856790781021\n",
      "0.03975081443786621\n",
      "0.02578502893447876\n",
      "0.027216622605919838\n",
      "0.025043252855539322\n",
      "0.018801014870405197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0180315263569355\n",
      "0.01854337379336357\n",
      "0.013930576853454113\n",
      "0.17552298307418823\n",
      "0.08178885281085968\n",
      "0.017570991069078445\n",
      "0.018812697380781174\n",
      "0.014365171082317829\n",
      "0.019496452063322067\n",
      "0.02607562206685543\n",
      "0.011980978772044182\n",
      "0.027146782726049423\n",
      "0.03129435330629349\n",
      "0.03073784150183201\n",
      "0.03943241387605667\n",
      "0.011925684288144112\n",
      "0.018371786922216415\n",
      "0.023452162742614746\n",
      "0.023237871006131172\n",
      "0.013017970137298107\n",
      "0.03262631967663765\n",
      "0.5680279731750488\n",
      "0.09050224721431732\n",
      "0.02867211028933525\n",
      "0.020644044503569603\n",
      "0.017019031569361687\n",
      "0.009656714275479317\n",
      "0.00796930305659771\n",
      "0.0048257047310471535\n",
      "0.012187579646706581\n",
      "0.005858680699020624\n",
      "0.018924573436379433\n",
      "0.012378862127661705\n",
      "0.008606376126408577\n",
      "0.008068484254181385\n",
      "0.011448360979557037\n",
      "0.010204454883933067\n",
      "0.011423492804169655\n",
      "0.014795228838920593\n",
      "0.0108663160353899\n",
      "0.01352989673614502\n",
      "0.013727346435189247\n",
      "0.012273348867893219\n",
      "0.008723285049200058\n",
      "0.011568639427423477\n",
      "0.027541857212781906\n",
      "0.035224273800849915\n",
      "0.017182353883981705\n",
      "0.030802279710769653\n",
      "0.0463135689496994\n",
      "0.045061469078063965\n",
      "0.012188899330794811\n",
      "0.018247762694954872\n",
      "0.024703238159418106\n",
      "0.02518218196928501\n",
      "0.0393398217856884\n",
      "0.025065023452043533\n",
      "0.02318618819117546\n",
      "0.05152237415313721\n",
      "0.03020157478749752\n",
      "0.05698823928833008\n",
      "0.019113663583993912\n",
      "0.023369228467345238\n",
      "0.02220011316239834\n",
      "0.032041002064943314\n",
      "0.026515116915106773\n",
      "0.028332237154245377\n",
      "0.021408146247267723\n",
      "0.024420225992798805\n",
      "0.01276811957359314\n",
      "0.03205922245979309\n",
      "0.026085499674081802\n",
      "0.054741255939006805\n",
      "0.0495309978723526\n",
      "0.024569686502218246\n",
      "0.030872315168380737\n",
      "0.027307670563459396\n",
      "0.02700415626168251\n",
      "0.024374984204769135\n",
      "0.042794711887836456\n",
      "0.038692183792591095\n",
      "0.034006256610155106\n",
      "0.03918160870671272\n",
      "0.040035221725702286\n",
      "0.04693039506673813\n",
      "0.04446520283818245\n",
      "0.040021881461143494\n",
      "0.025565946474671364\n",
      "0.08088784664869308\n",
      "0.056315042078495026\n",
      "0.6619138121604919\n",
      "0.6000272035598755\n",
      "0.3975380063056946\n",
      "0.14679071307182312\n",
      "0.0373937264084816\n",
      "0.008715751580893993\n",
      "0.010697312653064728\n",
      "0.0100595373660326\n",
      "0.00866184663027525\n",
      "0.012463818304240704\n",
      "0.01871402934193611\n",
      "0.006656921003013849\n",
      "0.009811053052544594\n",
      "0.027606850489974022\n",
      "0.010941706597805023\n",
      "0.022616323083639145\n",
      "0.01070430688560009\n",
      "0.04000265523791313\n",
      "0.01313190720975399\n",
      "0.007155239582061768\n",
      "0.005900322459638119\n",
      "0.007841098122298717\n",
      "0.010381077416241169\n",
      "0.009320402517914772\n",
      "0.011248680762946606\n",
      "0.024489523842930794\n",
      "0.044165708124637604\n",
      "0.015495109371840954\n",
      "0.011078063398599625\n",
      "0.022908266633749008\n",
      "0.025018852204084396\n",
      "0.027827978134155273\n",
      "0.0429425910115242\n",
      "0.007111581973731518\n",
      "0.006054040044546127\n",
      "0.017669280990958214\n",
      "0.010503066703677177\n",
      "0.008402103558182716\n",
      "0.008935120888054371\n",
      "0.023935949429869652\n",
      "0.05494061857461929\n",
      "0.07035376131534576\n",
      "0.060509610921144485\n",
      "0.018222179263830185\n",
      "0.018681935966014862\n",
      "0.019240785390138626\n",
      "0.01925983838737011\n",
      "0.01594792865216732\n",
      "0.01566554419696331\n",
      "0.02434086613357067\n",
      "0.01576460525393486\n",
      "0.015563039109110832\n",
      "0.0187399722635746\n",
      "0.018363144248723984\n",
      "0.018426576629281044\n",
      "0.04250594228506088\n",
      "0.024652251973748207\n",
      "0.022857898846268654\n",
      "0.02264605462551117\n",
      "0.029056638479232788\n",
      "0.024559536948800087\n",
      "0.02114943414926529\n",
      "0.019644808024168015\n",
      "0.02601306140422821\n",
      "0.023672252893447876\n",
      "0.012917766347527504\n",
      "0.024794895201921463\n",
      "0.016286836937069893\n",
      "0.01817847415804863\n",
      "0.015081641264259815\n",
      "0.011900098994374275\n",
      "0.01974624954164028\n",
      "0.12957461178302765\n",
      "0.12139930576086044\n",
      "0.768833577632904\n",
      "0.32968300580978394\n",
      "0.041558802127838135\n",
      "0.01814265549182892\n",
      "0.012414814904332161\n",
      "0.04669036343693733\n",
      "0.026797935366630554\n",
      "0.041470423340797424\n",
      "0.022148847579956055\n",
      "0.018375519663095474\n",
      "0.031418394297361374\n",
      "0.027044307440519333\n",
      "0.011715098284184933\n",
      "0.013821627013385296\n",
      "0.011340649798512459\n",
      "0.013545784167945385\n",
      "0.007722313515841961\n",
      "0.012331869453191757\n",
      "0.01412125863134861\n",
      "0.02342897467315197\n",
      "0.01203529816120863\n",
      "0.022338509559631348\n",
      "0.006971931550651789\n",
      "0.023096296936273575\n",
      "0.03768489137291908\n",
      "0.011198336258530617\n",
      "0.011415136978030205\n",
      "0.005691076163202524\n",
      "0.010921971872448921\n",
      "0.007036538328975439\n",
      "0.00776069238781929\n",
      "0.02774038352072239\n",
      "0.015338274650275707\n",
      "0.03198675066232681\n",
      "0.04164603725075722\n",
      "0.02608538046479225\n",
      "0.010037010535597801\n",
      "0.04021104425191879\n",
      "0.009291018359363079\n",
      "0.010311081074178219\n",
      "0.03889118880033493\n",
      "0.05710594728589058\n",
      "0.01996968872845173\n",
      "0.00946672260761261\n",
      "0.010791261680424213\n",
      "0.017595279961824417\n",
      "0.025116760283708572\n",
      "0.023065978661179543\n",
      "0.017382685095071793\n",
      "0.04722476005554199\n",
      "0.014768938533961773\n",
      "0.01154390163719654\n",
      "0.016943827271461487\n",
      "0.014655714854598045\n",
      "0.016322998329997063\n",
      "0.015146833844482899\n",
      "0.016715500503778458\n",
      "0.01576070301234722\n",
      "0.02021602913737297\n",
      "0.023744842037558556\n",
      "0.018505992367863655\n",
      "0.021732697263360023\n",
      "0.03695632144808769\n",
      "0.016432520002126694\n",
      "0.014572729356586933\n",
      "0.019379466772079468\n",
      "0.02989095449447632\n",
      "0.02909453772008419\n",
      "0.021543387323617935\n",
      "0.03136775642633438\n",
      "0.02330002561211586\n",
      "0.2565714418888092\n",
      "0.03624344244599342\n",
      "0.07196007668972015\n",
      "0.03295866772532463\n",
      "0.020648466423153877\n",
      "0.01870306022465229\n",
      "0.048864495009183884\n",
      "0.05631767585873604\n",
      "0.03902391344308853\n",
      "0.009068687446415424\n",
      "0.019223788753151894\n",
      "0.00994576420634985\n",
      "0.007201500236988068\n",
      "0.016821783035993576\n",
      "0.01685870625078678\n",
      "0.008231285959482193\n",
      "0.01515111792832613\n",
      "0.013442871160805225\n",
      "0.006054379045963287\n",
      "0.02572013810276985\n",
      "0.014830891974270344\n",
      "0.03459620475769043\n",
      "0.012014348991215229\n",
      "0.008752910420298576\n",
      "0.01648024469614029\n",
      "0.009908424690365791\n",
      "0.007574586663395166\n",
      "0.0056555671617388725\n",
      "0.009199535474181175\n",
      "0.01711578294634819\n",
      "0.007013650145381689\n",
      "0.0078611820936203\n",
      "0.009067827835679054\n",
      "0.010142460465431213\n",
      "0.006041061133146286\n",
      "0.008513130247592926\n",
      "0.02130332961678505\n",
      "0.052632175385951996\n",
      "0.04710279032588005\n",
      "0.030742406845092773\n",
      "1.6480824947357178\n",
      "0.39521434903144836\n",
      "1.3136045932769775\n",
      "0.043450597673654556\n",
      "0.01374969445168972\n",
      "0.019614486023783684\n",
      "0.011916123330593109\n",
      "0.04103925824165344\n",
      "0.02190168760716915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [156]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownsize_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     27\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(args_dict)\n\u001b[0;32m---> 28\u001b[0m momo \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [155]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the simple TCN block model\n",
    "\n",
    "train_window = 100\n",
    "pred_seq_len = 20\n",
    "input_size = 3 # FIGURE OUT WTF THIS IS\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": False,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"input_size\": input_size,\n",
    "    \"sequence_len\": train_window,\n",
    "    \"output_size\": pred_seq_len,\n",
    "    \"model\": \"VolForecast\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001, \n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": False,\n",
    "    \"experiment_name\": \"simple_TCN_volatility\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "args.update(args_dict)\n",
    "momo = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "59066ca8",
   "metadata": {
    "id": "59066ca8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "def get_data_single_horizon(file_name, seq_len, horizon, input_fields, output_fields, val_ratio=0.01):\n",
    "    # Get data from csv\n",
    "    df = pd.read_csv(file_name)\n",
    "    inputs = df_to_tensor(df[input_fields])\n",
    "    outputs = df_to_tensor(df[output_fields])\n",
    "    \n",
    "    # Group seq_len concecutive input points together, and horizon concecutive output vectors together\n",
    "    inputs = torch.stack([inputs[i:i - seq_len - horizon] for i in range(seq_len)], dim = 1)\n",
    "    outputs = outputs[seq_len + horizon:]\n",
    "\n",
    "    assert len(inputs) == len(outputs), f\"Input and output arrays have different lengths: {len(inputs)} \\= {len(outputs)}\"\n",
    "    \n",
    "    # Do train/validation split\n",
    "    val_len = math.ceil(val_ratio * len(inputs))\n",
    "    train_x = inputs[:-val_len].transpose(1,2)\n",
    "    train_y = outputs[:-val_len]\n",
    "    val_x = inputs[-val_len:].transpose(1,2)\n",
    "    val_y = outputs[-val_len:]\n",
    "\n",
    "    print(\"Train input dimension:\", train_x.shape)\n",
    "    print(\"Train output dimension:\", train_y.shape)\n",
    "    print(\"Validation input dimension:\", val_x.shape)\n",
    "    print(\"Validation output dimension:\", val_y.shape)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43beaa2a",
   "metadata": {
    "id": "43beaa2a"
   },
   "outputs": [],
   "source": [
    "from pandas.core.dtypes.cast import validate_numeric_casting\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, opts, train_x, train_y, val_x, val_y):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    batch_size = opts['batch_size']\n",
    "    validation_period = opts['validation_period']\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(opts['nepochs']):\n",
    "        # Sample batch input randomly\n",
    "        random_ids = np.random.randint(len(train_x), size=batch_size)\n",
    "        train_input = train_x[random_ids]\n",
    "        train_output = train_y[random_ids]        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(train_input), train_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += [loss]\n",
    "\n",
    "        if (epoch+1) % validation_period == 0:\n",
    "\n",
    "            val_loss = loss_fn(model(val_x), val_y)\n",
    "            val_losses += [val_loss]\n",
    "  \n",
    "            print(f\"Epoch: {epoch + 1}, training loss: {loss}, validation loss: {val_loss}\")\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch + 1}, training loss: {loss}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), './model')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba2eb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.dtypes.cast import validate_numeric_casting\n",
    "import torch.optim as optim\n",
    "\n",
    "def train2(model, opts):\n",
    "    loss_function = nn.MSELoss()\n",
    "    batch_size = opts['batch_size']\n",
    "    epochs = opts['nepochs']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for seq, labels in train_sequence:\n",
    "            optimizer.zero_grad()\n",
    "#             model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "#                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%25 == 1:\n",
    "            print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f611a88c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [25, 3, 7], but got 2-dimensional input of size [100, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VolForecast(seq_len\u001b[38;5;241m=\u001b[39mtrain_window ,input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, output_size\u001b[38;5;241m=\u001b[39mpred_seq_len)\n\u001b[1;32m      3\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_period\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36mtrain2\u001b[0;34m(model, opts)\u001b[0m\n\u001b[1;32m     12\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#             model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#                             torch.zeros(1, 1, model.hidden_layer_size))\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m             single_loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, labels)\n\u001b[1;32m     19\u001b[0m             single_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mVolForecast.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124;03m\"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# input should have dimension (N, C, L)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(y1[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mTemporalConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:301\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:297\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    295\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    296\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [25, 3, 7], but got 2-dimensional input of size [100, 3] instead"
     ]
    }
   ],
   "source": [
    "model = VolForecast(seq_len=train_window ,input_size=3, output_size=pred_seq_len)\n",
    "\n",
    "opts = {\n",
    "    \"nepochs\": 500,\n",
    "    \"batch_size\": 64,\n",
    "    \"validation_period\": 1000\n",
    "}\n",
    "\n",
    "train2(model, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "-ezI05mFiEiH",
   "metadata": {
    "id": "-ezI05mFiEiH"
   },
   "outputs": [],
   "source": [
    "def plot_model_performance_single_horizon(model, x, y, num_samples, horizon):\n",
    "  assert num_samples < len(x), f\"Too many samples: {num_samples}. Input size is only {len(x)}\"\n",
    "  pred = model(x[:num_samples])\n",
    "  reality = y[horizon:num_samples+horizon]\n",
    "\n",
    "  plt.plot(pred.detach().numpy(), 'r')\n",
    "  plt.plot(reality.detach().numpy(), 'b')\n",
    "\n",
    "# def plot_model_performance_multiple_horizons(model, x, y, num_samples, horizon):\n",
    "#   assert num_samples < len(x), f\"Too many samples: {num_samples}. Input size is only {len(x)}\"\n",
    "#   pred = model(x[:num_samples])[:, horizon-1]\n",
    "#   reality = y[:num_samples][:, horizon-1]\n",
    "\n",
    "#   plt.plot(pred.detach().numpy(), 'r')\n",
    "#   plt.plot(reality.detach().numpy(), 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92af5f4",
   "metadata": {
    "id": "c92af5f4"
   },
   "outputs": [],
   "source": [
    "## Setting some params\n",
    "model=None\n",
    "seq_len=300\n",
    "horizon=50\n",
    "\n",
    "input_fields = ['PRICE', 'SIZE', 'vol']\n",
    "output_fields = ['vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "FF9Hkq3BHzPq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF9Hkq3BHzPq",
    "outputId": "2fb1d515-9885-4242-96fb-d088e631b288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input dimension: torch.Size([571369, 3, 300])\n",
      "Train output dimension: torch.Size([571369, 1])\n",
      "Validation input dimension: torch.Size([5772, 3, 300])\n",
      "Validation output dimension: torch.Size([5772, 1])\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_data_single_horizon('5sec_intervals_no_na_normed.csv', seq_len=seq_len, horizon=horizon, \n",
    "                                                         input_fields=input_fields, output_fields=output_fields, val_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3HnEgpbzWM0S",
   "metadata": {
    "id": "3HnEgpbzWM0S"
   },
   "outputs": [],
   "source": [
    "model=VolForecast(input_size=len(input_fields), seq_len=seq_len, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fdd7646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fdd7646",
    "outputId": "60bced18-0bde-420d-c7b7-37430b5fcaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training loss: 0.46561405062675476\n",
      "Epoch: 2, training loss: 27.542495727539062\n",
      "Epoch: 3, training loss: 1.6271908283233643\n",
      "Epoch: 4, training loss: 1.0528066158294678\n",
      "Epoch: 5, training loss: 1.025953769683838\n",
      "Epoch: 6, training loss: 1.0483522415161133\n",
      "Epoch: 7, training loss: 0.7423053979873657\n",
      "Epoch: 8, training loss: 2.9472732543945312\n",
      "Epoch: 9, training loss: 0.7138708829879761\n",
      "Epoch: 10, training loss: 0.6462883353233337\n",
      "Epoch: 11, training loss: 0.9674425721168518\n",
      "Epoch: 12, training loss: 0.4953930377960205\n",
      "Epoch: 13, training loss: 0.38409215211868286\n",
      "Epoch: 14, training loss: 0.709173858165741\n",
      "Epoch: 15, training loss: 0.5367158651351929\n",
      "Epoch: 16, training loss: 0.33580198884010315\n",
      "Epoch: 17, training loss: 0.36139002442359924\n",
      "Epoch: 18, training loss: 0.6032704710960388\n",
      "Epoch: 19, training loss: 0.3713749051094055\n",
      "Epoch: 20, training loss: 0.33543550968170166\n",
      "Epoch: 21, training loss: 0.2532128691673279\n",
      "Epoch: 22, training loss: 0.1741548329591751\n",
      "Epoch: 23, training loss: 4.435169219970703\n",
      "Epoch: 24, training loss: 0.6078352332115173\n",
      "Epoch: 25, training loss: 0.20390884578227997\n",
      "Epoch: 26, training loss: 0.18510887026786804\n",
      "Epoch: 27, training loss: 5.682523727416992\n",
      "Epoch: 28, training loss: 1.7774733304977417\n",
      "Epoch: 29, training loss: 0.3913659453392029\n",
      "Epoch: 30, training loss: 0.2191825658082962\n",
      "Epoch: 31, training loss: 0.38581082224845886\n",
      "Epoch: 32, training loss: 0.23097582161426544\n",
      "Epoch: 33, training loss: 0.7511200904846191\n",
      "Epoch: 34, training loss: 0.126680389046669\n",
      "Epoch: 35, training loss: 0.08390839397907257\n",
      "Epoch: 36, training loss: 0.2913079261779785\n",
      "Epoch: 37, training loss: 0.12091932445764542\n",
      "Epoch: 38, training loss: 0.7829373478889465\n",
      "Epoch: 39, training loss: 0.07712384313344955\n",
      "Epoch: 40, training loss: 0.08379188179969788\n",
      "Epoch: 41, training loss: 0.09326861053705215\n",
      "Epoch: 42, training loss: 3.1920151710510254\n",
      "Epoch: 43, training loss: 0.29566383361816406\n",
      "Epoch: 44, training loss: 0.07314730435609818\n",
      "Epoch: 45, training loss: 0.06096590310335159\n",
      "Epoch: 46, training loss: 0.5201666355133057\n",
      "Epoch: 47, training loss: 0.0682658776640892\n",
      "Epoch: 48, training loss: 0.10766493529081345\n",
      "Epoch: 49, training loss: 0.4675038158893585\n",
      "Epoch: 50, training loss: 0.1429552435874939\n",
      "Epoch: 51, training loss: 5.534708499908447\n",
      "Epoch: 52, training loss: 0.21928417682647705\n",
      "Epoch: 53, training loss: 0.29886394739151\n",
      "Epoch: 54, training loss: 0.8484241962432861\n",
      "Epoch: 55, training loss: 0.13370144367218018\n",
      "Epoch: 56, training loss: 0.15529850125312805\n",
      "Epoch: 57, training loss: 0.4572778642177582\n",
      "Epoch: 58, training loss: 0.14309465885162354\n",
      "Epoch: 59, training loss: 0.49883565306663513\n",
      "Epoch: 60, training loss: 0.2252621352672577\n",
      "Epoch: 61, training loss: 0.6296305656433105\n",
      "Epoch: 62, training loss: 0.1854623556137085\n",
      "Epoch: 63, training loss: 0.07468906790018082\n",
      "Epoch: 64, training loss: 0.16379272937774658\n",
      "Epoch: 65, training loss: 0.7838847637176514\n",
      "Epoch: 66, training loss: 0.14656321704387665\n",
      "Epoch: 67, training loss: 1.5305639505386353\n",
      "Epoch: 68, training loss: 1.0517387390136719\n",
      "Epoch: 69, training loss: 0.12495492398738861\n",
      "Epoch: 70, training loss: 0.195842444896698\n",
      "Epoch: 71, training loss: 0.09075020998716354\n",
      "Epoch: 72, training loss: 0.18026623129844666\n",
      "Epoch: 73, training loss: 0.0711953267455101\n",
      "Epoch: 74, training loss: 0.08464875817298889\n",
      "Epoch: 75, training loss: 0.17731615900993347\n",
      "Epoch: 76, training loss: 0.5646161437034607\n",
      "Epoch: 77, training loss: 0.5227370858192444\n",
      "Epoch: 78, training loss: 0.14011099934577942\n",
      "Epoch: 79, training loss: 0.16843315958976746\n",
      "Epoch: 80, training loss: 0.07855797559022903\n",
      "Epoch: 81, training loss: 0.12004020810127258\n",
      "Epoch: 82, training loss: 0.25488799810409546\n",
      "Epoch: 83, training loss: 0.09336954355239868\n",
      "Epoch: 84, training loss: 0.1271720677614212\n",
      "Epoch: 85, training loss: 0.7582966685295105\n",
      "Epoch: 86, training loss: 0.10394079983234406\n",
      "Epoch: 87, training loss: 2.5516765117645264\n",
      "Epoch: 88, training loss: 0.16571475565433502\n",
      "Epoch: 89, training loss: 0.10104309767484665\n",
      "Epoch: 90, training loss: 0.2666476368904114\n",
      "Epoch: 91, training loss: 0.06868147850036621\n",
      "Epoch: 92, training loss: 0.12414814531803131\n",
      "Epoch: 93, training loss: 0.20783445239067078\n",
      "Epoch: 94, training loss: 0.09340827912092209\n",
      "Epoch: 95, training loss: 0.1435198038816452\n",
      "Epoch: 96, training loss: 0.24527432024478912\n",
      "Epoch: 97, training loss: 0.2950323522090912\n",
      "Epoch: 98, training loss: 0.0839238166809082\n",
      "Epoch: 99, training loss: 0.10003750026226044\n",
      "Epoch: 100, training loss: 0.09242231398820877\n",
      "Epoch: 101, training loss: 0.11065267771482468\n",
      "Epoch: 102, training loss: 0.9720754623413086\n",
      "Epoch: 103, training loss: 0.06764274090528488\n",
      "Epoch: 104, training loss: 0.053170785307884216\n",
      "Epoch: 105, training loss: 0.13624146580696106\n",
      "Epoch: 106, training loss: 0.08531311899423599\n",
      "Epoch: 107, training loss: 0.09852835536003113\n",
      "Epoch: 108, training loss: 0.4816123843193054\n",
      "Epoch: 109, training loss: 0.1373588591814041\n",
      "Epoch: 110, training loss: 5.55129337310791\n",
      "Epoch: 111, training loss: 0.23006220161914825\n",
      "Epoch: 112, training loss: 0.11490435898303986\n",
      "Epoch: 113, training loss: 0.3565128743648529\n",
      "Epoch: 114, training loss: 0.13845999538898468\n",
      "Epoch: 115, training loss: 0.1729278564453125\n",
      "Epoch: 116, training loss: 0.057700879871845245\n",
      "Epoch: 117, training loss: 0.3934547007083893\n",
      "Epoch: 118, training loss: 0.17586255073547363\n",
      "Epoch: 119, training loss: 0.08026110380887985\n",
      "Epoch: 120, training loss: 0.0790509283542633\n",
      "Epoch: 121, training loss: 0.1254872977733612\n",
      "Epoch: 122, training loss: 0.3955095112323761\n",
      "Epoch: 123, training loss: 4.57232666015625\n",
      "Epoch: 124, training loss: 0.160858154296875\n",
      "Epoch: 125, training loss: 0.06928961724042892\n",
      "Epoch: 126, training loss: 0.11537569761276245\n",
      "Epoch: 127, training loss: 0.0830245241522789\n",
      "Epoch: 128, training loss: 0.07878699153661728\n",
      "Epoch: 129, training loss: 0.09684856235980988\n",
      "Epoch: 130, training loss: 0.195005863904953\n",
      "Epoch: 131, training loss: 0.2954411208629608\n",
      "Epoch: 132, training loss: 0.07863832265138626\n",
      "Epoch: 133, training loss: 0.15245534479618073\n",
      "Epoch: 134, training loss: 0.31232741475105286\n",
      "Epoch: 135, training loss: 0.05255930498242378\n",
      "Epoch: 136, training loss: 0.10739535093307495\n",
      "Epoch: 137, training loss: 0.05278311297297478\n",
      "Epoch: 138, training loss: 0.05712609738111496\n",
      "Epoch: 139, training loss: 0.6540212035179138\n",
      "Epoch: 140, training loss: 0.15676259994506836\n",
      "Epoch: 141, training loss: 1.6226269006729126\n",
      "Epoch: 142, training loss: 2.4171464443206787\n",
      "Epoch: 143, training loss: 0.24969443678855896\n",
      "Epoch: 144, training loss: 0.16901865601539612\n",
      "Epoch: 145, training loss: 0.1311585158109665\n",
      "Epoch: 146, training loss: 0.18667727708816528\n",
      "Epoch: 147, training loss: 0.16504217684268951\n",
      "Epoch: 148, training loss: 0.3852460980415344\n",
      "Epoch: 149, training loss: 0.09061222523450851\n",
      "Epoch: 150, training loss: 0.29702627658843994\n",
      "Epoch: 151, training loss: 0.5121480226516724\n",
      "Epoch: 152, training loss: 0.1300109624862671\n",
      "Epoch: 153, training loss: 0.26961904764175415\n",
      "Epoch: 154, training loss: 0.07394123077392578\n",
      "Epoch: 155, training loss: 0.17350172996520996\n",
      "Epoch: 156, training loss: 0.1270710825920105\n",
      "Epoch: 157, training loss: 0.07703882455825806\n",
      "Epoch: 158, training loss: 1.5815277099609375\n",
      "Epoch: 159, training loss: 0.09069671481847763\n",
      "Epoch: 160, training loss: 0.11130785197019577\n",
      "Epoch: 161, training loss: 0.12744615972042084\n",
      "Epoch: 162, training loss: 0.17097172141075134\n",
      "Epoch: 163, training loss: 2.559480667114258\n",
      "Epoch: 164, training loss: 0.13504913449287415\n",
      "Epoch: 165, training loss: 0.04741667956113815\n",
      "Epoch: 166, training loss: 0.47110646963119507\n",
      "Epoch: 167, training loss: 0.38587287068367004\n",
      "Epoch: 168, training loss: 0.09795990586280823\n",
      "Epoch: 169, training loss: 0.1422785222530365\n",
      "Epoch: 170, training loss: 0.2775469422340393\n",
      "Epoch: 171, training loss: 0.13908107578754425\n",
      "Epoch: 172, training loss: 0.35533416271209717\n",
      "Epoch: 173, training loss: 0.3466435372829437\n",
      "Epoch: 174, training loss: 0.19812113046646118\n",
      "Epoch: 175, training loss: 0.26110178232192993\n",
      "Epoch: 176, training loss: 0.2963944673538208\n",
      "Epoch: 177, training loss: 0.18483828008174896\n",
      "Epoch: 178, training loss: 0.13409942388534546\n",
      "Epoch: 179, training loss: 0.0797477588057518\n",
      "Epoch: 180, training loss: 0.07768463343381882\n",
      "Epoch: 181, training loss: 0.07494543492794037\n",
      "Epoch: 182, training loss: 0.13945385813713074\n",
      "Epoch: 183, training loss: 0.1747528314590454\n",
      "Epoch: 184, training loss: 0.33397674560546875\n",
      "Epoch: 185, training loss: 2.1736066341400146\n",
      "Epoch: 186, training loss: 0.04888511821627617\n",
      "Epoch: 187, training loss: 0.09872960299253464\n",
      "Epoch: 188, training loss: 0.3389126658439636\n",
      "Epoch: 189, training loss: 5.610671043395996\n",
      "Epoch: 190, training loss: 0.11820021271705627\n",
      "Epoch: 191, training loss: 0.05250875651836395\n",
      "Epoch: 192, training loss: 0.21061229705810547\n",
      "Epoch: 193, training loss: 0.28830206394195557\n",
      "Epoch: 194, training loss: 0.3822151720523834\n",
      "Epoch: 195, training loss: 0.16593095660209656\n",
      "Epoch: 196, training loss: 0.15690791606903076\n",
      "Epoch: 197, training loss: 0.17669619619846344\n",
      "Epoch: 198, training loss: 0.26011162996292114\n",
      "Epoch: 199, training loss: 0.10714152455329895\n",
      "Epoch: 200, training loss: 0.1387556791305542\n",
      "Epoch: 201, training loss: 0.0752098485827446\n",
      "Epoch: 202, training loss: 1.4057034254074097\n",
      "Epoch: 203, training loss: 0.08482461422681808\n",
      "Epoch: 204, training loss: 0.056001074612140656\n",
      "Epoch: 205, training loss: 0.11079628020524979\n",
      "Epoch: 206, training loss: 0.15171180665493011\n",
      "Epoch: 207, training loss: 0.1316787451505661\n",
      "Epoch: 208, training loss: 0.08852261304855347\n",
      "Epoch: 209, training loss: 0.24508258700370789\n",
      "Epoch: 210, training loss: 0.08481714874505997\n",
      "Epoch: 211, training loss: 0.17452189326286316\n",
      "Epoch: 212, training loss: 0.44897493720054626\n",
      "Epoch: 213, training loss: 0.1364758014678955\n",
      "Epoch: 214, training loss: 1.9473930597305298\n",
      "Epoch: 215, training loss: 0.4146419167518616\n",
      "Epoch: 216, training loss: 0.12527857720851898\n",
      "Epoch: 217, training loss: 0.054529834538698196\n",
      "Epoch: 218, training loss: 0.27069926261901855\n",
      "Epoch: 219, training loss: 0.04234786704182625\n",
      "Epoch: 220, training loss: 0.1874687373638153\n",
      "Epoch: 221, training loss: 0.07278753817081451\n",
      "Epoch: 222, training loss: 0.10786624997854233\n",
      "Epoch: 223, training loss: 0.07676036655902863\n",
      "Epoch: 224, training loss: 0.04993350803852081\n",
      "Epoch: 225, training loss: 0.09025383740663528\n",
      "Epoch: 226, training loss: 0.08432899415493011\n",
      "Epoch: 227, training loss: 0.10179152339696884\n",
      "Epoch: 228, training loss: 0.22134409844875336\n",
      "Epoch: 229, training loss: 0.8762494325637817\n",
      "Epoch: 230, training loss: 0.2541205585002899\n",
      "Epoch: 231, training loss: 0.47225329279899597\n",
      "Epoch: 232, training loss: 0.08464720100164413\n",
      "Epoch: 233, training loss: 0.2213461995124817\n",
      "Epoch: 234, training loss: 0.06904902309179306\n",
      "Epoch: 235, training loss: 0.23163050413131714\n",
      "Epoch: 236, training loss: 0.6879942417144775\n",
      "Epoch: 237, training loss: 0.08348056674003601\n",
      "Epoch: 238, training loss: 0.08836539089679718\n",
      "Epoch: 239, training loss: 0.15144386887550354\n",
      "Epoch: 240, training loss: 0.05764300376176834\n",
      "Epoch: 241, training loss: 0.09804162383079529\n",
      "Epoch: 242, training loss: 0.29769575595855713\n",
      "Epoch: 243, training loss: 0.4516616463661194\n",
      "Epoch: 244, training loss: 0.16396868228912354\n",
      "Epoch: 245, training loss: 1.1308238506317139\n",
      "Epoch: 246, training loss: 4.053860187530518\n",
      "Epoch: 247, training loss: 0.15051084756851196\n",
      "Epoch: 248, training loss: 0.1339779496192932\n",
      "Epoch: 249, training loss: 0.0820818617939949\n",
      "Epoch: 250, training loss: 0.7203816771507263\n",
      "Epoch: 251, training loss: 0.11579763144254684\n",
      "Epoch: 252, training loss: 0.11966012418270111\n",
      "Epoch: 253, training loss: 0.14305950701236725\n",
      "Epoch: 254, training loss: 0.25643378496170044\n",
      "Epoch: 255, training loss: 0.20718815922737122\n",
      "Epoch: 256, training loss: 0.2377321720123291\n",
      "Epoch: 257, training loss: 0.1795622855424881\n",
      "Epoch: 258, training loss: 0.5653698444366455\n",
      "Epoch: 259, training loss: 0.2255437970161438\n",
      "Epoch: 260, training loss: 0.07356668263673782\n",
      "Epoch: 261, training loss: 0.15401512384414673\n",
      "Epoch: 262, training loss: 0.5527581572532654\n",
      "Epoch: 263, training loss: 0.31762370467185974\n",
      "Epoch: 264, training loss: 0.8289601802825928\n",
      "Epoch: 265, training loss: 0.23188279569149017\n",
      "Epoch: 266, training loss: 0.2420046478509903\n",
      "Epoch: 267, training loss: 0.26641809940338135\n",
      "Epoch: 268, training loss: 0.4657323658466339\n",
      "Epoch: 269, training loss: 0.28542643785476685\n",
      "Epoch: 270, training loss: 0.06923682242631912\n",
      "Epoch: 271, training loss: 0.06812634319067001\n",
      "Epoch: 272, training loss: 0.1078680232167244\n",
      "Epoch: 273, training loss: 0.10768970847129822\n",
      "Epoch: 274, training loss: 0.07935766875743866\n",
      "Epoch: 275, training loss: 0.0733606219291687\n",
      "Epoch: 276, training loss: 0.04828815534710884\n",
      "Epoch: 277, training loss: 0.09025392681360245\n",
      "Epoch: 278, training loss: 0.23553813993930817\n",
      "Epoch: 279, training loss: 0.15259264409542084\n",
      "Epoch: 280, training loss: 0.0934421718120575\n",
      "Epoch: 281, training loss: 0.15772944688796997\n",
      "Epoch: 282, training loss: 0.6741044521331787\n",
      "Epoch: 283, training loss: 0.12019556760787964\n",
      "Epoch: 284, training loss: 0.07713919132947922\n",
      "Epoch: 285, training loss: 0.10610951483249664\n",
      "Epoch: 286, training loss: 0.21794188022613525\n",
      "Epoch: 287, training loss: 0.12135372310876846\n",
      "Epoch: 288, training loss: 0.19194406270980835\n",
      "Epoch: 289, training loss: 0.23220564424991608\n",
      "Epoch: 290, training loss: 0.07408822327852249\n",
      "Epoch: 291, training loss: 0.12075336277484894\n",
      "Epoch: 292, training loss: 0.3716579079627991\n",
      "Epoch: 293, training loss: 0.17469248175621033\n",
      "Epoch: 294, training loss: 0.2384977489709854\n",
      "Epoch: 295, training loss: 0.07199640572071075\n",
      "Epoch: 296, training loss: 0.10202385485172272\n",
      "Epoch: 297, training loss: 0.6490597128868103\n",
      "Epoch: 298, training loss: 0.5129608511924744\n",
      "Epoch: 299, training loss: 0.09303776919841766\n",
      "Epoch: 300, training loss: 0.3607252240180969\n",
      "Epoch: 301, training loss: 0.10030718147754669\n",
      "Epoch: 302, training loss: 0.08526802808046341\n",
      "Epoch: 303, training loss: 0.13073696196079254\n",
      "Epoch: 304, training loss: 0.08991178870201111\n",
      "Epoch: 305, training loss: 4.505570888519287\n",
      "Epoch: 306, training loss: 0.06305192410945892\n",
      "Epoch: 307, training loss: 0.07716742157936096\n",
      "Epoch: 308, training loss: 0.03519056737422943\n",
      "Epoch: 309, training loss: 0.03463117405772209\n",
      "Epoch: 310, training loss: 0.09684807807207108\n",
      "Epoch: 311, training loss: 0.25456154346466064\n",
      "Epoch: 312, training loss: 0.03670688718557358\n",
      "Epoch: 313, training loss: 0.06673548370599747\n",
      "Epoch: 314, training loss: 0.5124513506889343\n",
      "Epoch: 315, training loss: 0.20055446028709412\n",
      "Epoch: 316, training loss: 0.07090417295694351\n",
      "Epoch: 317, training loss: 0.15498298406600952\n",
      "Epoch: 318, training loss: 0.16979369521141052\n",
      "Epoch: 319, training loss: 0.20266984403133392\n",
      "Epoch: 320, training loss: 0.518804669380188\n",
      "Epoch: 321, training loss: 0.10749191790819168\n",
      "Epoch: 322, training loss: 0.03315068036317825\n",
      "Epoch: 323, training loss: 0.37652987241744995\n",
      "Epoch: 324, training loss: 0.047988198697566986\n",
      "Epoch: 325, training loss: 0.059008777141571045\n",
      "Epoch: 326, training loss: 0.6119340062141418\n",
      "Epoch: 327, training loss: 0.07608215510845184\n",
      "Epoch: 328, training loss: 0.18514180183410645\n",
      "Epoch: 329, training loss: 0.11604203283786774\n",
      "Epoch: 330, training loss: 0.31738999485969543\n",
      "Epoch: 331, training loss: 0.0726759061217308\n",
      "Epoch: 332, training loss: 0.0697375163435936\n",
      "Epoch: 333, training loss: 0.044853001832962036\n",
      "Epoch: 334, training loss: 0.07028964161872864\n",
      "Epoch: 335, training loss: 0.09521182626485825\n",
      "Epoch: 336, training loss: 0.05709560215473175\n",
      "Epoch: 337, training loss: 0.08317390084266663\n",
      "Epoch: 338, training loss: 0.12291024625301361\n",
      "Epoch: 339, training loss: 0.08738045394420624\n",
      "Epoch: 340, training loss: 0.11038200557231903\n",
      "Epoch: 341, training loss: 0.072688527405262\n",
      "Epoch: 342, training loss: 0.11536748707294464\n",
      "Epoch: 343, training loss: 0.17682939767837524\n",
      "Epoch: 344, training loss: 0.24863815307617188\n",
      "Epoch: 345, training loss: 0.5289267897605896\n",
      "Epoch: 346, training loss: 0.6917366981506348\n",
      "Epoch: 347, training loss: 0.09333811700344086\n",
      "Epoch: 348, training loss: 0.13158617913722992\n",
      "Epoch: 349, training loss: 0.17229190468788147\n",
      "Epoch: 350, training loss: 0.19494719803333282\n",
      "Epoch: 351, training loss: 0.09080549329519272\n",
      "Epoch: 352, training loss: 0.13876067101955414\n",
      "Epoch: 353, training loss: 0.048503730446100235\n",
      "Epoch: 354, training loss: 0.2746855616569519\n",
      "Epoch: 355, training loss: 0.03034328669309616\n",
      "Epoch: 356, training loss: 0.09106789529323578\n",
      "Epoch: 357, training loss: 0.2102925032377243\n",
      "Epoch: 358, training loss: 0.049072954803705215\n",
      "Epoch: 359, training loss: 0.7362197041511536\n",
      "Epoch: 360, training loss: 0.07687512040138245\n",
      "Epoch: 361, training loss: 0.086698979139328\n",
      "Epoch: 362, training loss: 0.07978229224681854\n",
      "Epoch: 363, training loss: 0.04660140722990036\n",
      "Epoch: 364, training loss: 0.06152794510126114\n",
      "Epoch: 365, training loss: 0.10761015117168427\n",
      "Epoch: 366, training loss: 0.11504687368869781\n",
      "Epoch: 367, training loss: 0.04760265350341797\n",
      "Epoch: 368, training loss: 0.09534527361392975\n",
      "Epoch: 369, training loss: 0.1952766478061676\n",
      "Epoch: 370, training loss: 0.06952712684869766\n",
      "Epoch: 371, training loss: 0.040168646723032\n",
      "Epoch: 372, training loss: 0.14904442429542542\n",
      "Epoch: 373, training loss: 0.2951188087463379\n",
      "Epoch: 374, training loss: 0.13306695222854614\n",
      "Epoch: 375, training loss: 0.0951947271823883\n",
      "Epoch: 376, training loss: 0.2512591481208801\n",
      "Epoch: 377, training loss: 0.10415713489055634\n",
      "Epoch: 378, training loss: 0.047799840569496155\n",
      "Epoch: 379, training loss: 0.6672038435935974\n",
      "Epoch: 380, training loss: 0.10848033428192139\n",
      "Epoch: 381, training loss: 0.08065284788608551\n",
      "Epoch: 382, training loss: 0.379685640335083\n",
      "Epoch: 383, training loss: 0.0528571791946888\n",
      "Epoch: 384, training loss: 0.024198345839977264\n",
      "Epoch: 385, training loss: 0.13475622236728668\n",
      "Epoch: 386, training loss: 0.06515567004680634\n",
      "Epoch: 387, training loss: 0.19251376390457153\n",
      "Epoch: 388, training loss: 0.4129074215888977\n",
      "Epoch: 389, training loss: 0.3429100215435028\n",
      "Epoch: 390, training loss: 0.13896825909614563\n",
      "Epoch: 391, training loss: 0.3202846050262451\n",
      "Epoch: 392, training loss: 0.062141042202711105\n",
      "Epoch: 393, training loss: 0.0763448029756546\n",
      "Epoch: 394, training loss: 0.08825436979532242\n",
      "Epoch: 395, training loss: 0.12064336240291595\n",
      "Epoch: 396, training loss: 0.06004991754889488\n",
      "Epoch: 397, training loss: 0.14529924094676971\n",
      "Epoch: 398, training loss: 0.1125369668006897\n",
      "Epoch: 399, training loss: 0.3969932794570923\n",
      "Epoch: 400, training loss: 0.04229513183236122\n",
      "Epoch: 401, training loss: 0.2721157670021057\n",
      "Epoch: 402, training loss: 0.09838544577360153\n",
      "Epoch: 403, training loss: 0.03741796687245369\n",
      "Epoch: 404, training loss: 0.057031773030757904\n",
      "Epoch: 405, training loss: 0.3349054157733917\n",
      "Epoch: 406, training loss: 0.04114537686109543\n",
      "Epoch: 407, training loss: 0.07440182566642761\n",
      "Epoch: 408, training loss: 0.17026954889297485\n",
      "Epoch: 409, training loss: 0.18827342987060547\n",
      "Epoch: 410, training loss: 0.06181386858224869\n",
      "Epoch: 411, training loss: 0.46281713247299194\n",
      "Epoch: 412, training loss: 0.055899329483509064\n",
      "Epoch: 413, training loss: 0.25095149874687195\n",
      "Epoch: 414, training loss: 0.10210791230201721\n",
      "Epoch: 415, training loss: 0.1836220920085907\n",
      "Epoch: 416, training loss: 0.3526853621006012\n",
      "Epoch: 417, training loss: 0.6107417345046997\n",
      "Epoch: 418, training loss: 0.35425519943237305\n",
      "Epoch: 419, training loss: 0.11042250692844391\n",
      "Epoch: 420, training loss: 0.06785240769386292\n",
      "Epoch: 421, training loss: 4.0235395431518555\n",
      "Epoch: 422, training loss: 0.08757780492305756\n",
      "Epoch: 423, training loss: 0.09990069270133972\n",
      "Epoch: 424, training loss: 0.0743749588727951\n",
      "Epoch: 425, training loss: 0.03658716008067131\n",
      "Epoch: 426, training loss: 0.23554487526416779\n",
      "Epoch: 427, training loss: 0.4002240300178528\n",
      "Epoch: 428, training loss: 0.186665341258049\n",
      "Epoch: 429, training loss: 0.10984648764133453\n",
      "Epoch: 430, training loss: 0.7142040729522705\n",
      "Epoch: 431, training loss: 0.07504639774560928\n",
      "Epoch: 432, training loss: 0.05134989321231842\n",
      "Epoch: 433, training loss: 0.09002310782670975\n",
      "Epoch: 434, training loss: 0.26169952750205994\n",
      "Epoch: 435, training loss: 0.42651498317718506\n",
      "Epoch: 436, training loss: 0.050887856632471085\n",
      "Epoch: 437, training loss: 0.051806528121232986\n",
      "Epoch: 438, training loss: 0.06185908988118172\n",
      "Epoch: 439, training loss: 0.06359659135341644\n",
      "Epoch: 440, training loss: 0.07721791416406631\n",
      "Epoch: 441, training loss: 0.06168461591005325\n",
      "Epoch: 442, training loss: 0.4000788629055023\n",
      "Epoch: 443, training loss: 0.27516502141952515\n",
      "Epoch: 444, training loss: 0.04287024214863777\n",
      "Epoch: 445, training loss: 0.055928874760866165\n",
      "Epoch: 446, training loss: 0.1513315886259079\n",
      "Epoch: 447, training loss: 0.3801460564136505\n",
      "Epoch: 448, training loss: 0.14345237612724304\n",
      "Epoch: 449, training loss: 0.06080346181988716\n",
      "Epoch: 450, training loss: 0.07015808671712875\n",
      "Epoch: 451, training loss: 0.9612947702407837\n",
      "Epoch: 452, training loss: 0.05560192093253136\n",
      "Epoch: 453, training loss: 0.810935378074646\n",
      "Epoch: 454, training loss: 0.09310847520828247\n",
      "Epoch: 455, training loss: 0.12706944346427917\n",
      "Epoch: 456, training loss: 0.06292854249477386\n",
      "Epoch: 457, training loss: 0.11040899902582169\n",
      "Epoch: 458, training loss: 0.16763268411159515\n",
      "Epoch: 459, training loss: 0.21182624995708466\n",
      "Epoch: 460, training loss: 0.17863604426383972\n",
      "Epoch: 461, training loss: 0.3062433898448944\n",
      "Epoch: 462, training loss: 0.148667573928833\n",
      "Epoch: 463, training loss: 0.04766349121928215\n",
      "Epoch: 464, training loss: 0.08134592324495316\n",
      "Epoch: 465, training loss: 0.10428410023450851\n",
      "Epoch: 466, training loss: 0.10173143446445465\n",
      "Epoch: 467, training loss: 0.23791316151618958\n",
      "Epoch: 468, training loss: 0.0700022280216217\n",
      "Epoch: 469, training loss: 0.08740268647670746\n",
      "Epoch: 470, training loss: 0.1447218507528305\n",
      "Epoch: 471, training loss: 0.21700115501880646\n",
      "Epoch: 472, training loss: 0.2514016032218933\n",
      "Epoch: 473, training loss: 0.3755030333995819\n",
      "Epoch: 474, training loss: 0.17350947856903076\n",
      "Epoch: 475, training loss: 0.1602281928062439\n",
      "Epoch: 476, training loss: 0.06568333506584167\n",
      "Epoch: 477, training loss: 0.036224111914634705\n",
      "Epoch: 478, training loss: 0.31379711627960205\n",
      "Epoch: 479, training loss: 3.6934356689453125\n",
      "Epoch: 480, training loss: 0.14180076122283936\n",
      "Epoch: 481, training loss: 0.1506337821483612\n",
      "Epoch: 482, training loss: 0.11462365090847015\n",
      "Epoch: 483, training loss: 0.20598673820495605\n",
      "Epoch: 484, training loss: 0.38181132078170776\n",
      "Epoch: 485, training loss: 0.1287510097026825\n",
      "Epoch: 486, training loss: 0.3601471185684204\n",
      "Epoch: 487, training loss: 0.11636590957641602\n",
      "Epoch: 488, training loss: 0.08957234770059586\n",
      "Epoch: 489, training loss: 0.18512536585330963\n",
      "Epoch: 490, training loss: 0.04993700981140137\n",
      "Epoch: 491, training loss: 0.09560976922512054\n",
      "Epoch: 492, training loss: 0.31098610162734985\n",
      "Epoch: 493, training loss: 0.098048634827137\n",
      "Epoch: 494, training loss: 0.03443076089024544\n",
      "Epoch: 495, training loss: 0.0513182058930397\n",
      "Epoch: 496, training loss: 0.09055103361606598\n",
      "Epoch: 497, training loss: 0.07104504853487015\n",
      "Epoch: 498, training loss: 0.251727819442749\n",
      "Epoch: 499, training loss: 0.0629984438419342\n",
      "Epoch: 500, training loss: 0.413496196269989\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "opts = {\n",
    "    \"nepochs\": 500,\n",
    "    \"batch_size\": 64,\n",
    "    \"validation_period\": 1000\n",
    "}\n",
    "train_losses, val_losses = train(model, opts, train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a1bf7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "52a1bf7e",
    "outputId": "6f271e1b-d65b-4fc8-b4ff-955849a1f58e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2097308b10>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphing losses\n",
    "plt.plot(np.array([x.detach().numpy() for x in train_losses]), 'r')\n",
    "plt.plot(np.array([x.detach().numpy() for x in val_losses]), 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3jBqmTq_TUlT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3jBqmTq_TUlT",
    "outputId": "ff804b21-b887-4071-9b08-5b819036883b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL PERFORMANCE - TRAINING PART\n",
    "if not model:\n",
    "  model=VolForecast(input_size=len(input_fields), seq_len=seq_len, horizon=horizon)\n",
    "  model.load_state_dict(torch.load('./model'))\n",
    "  \n",
    "plot_model_performance_single_horizon(model, train_x, train_y, num_samples=600, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "HwOpx_9ZhOH3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "HwOpx_9ZhOH3",
    "outputId": "c5d689b4-2108-48ae-c477-f6c8327a9858"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL'S PERFORMANCE - VALIDATION PART\n",
    "if not model:\n",
    "  model=VolForecast(input_size=len(input_fields), seq_len=seq_len, horizon=horizon)\n",
    "  model.load_state_dict(torch.load('./model'))\n",
    "\n",
    "plot_model_performance_single_horizon(model, val_x, val_y, num_samples=600, horizon=horizon)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
