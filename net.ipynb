{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c04b9a8",
      "metadata": {
        "id": "1c04b9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569f62f4-1107-4102-86c6-c691a4570d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "import math\n",
        "\n",
        "from pandas.core.dtypes.cast import validate_numeric_casting\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install colorama\n",
        "import colorama\n",
        "from colorama import Fore, Style"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNv94H0WjuEo",
        "outputId": "02a862a8-9fc4-4cc8-ca27-ed06268d62d4"
      },
      "id": "NNv94H0WjuEo",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions"
      ],
      "metadata": {
        "id": "Mc7aWdIfrUqt"
      },
      "id": "Mc7aWdIfrUqt"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59066ca8",
      "metadata": {
        "id": "59066ca8"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') # don't have GPU \n",
        "    return device\n",
        "\n",
        "def df_to_tensor(df):\n",
        "    device = get_device()\n",
        "    return torch.tensor(df.values).float().to(device)\n",
        "\n",
        "def get_data(file_name, seq_len, input_fields, output_fields, val_ratio=0.01, transpose_data=False):\n",
        "    # Get data from csv\n",
        "    df = pd.read_csv(file_name, names=['datetime','PRICE','SIZE','vol','vol_horizon','PRICE_horizon','PRICE_slope_horizon','vol_slope_horizon'])\n",
        "    inputs = df_to_tensor(df[input_fields])\n",
        "    outputs = df_to_tensor(df[output_fields])\n",
        "    \n",
        "    # Group seq_len concecutive input points together, and horizon concecutive output vectors together\n",
        "    inputs = torch.stack([inputs[i:i - seq_len] for i in range(seq_len)], dim = 1)\n",
        "    outputs = outputs[seq_len:]\n",
        "\n",
        "    assert len(inputs) == len(outputs), f\"Input and output arrays have different lengths: {len(inputs)} \\= {len(outputs)}\"\n",
        "    \n",
        "    # Do train/validation split\n",
        "    val_len = math.ceil(val_ratio * len(inputs))\n",
        "\n",
        "    if transpose_data:\n",
        "      train_x = inputs[:-val_len].transpose(1,2)\n",
        "      train_y = outputs[:-val_len]\n",
        "      val_x = inputs[-val_len:].transpose(1,2)\n",
        "      val_y = outputs[-val_len:]\n",
        "    else:\n",
        "      train_x = inputs[:-val_len]\n",
        "      train_y = outputs[:-val_len]\n",
        "      val_x = inputs[-val_len:]\n",
        "      val_y = outputs[-val_len:]\n",
        "\n",
        "    print(\"Train input dimension:\", train_x.shape)\n",
        "    print(\"Train output dimension:\", train_y.shape)\n",
        "    print(\"Validation input dimension:\", val_x.shape)\n",
        "    print(\"Validation output dimension:\", val_y.shape)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y\n",
        "\n",
        "# def get_data_multiple_horizons(file_name, input_dim, horizon, val_ratio=0.01):\n",
        "#     # Get data from csv\n",
        "#     df = pd.read_csv(file_name)\n",
        "#     inputs = df_to_tensor(df[['vol']])\n",
        "#     outputs = df_to_tensor(df[['vol']])\n",
        "    \n",
        "#     # Group input_dim concecutive input points together, and horizon concecutive output vectors together\n",
        "#     inputs = torch.stack([inputs[i:i - input_dim - horizon] for i in range(input_dim)], dim = 1)\n",
        "#     outputs = torch.stack([outputs[input_dim + i:i - horizon] for i in range(horizon)], dim = 1).squeeze()\n",
        "\n",
        "#     assert len(inputs) == len(outputs), f\"Input and output arrays have different lengths: {len(inputs)} \\= {len(outputs)}\"\n",
        "    \n",
        "#     # Do train/validation split\n",
        "#     val_len = math.ceil(val_ratio * len(inputs))\n",
        "#     train_x = inputs[:-val_len]\n",
        "#     train_y = outputs[:-val_len]\n",
        "#     val_x = inputs[-val_len:]\n",
        "#     val_y = outputs[-val_len:]\n",
        "\n",
        "#     print(\"Train input dimension:\", train_x.shape)\n",
        "#     print(\"Train output dimension:\", train_y.shape)\n",
        "#     print(\"Validation input dimension:\", val_x.shape)\n",
        "#     print(\"Validation output dimension:\", val_y.shape)\n",
        "\n",
        "#     return train_x, train_y, val_x, val_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_accuracy(model, x, y, num_samples):\n",
        "  assert num_samples <= len(x), f\"Too many samples: {num_samples}. Input size is only {len(x)}\"\n",
        "  out = model(x[:num_samples]).squeeze()\n",
        "  pred = out >= 0\n",
        "  reality = y[:num_samples].squeeze()\n",
        "\n",
        "  correct = (pred == reality).sum().item()\n",
        "  return correct/num_samples"
      ],
      "metadata": {
        "id": "kBZcPTcvcCij"
      },
      "id": "kBZcPTcvcCij",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-ezI05mFiEiH"
      },
      "outputs": [],
      "source": [
        "def plot_model_performance_single_horizon(model, x, y, num_samples):\n",
        "  assert num_samples <= len(x), f\"Too many samples: {num_samples}. Input size is only {len(x)}\"\n",
        "  pred = model(x[:num_samples])\n",
        "  reality = y[:num_samples]\n",
        "\n",
        "  plt.plot(pred.cpu().detach().numpy(), 'r')\n",
        "  plt.plot(reality.cpu().detach().numpy(), 'b')\n",
        "\n",
        "# def plot_model_performance_multiple_horizons(model, x, y, num_samples, horizon):\n",
        "#   assert num_samples < len(x), f\"Too many samples: {num_samples}. Input size is only {len(x)}\"\n",
        "#   pred = model(x[:num_samples])[:, horizon-1]\n",
        "#   reality = y[:num_samples][:, horizon-1]\n",
        "\n",
        "#   plt.plot(pred.detach().numpy(), 'r')\n",
        "#   plt.plot(reality.detach().numpy(), 'b')"
      ],
      "id": "-ezI05mFiEiH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapted TCN Implementation\n",
        "\n",
        "Bai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.\n",
        "\n",
        "Official TCN PyTorch implementation: https://github.com/locuslab/TCN\n"
      ],
      "metadata": {
        "id": "vVW36Qdhr22e"
      },
      "id": "vVW36Qdhr22e"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5e238f44",
      "metadata": {
        "id": "5e238f44"
      },
      "outputs": [],
      "source": [
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2,device=None):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation, device=device))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation, device=device))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1, device=device) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, device=None):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout, device=device)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom model definitions "
      ],
      "metadata": {
        "id": "JkKKZvb3sIMx"
      },
      "id": "JkKKZvb3sIMx"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8988bb29",
      "metadata": {
        "id": "8988bb29"
      },
      "outputs": [],
      "source": [
        "# Generic TCN class\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, device=None):\n",
        "        super(TCN, self).__init__()\n",
        "\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout, device=device)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size, device=device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
        "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
        "        o = self.linear(y1[:, :, -1])\n",
        "        return o\n",
        "\n",
        "# Generic MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, output_size, dropout, device=None):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        dims = [input_size] + num_channels + [output_size]\n",
        "        mlp_layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            if i == len(dims) - 2:\n",
        "              linear = nn.Linear(dims[i], dims[i+1], device=device)\n",
        "              mlp_layers += [linear]\n",
        "            else:\n",
        "              linear = nn.Linear(dims[i], dims[i+1], device=device)\n",
        "              drop = nn.Dropout(dropout)\n",
        "              relu = nn.ReLU()\n",
        "              mlp_layers += [linear, drop, relu]\n",
        "        \n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.mlp(inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model that uses LSTM\n",
        "class ForecastLSTM(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ForecastLSTM, self).__init__()\n",
        "        self.sequence_len = args.sequence_len\n",
        "        self.input_size = args.input_size\n",
        "\n",
        "        device = get_device()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=args.input_size, hidden_size=args.hidden_size, num_layers=args.num_layers_lstm, bias=False, \n",
        "                            batch_first=True, dropout=args.dropout_lstm, device=device)\n",
        "        \n",
        "        self.mlp = MLP(input_size=args.sequence_len * args.hidden_size, num_channels=args.num_channels_mlp, \n",
        "                       output_size=1, dropout=args.dropout_mlp,device=device)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        o, _ = self.lstm(x)\n",
        "        return self.mlp(o.reshape((o.shape[0], o.shape[1]*o.shape[2])))\n",
        "\n",
        "\n",
        "# Model that uses TCN\n",
        "class ForecastTCN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ForecastTCN, self).__init__()\n",
        "        self.sequence_len = args.sequence_len\n",
        "        self.input_size = args.input_size\n",
        "        device=get_device()\n",
        "\n",
        "        self.tcn = TCN(input_size=args.input_size, output_size=args.num_channels_mlp[0], \n",
        "                       num_channels=args.num_channels_tcn, kernel_size=args.kernel_size, \n",
        "                       dropout=args.dropout_tcn, device=device)\n",
        "\n",
        "        self.mlp = MLP(input_size=args.num_channels_mlp[0], num_channels=args.num_channels_mlp[1:], \n",
        "                       output_size=1, dropout=args.dropout_mlp,device=device)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        tcn_o = F.relu(self.tcn(x))\n",
        "        mlp_o = self.mlp(tcn_o)\n",
        "        return mlp_o\n",
        "\n",
        "\n",
        "# Model that predicts output accross multiple horizons simultaneously\n",
        "# \n",
        "# class VolForecast(nn.Module):\n",
        "#     def __init__(self, input_dim, horizon):\n",
        "#         super(VolForecast, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.horizon = horizon\n",
        "\n",
        "#         self.tcn = TCN(input_size=input_dim, output_size=horizon, num_channels=[300, 250, 200, 200, 150, 100], kernel_size=7, dropout=0.1)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return self.tcn(x)"
      ],
      "metadata": {
        "id": "pbl1joAlY2sj"
      },
      "id": "pbl1joAlY2sj",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation"
      ],
      "metadata": {
        "id": "rvaAZuCnsWLJ"
      },
      "id": "rvaAZuCnsWLJ"
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "5Tu24bmR0ed0"
      },
      "id": "5Tu24bmR0ed0",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43beaa2a",
      "metadata": {
        "id": "43beaa2a"
      },
      "outputs": [],
      "source": [
        "def train(args, train_x, train_y, val_x, val_y):\n",
        "    \n",
        "    print(args)\n",
        "    \n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "    pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save params for cross-reference\n",
        "    ags_file = open(f\"{save_dir}/args\", \"wb\")\n",
        "    pickle.dump(args, ags_file)\n",
        "    ags_file.close()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = args.loss_fn\n",
        "\n",
        "    # Model and optimizer\n",
        "    model = args.model(args)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.learn_rate)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    for epoch in range(args.nepochs):\n",
        "        # Sample batch input randomly\n",
        "        random_ids = np.random.randint(len(train_x), size=args.batch_size)\n",
        "        train_input = train_x[random_ids]\n",
        "        train_output = train_y[random_ids]        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(model(train_input), train_output)\n",
        "        accuracy = classifier_accuracy(model, train_input, train_output, args.batch_size)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses += [loss]\n",
        "        train_accuracies += [accuracy]\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}, training loss: {loss}, training accuracy: {accuracy}\")\n",
        "\n",
        "        if args.validate and (epoch+1) % args.validation_frequency == 0:\n",
        "\n",
        "            val_loss = loss_fn(model(val_x), val_y)\n",
        "            val_losses += [val_loss]\n",
        "\n",
        "            val_accuracy = classifier_accuracy(model, val_x, val_y, len(val_x))\n",
        "            val_accuracies += [val_accuracy]\n",
        "\n",
        "            print(f\"{Fore.RED} Validation loss: {val_loss}, validation accuracy: {val_accuracy} {Style.RESET_ALL}\")\n",
        "    \n",
        "    torch.save(model.state_dict(), f\"{save_dir}/model_params\")\n",
        "\n",
        "\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c92af5f4",
      "metadata": {
        "id": "c92af5f4"
      },
      "outputs": [],
      "source": [
        "args = AttrDict()\n",
        "\n",
        "### Arguments for data split, training hyperparameters, etc.\n",
        "args_generic = {\n",
        "    # Generic\n",
        "    \"data_path\": '/content/drive/MyDrive/sasha/csc413/2sec.csv',\n",
        "    \"seed\": 0,\n",
        "    \"experiment_name\": \"simple_TCN_volatility\",\n",
        "\n",
        "    # Validation\n",
        "    \"validate\": True,\n",
        "    \"validation_frequency\": 500,\n",
        "    \"val_ratio\": 0.002,\n",
        "\n",
        "    # Input/output params\n",
        "    \"sequence_len\": 300,\n",
        "    \"input_fields\": ['PRICE', 'SIZE', 'vol'],\n",
        "    \"output_fields\": ['vol_slope_horizon'],\n",
        "\n",
        "    # Model \n",
        "    \"model\": ForecastTCN,\n",
        "    \"loss_fn\": nn.BCEWithLogitsLoss(),\n",
        "\n",
        "    # Learning\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"nepochs\": 2000,\n",
        "}\n",
        "args.update(args_generic)\n",
        "\n",
        "\n",
        "### Architecture params, given per model\n",
        "if args_generic['model'] == ForecastTCN:\n",
        "  args_tcn = {\n",
        "      \"transpose_data\": True, # don't change\n",
        "      \"input_size\": len(args_generic['input_fields']), # don't change\n",
        "\n",
        "      # Parameters for TCN part\n",
        "      \"num_channels_tcn\": 6*[25],\n",
        "      \"kernel_size\": 7,\n",
        "      \"dropout_tcn\": 0.1,\n",
        "\n",
        "      # Params for MLP part\n",
        "      \"num_channels_mlp\": 2*[25],\n",
        "      \"dropout_mlp\": 0.1,\n",
        "  }\n",
        "\n",
        "  args.update(args_tcn)\n",
        "elif args_generic['model'] == ForecastLSTM:\n",
        "  args_lstm = {\n",
        "      \"transpose_data\": False, # don't change\n",
        "      \"input_size\": len(args_generic['input_fields']), # don't change\n",
        "\n",
        "      # Parameters for LSTM part\n",
        "      \"hidden_size\": 64,\n",
        "      \"num_layers_lstm\": 2,\n",
        "      \"dropout_lstm\": 0.1,\n",
        "\n",
        "      # Parameters for MLP part\n",
        "      \"num_channels_mlp\": 2*[25],\n",
        "      \"dropout_mlp\": 0.1,\n",
        "  }\n",
        "  args.update(args_lstm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "train_x, train_y, val_x, val_y = get_data(args.data_path, seq_len=args.sequence_len, input_fields=args.input_fields, \n",
        "                                          output_fields=args.output_fields, val_ratio=args.val_ratio, transpose_data=args.transpose_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMs2k8zT8Hm8",
        "outputId": "3a25d65a-ffd9-433a-fab0-b0f9bd7df730"
      },
      "id": "GMs2k8zT8Hm8",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train input dimension: torch.Size([1043582, 3, 300])\n",
            "Train output dimension: torch.Size([1043582, 1])\n",
            "Validation input dimension: torch.Size([2092, 3, 300])\n",
            "Validation output dimension: torch.Size([2092, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "model, train_losses, val_losses, train_accuracies, val_accuracies = train(args, train_x, train_y, val_x, val_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz0WYBmI78j_",
        "outputId": "c4f7efc5-3628-438e-8eaf-a4cb3279d87d"
      },
      "id": "mz0WYBmI78j_",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_path': '/content/drive/MyDrive/sasha/csc413/2sec.csv', 'seed': 0, 'experiment_name': 'simple_TCN_volatility', 'validate': True, 'validation_frequency': 500, 'val_ratio': 0.002, 'sequence_len': 300, 'input_fields': ['PRICE', 'SIZE', 'vol'], 'output_fields': ['vol_slope_horizon'], 'model': <class '__main__.ForecastTCN'>, 'loss_fn': BCEWithLogitsLoss(), 'learn_rate': 0.001, 'batch_size': 64, 'nepochs': 2000, 'transpose_data': True, 'input_size': 3, 'num_channels_tcn': [25, 25, 25, 25, 25, 25], 'kernel_size': 7, 'dropout_tcn': 0.1, 'num_channels_mlp': [25, 25], 'dropout_mlp': 0.1}\n",
            "Epoch: 1, training loss: 0.6956802010536194, training accuracy: 0.515625\n",
            "Epoch: 2, training loss: 0.7061737775802612, training accuracy: 0.46875\n",
            "Epoch: 3, training loss: 0.6881448030471802, training accuracy: 0.578125\n",
            "Epoch: 4, training loss: 0.676539421081543, training accuracy: 0.609375\n",
            "Epoch: 5, training loss: 0.6780059337615967, training accuracy: 0.59375\n",
            "Epoch: 6, training loss: 0.715370774269104, training accuracy: 0.421875\n",
            "Epoch: 7, training loss: 0.6725649237632751, training accuracy: 0.625\n",
            "Epoch: 8, training loss: 0.6981164216995239, training accuracy: 0.5\n",
            "Epoch: 9, training loss: 0.6979317665100098, training accuracy: 0.5\n",
            "Epoch: 10, training loss: 0.7102122902870178, training accuracy: 0.4375\n",
            "Epoch: 11, training loss: 0.6790663599967957, training accuracy: 0.59375\n",
            "Epoch: 12, training loss: 0.6998506784439087, training accuracy: 0.484375\n",
            "Epoch: 13, training loss: 0.6989884376525879, training accuracy: 0.484375\n",
            "Epoch: 14, training loss: 0.6878000497817993, training accuracy: 0.546875\n",
            "Epoch: 15, training loss: 0.6912693977355957, training accuracy: 0.53125\n",
            "Epoch: 16, training loss: 0.6821244955062866, training accuracy: 0.609375\n",
            "Epoch: 17, training loss: 0.6927616596221924, training accuracy: 0.515625\n",
            "Epoch: 18, training loss: 0.6853739619255066, training accuracy: 0.5625\n",
            "Epoch: 19, training loss: 0.6740557551383972, training accuracy: 0.65625\n",
            "Epoch: 20, training loss: 0.6995189785957336, training accuracy: 0.46875\n",
            "Epoch: 21, training loss: 0.6795532703399658, training accuracy: 0.59375\n",
            "Epoch: 22, training loss: 0.6972168684005737, training accuracy: 0.5\n",
            "Epoch: 23, training loss: 0.7063688039779663, training accuracy: 0.421875\n",
            "Epoch: 24, training loss: 0.6775071620941162, training accuracy: 0.609375\n",
            "Epoch: 25, training loss: 0.6929966807365417, training accuracy: 0.5\n",
            "Epoch: 26, training loss: 0.6833382844924927, training accuracy: 0.578125\n",
            "Epoch: 27, training loss: 0.6854081153869629, training accuracy: 0.5625\n",
            "Epoch: 28, training loss: 0.6901230812072754, training accuracy: 0.515625\n",
            "Epoch: 29, training loss: 0.684518575668335, training accuracy: 0.59375\n",
            "Epoch: 30, training loss: 0.6894677877426147, training accuracy: 0.53125\n",
            "Epoch: 31, training loss: 0.6960949897766113, training accuracy: 0.5\n",
            "Epoch: 32, training loss: 0.7034204006195068, training accuracy: 0.453125\n",
            "Epoch: 33, training loss: 0.6863847374916077, training accuracy: 0.53125\n",
            "Epoch: 34, training loss: 0.68776935338974, training accuracy: 0.546875\n",
            "Epoch: 35, training loss: 0.6884745359420776, training accuracy: 0.5625\n",
            "Epoch: 36, training loss: 0.7024097442626953, training accuracy: 0.40625\n",
            "Epoch: 37, training loss: 0.6773382425308228, training accuracy: 0.609375\n",
            "Epoch: 38, training loss: 0.6863604784011841, training accuracy: 0.5625\n",
            "Epoch: 39, training loss: 0.6856644153594971, training accuracy: 0.5625\n",
            "Epoch: 40, training loss: 0.697614312171936, training accuracy: 0.46875\n",
            "Epoch: 41, training loss: 0.6863194704055786, training accuracy: 0.546875\n",
            "Epoch: 42, training loss: 0.6873655915260315, training accuracy: 0.5\n",
            "Epoch: 43, training loss: 0.6884974241256714, training accuracy: 0.578125\n",
            "Epoch: 44, training loss: 0.6889141798019409, training accuracy: 0.46875\n",
            "Epoch: 45, training loss: 0.6881872415542603, training accuracy: 0.484375\n",
            "Epoch: 46, training loss: 0.681830644607544, training accuracy: 0.609375\n",
            "Epoch: 47, training loss: 0.6870418787002563, training accuracy: 0.625\n",
            "Epoch: 48, training loss: 0.6888182163238525, training accuracy: 0.515625\n",
            "Epoch: 49, training loss: 0.6783906817436218, training accuracy: 0.65625\n",
            "Epoch: 50, training loss: 0.6931737661361694, training accuracy: 0.515625\n",
            "Epoch: 51, training loss: 0.6759822368621826, training accuracy: 0.609375\n",
            "Epoch: 52, training loss: 0.6972005367279053, training accuracy: 0.53125\n",
            "Epoch: 53, training loss: 0.705984354019165, training accuracy: 0.484375\n",
            "Epoch: 54, training loss: 0.6843581199645996, training accuracy: 0.546875\n",
            "Epoch: 55, training loss: 0.6857381463050842, training accuracy: 0.515625\n",
            "Epoch: 56, training loss: 0.6978658437728882, training accuracy: 0.5\n",
            "Epoch: 57, training loss: 0.6843900680541992, training accuracy: 0.609375\n",
            "Epoch: 58, training loss: 0.6926500201225281, training accuracy: 0.515625\n",
            "Epoch: 59, training loss: 0.6913858652114868, training accuracy: 0.578125\n",
            "Epoch: 60, training loss: 0.6746784448623657, training accuracy: 0.578125\n",
            "Epoch: 61, training loss: 0.6814937591552734, training accuracy: 0.578125\n",
            "Epoch: 62, training loss: 0.6849358081817627, training accuracy: 0.546875\n",
            "Epoch: 63, training loss: 0.6697863936424255, training accuracy: 0.65625\n",
            "Epoch: 64, training loss: 0.6804884076118469, training accuracy: 0.53125\n",
            "Epoch: 65, training loss: 0.668315589427948, training accuracy: 0.65625\n",
            "Epoch: 66, training loss: 0.6929240822792053, training accuracy: 0.59375\n",
            "Epoch: 67, training loss: 0.6926916837692261, training accuracy: 0.546875\n",
            "Epoch: 68, training loss: 0.6837581992149353, training accuracy: 0.625\n",
            "Epoch: 69, training loss: 0.689741849899292, training accuracy: 0.59375\n",
            "Epoch: 70, training loss: 0.7028393149375916, training accuracy: 0.421875\n",
            "Epoch: 71, training loss: 0.6666112542152405, training accuracy: 0.640625\n",
            "Epoch: 72, training loss: 0.695177435874939, training accuracy: 0.609375\n",
            "Epoch: 73, training loss: 0.6860756874084473, training accuracy: 0.5625\n",
            "Epoch: 74, training loss: 0.6826605796813965, training accuracy: 0.5\n",
            "Epoch: 75, training loss: 0.7129361033439636, training accuracy: 0.46875\n",
            "Epoch: 76, training loss: 0.6746642589569092, training accuracy: 0.59375\n",
            "Epoch: 77, training loss: 0.6846189498901367, training accuracy: 0.578125\n",
            "Epoch: 78, training loss: 0.6927838325500488, training accuracy: 0.546875\n",
            "Epoch: 79, training loss: 0.6741620302200317, training accuracy: 0.5625\n",
            "Epoch: 80, training loss: 0.6637106537818909, training accuracy: 0.640625\n",
            "Epoch: 81, training loss: 0.678851306438446, training accuracy: 0.546875\n",
            "Epoch: 82, training loss: 0.6652662754058838, training accuracy: 0.546875\n",
            "Epoch: 83, training loss: 0.6717600226402283, training accuracy: 0.625\n",
            "Epoch: 84, training loss: 0.6601978540420532, training accuracy: 0.625\n",
            "Epoch: 85, training loss: 0.6868823766708374, training accuracy: 0.5\n",
            "Epoch: 86, training loss: 0.6736056804656982, training accuracy: 0.609375\n",
            "Epoch: 87, training loss: 0.6547675132751465, training accuracy: 0.609375\n",
            "Epoch: 88, training loss: 0.6852149963378906, training accuracy: 0.515625\n",
            "Epoch: 89, training loss: 0.6638896465301514, training accuracy: 0.65625\n",
            "Epoch: 90, training loss: 0.679499626159668, training accuracy: 0.578125\n",
            "Epoch: 91, training loss: 0.6888819336891174, training accuracy: 0.5625\n",
            "Epoch: 92, training loss: 0.6645309925079346, training accuracy: 0.625\n",
            "Epoch: 93, training loss: 0.6638264656066895, training accuracy: 0.65625\n",
            "Epoch: 94, training loss: 0.6843587756156921, training accuracy: 0.546875\n",
            "Epoch: 95, training loss: 0.6760267019271851, training accuracy: 0.59375\n",
            "Epoch: 96, training loss: 0.6784852743148804, training accuracy: 0.65625\n",
            "Epoch: 97, training loss: 0.6697285175323486, training accuracy: 0.65625\n",
            "Epoch: 98, training loss: 0.6951591968536377, training accuracy: 0.546875\n",
            "Epoch: 99, training loss: 0.6650505661964417, training accuracy: 0.546875\n",
            "Epoch: 100, training loss: 0.6799174547195435, training accuracy: 0.53125\n",
            "Epoch: 101, training loss: 0.6520848274230957, training accuracy: 0.640625\n",
            "Epoch: 102, training loss: 0.6860328316688538, training accuracy: 0.640625\n",
            "Epoch: 103, training loss: 0.6569315791130066, training accuracy: 0.625\n",
            "Epoch: 104, training loss: 0.6643049120903015, training accuracy: 0.5625\n",
            "Epoch: 105, training loss: 0.6900026798248291, training accuracy: 0.5625\n",
            "Epoch: 106, training loss: 0.6755139827728271, training accuracy: 0.609375\n",
            "Epoch: 107, training loss: 0.6703944802284241, training accuracy: 0.5625\n",
            "Epoch: 108, training loss: 0.6787329912185669, training accuracy: 0.59375\n",
            "Epoch: 109, training loss: 0.6099333763122559, training accuracy: 0.6875\n",
            "Epoch: 110, training loss: 0.6473869681358337, training accuracy: 0.609375\n",
            "Epoch: 111, training loss: 0.6629146337509155, training accuracy: 0.578125\n",
            "Epoch: 112, training loss: 0.6631757020950317, training accuracy: 0.609375\n",
            "Epoch: 113, training loss: 0.6933119297027588, training accuracy: 0.59375\n",
            "Epoch: 114, training loss: 0.6667834520339966, training accuracy: 0.5625\n",
            "Epoch: 115, training loss: 0.6266109347343445, training accuracy: 0.703125\n",
            "Epoch: 116, training loss: 0.696733832359314, training accuracy: 0.609375\n",
            "Epoch: 117, training loss: 0.6634982824325562, training accuracy: 0.609375\n",
            "Epoch: 118, training loss: 0.6858419179916382, training accuracy: 0.546875\n",
            "Epoch: 119, training loss: 0.6363471746444702, training accuracy: 0.75\n",
            "Epoch: 120, training loss: 0.6165482997894287, training accuracy: 0.75\n",
            "Epoch: 121, training loss: 0.6479364633560181, training accuracy: 0.59375\n",
            "Epoch: 122, training loss: 0.693626880645752, training accuracy: 0.484375\n",
            "Epoch: 123, training loss: 0.6467244029045105, training accuracy: 0.609375\n",
            "Epoch: 124, training loss: 0.6886187791824341, training accuracy: 0.625\n",
            "Epoch: 125, training loss: 0.6742420792579651, training accuracy: 0.546875\n",
            "Epoch: 126, training loss: 0.6318102478981018, training accuracy: 0.703125\n",
            "Epoch: 127, training loss: 0.6384352445602417, training accuracy: 0.671875\n",
            "Epoch: 128, training loss: 0.632213294506073, training accuracy: 0.703125\n",
            "Epoch: 129, training loss: 0.6487717628479004, training accuracy: 0.546875\n",
            "Epoch: 130, training loss: 0.6650348901748657, training accuracy: 0.59375\n",
            "Epoch: 131, training loss: 0.649728000164032, training accuracy: 0.640625\n",
            "Epoch: 132, training loss: 0.6842629313468933, training accuracy: 0.53125\n",
            "Epoch: 133, training loss: 0.6317211985588074, training accuracy: 0.625\n",
            "Epoch: 134, training loss: 0.6923657655715942, training accuracy: 0.453125\n",
            "Epoch: 135, training loss: 0.7010490894317627, training accuracy: 0.609375\n",
            "Epoch: 136, training loss: 0.6661652326583862, training accuracy: 0.609375\n",
            "Epoch: 137, training loss: 0.6900856494903564, training accuracy: 0.484375\n",
            "Epoch: 138, training loss: 0.6196136474609375, training accuracy: 0.703125\n",
            "Epoch: 139, training loss: 0.6363545656204224, training accuracy: 0.609375\n",
            "Epoch: 140, training loss: 0.6253661513328552, training accuracy: 0.625\n",
            "Epoch: 141, training loss: 0.6840597987174988, training accuracy: 0.625\n",
            "Epoch: 142, training loss: 0.6561621427536011, training accuracy: 0.625\n",
            "Epoch: 143, training loss: 0.6224006414413452, training accuracy: 0.65625\n",
            "Epoch: 144, training loss: 0.8390303254127502, training accuracy: 0.640625\n",
            "Epoch: 145, training loss: 0.6564158797264099, training accuracy: 0.625\n",
            "Epoch: 146, training loss: 0.6943240761756897, training accuracy: 0.71875\n",
            "Epoch: 147, training loss: 0.6698321104049683, training accuracy: 0.5\n",
            "Epoch: 148, training loss: 0.6563779711723328, training accuracy: 0.546875\n",
            "Epoch: 149, training loss: 0.6335830688476562, training accuracy: 0.625\n",
            "Epoch: 150, training loss: 0.6043179035186768, training accuracy: 0.6875\n",
            "Epoch: 151, training loss: 0.6788498163223267, training accuracy: 0.6875\n",
            "Epoch: 152, training loss: 0.6766178607940674, training accuracy: 0.484375\n",
            "Epoch: 153, training loss: 0.6648951768875122, training accuracy: 0.59375\n",
            "Epoch: 154, training loss: 0.6612812280654907, training accuracy: 0.546875\n",
            "Epoch: 155, training loss: 0.6387200355529785, training accuracy: 0.5\n",
            "Epoch: 156, training loss: 0.6757537722587585, training accuracy: 0.578125\n",
            "Epoch: 157, training loss: 0.6667054891586304, training accuracy: 0.578125\n",
            "Epoch: 158, training loss: 0.6481544971466064, training accuracy: 0.578125\n",
            "Epoch: 159, training loss: 0.6816736459732056, training accuracy: 0.46875\n",
            "Epoch: 160, training loss: 0.666710615158081, training accuracy: 0.703125\n",
            "Epoch: 161, training loss: 0.6465635895729065, training accuracy: 0.6875\n",
            "Epoch: 162, training loss: 0.6771495342254639, training accuracy: 0.5625\n",
            "Epoch: 163, training loss: 0.7004658579826355, training accuracy: 0.609375\n",
            "Epoch: 164, training loss: 0.6588649749755859, training accuracy: 0.640625\n",
            "Epoch: 165, training loss: 0.6443031430244446, training accuracy: 0.640625\n",
            "Epoch: 166, training loss: 0.627475380897522, training accuracy: 0.625\n",
            "Epoch: 167, training loss: 0.6174893975257874, training accuracy: 0.71875\n",
            "Epoch: 168, training loss: 0.6195309162139893, training accuracy: 0.640625\n",
            "Epoch: 169, training loss: 0.7447209358215332, training accuracy: 0.53125\n",
            "Epoch: 170, training loss: 0.6390054225921631, training accuracy: 0.71875\n",
            "Epoch: 171, training loss: 0.6276866793632507, training accuracy: 0.59375\n",
            "Epoch: 172, training loss: 0.6656924486160278, training accuracy: 0.609375\n",
            "Epoch: 173, training loss: 0.6264580488204956, training accuracy: 0.65625\n",
            "Epoch: 174, training loss: 0.6580262184143066, training accuracy: 0.625\n",
            "Epoch: 175, training loss: 0.6914777755737305, training accuracy: 0.578125\n",
            "Epoch: 176, training loss: 0.6411964893341064, training accuracy: 0.6875\n",
            "Epoch: 177, training loss: 0.6479178071022034, training accuracy: 0.671875\n",
            "Epoch: 178, training loss: 0.6428050994873047, training accuracy: 0.59375\n",
            "Epoch: 179, training loss: 0.6370624303817749, training accuracy: 0.671875\n",
            "Epoch: 180, training loss: 0.644751787185669, training accuracy: 0.59375\n",
            "Epoch: 181, training loss: 0.6906493902206421, training accuracy: 0.59375\n",
            "Epoch: 182, training loss: 0.6914023160934448, training accuracy: 0.546875\n",
            "Epoch: 183, training loss: 0.7298595905303955, training accuracy: 0.53125\n",
            "Epoch: 184, training loss: 0.6668392419815063, training accuracy: 0.609375\n",
            "Epoch: 185, training loss: 0.6375502347946167, training accuracy: 0.578125\n",
            "Epoch: 186, training loss: 0.7397637367248535, training accuracy: 0.484375\n",
            "Epoch: 187, training loss: 0.6777598857879639, training accuracy: 0.640625\n",
            "Epoch: 188, training loss: 0.6325854063034058, training accuracy: 0.578125\n",
            "Epoch: 189, training loss: 0.6750827431678772, training accuracy: 0.609375\n",
            "Epoch: 190, training loss: 0.6627618074417114, training accuracy: 0.59375\n",
            "Epoch: 191, training loss: 0.6754179000854492, training accuracy: 0.609375\n",
            "Epoch: 192, training loss: 0.6570676565170288, training accuracy: 0.625\n",
            "Epoch: 193, training loss: 0.6613383293151855, training accuracy: 0.609375\n",
            "Epoch: 194, training loss: 0.6425091028213501, training accuracy: 0.6875\n",
            "Epoch: 195, training loss: 0.6348645687103271, training accuracy: 0.6875\n",
            "Epoch: 196, training loss: 0.6554988622665405, training accuracy: 0.640625\n",
            "Epoch: 197, training loss: 0.6794949769973755, training accuracy: 0.578125\n",
            "Epoch: 198, training loss: 0.6472247838973999, training accuracy: 0.65625\n",
            "Epoch: 199, training loss: 0.6219514608383179, training accuracy: 0.75\n",
            "Epoch: 200, training loss: 0.6912894248962402, training accuracy: 0.578125\n",
            "Epoch: 201, training loss: 0.6462271213531494, training accuracy: 0.59375\n",
            "Epoch: 202, training loss: 0.6171782612800598, training accuracy: 0.75\n",
            "Epoch: 203, training loss: 0.5869616866111755, training accuracy: 0.765625\n",
            "Epoch: 204, training loss: 0.6509810090065002, training accuracy: 0.65625\n",
            "Epoch: 205, training loss: 0.687827467918396, training accuracy: 0.5\n",
            "Epoch: 206, training loss: 0.5915085077285767, training accuracy: 0.703125\n",
            "Epoch: 207, training loss: 0.5699930787086487, training accuracy: 0.734375\n",
            "Epoch: 208, training loss: 0.7098779082298279, training accuracy: 0.59375\n",
            "Epoch: 209, training loss: 0.6446988582611084, training accuracy: 0.65625\n",
            "Epoch: 210, training loss: 0.717093825340271, training accuracy: 0.515625\n",
            "Epoch: 211, training loss: 0.5938211679458618, training accuracy: 0.671875\n",
            "Epoch: 212, training loss: 0.8355906009674072, training accuracy: 0.609375\n",
            "Epoch: 213, training loss: 0.6914917230606079, training accuracy: 0.609375\n",
            "Epoch: 214, training loss: 0.6441987752914429, training accuracy: 0.59375\n",
            "Epoch: 215, training loss: 0.6882266402244568, training accuracy: 0.5625\n",
            "Epoch: 216, training loss: 0.6216872334480286, training accuracy: 0.65625\n",
            "Epoch: 217, training loss: 0.6100758910179138, training accuracy: 0.671875\n",
            "Epoch: 218, training loss: 0.7117561101913452, training accuracy: 0.53125\n",
            "Epoch: 219, training loss: 0.6790294051170349, training accuracy: 0.59375\n",
            "Epoch: 220, training loss: 0.6350494623184204, training accuracy: 0.65625\n",
            "Epoch: 221, training loss: 0.6592429280281067, training accuracy: 0.65625\n",
            "Epoch: 222, training loss: 0.6467283964157104, training accuracy: 0.640625\n",
            "Epoch: 223, training loss: 0.6967750787734985, training accuracy: 0.546875\n",
            "Epoch: 224, training loss: 0.6641894578933716, training accuracy: 0.578125\n",
            "Epoch: 225, training loss: 0.7092693448066711, training accuracy: 0.53125\n",
            "Epoch: 226, training loss: 0.6785274744033813, training accuracy: 0.515625\n",
            "Epoch: 227, training loss: 0.6119729280471802, training accuracy: 0.625\n",
            "Epoch: 228, training loss: 0.6447058916091919, training accuracy: 0.65625\n",
            "Epoch: 229, training loss: 0.6484363079071045, training accuracy: 0.65625\n",
            "Epoch: 230, training loss: 0.6280418634414673, training accuracy: 0.71875\n",
            "Epoch: 231, training loss: 0.667244017124176, training accuracy: 0.578125\n",
            "Epoch: 232, training loss: 0.6926193833351135, training accuracy: 0.609375\n",
            "Epoch: 233, training loss: 0.6547108888626099, training accuracy: 0.59375\n",
            "Epoch: 234, training loss: 0.6586467027664185, training accuracy: 0.5625\n",
            "Epoch: 235, training loss: 0.6436126232147217, training accuracy: 0.640625\n",
            "Epoch: 236, training loss: 0.6461812853813171, training accuracy: 0.640625\n",
            "Epoch: 237, training loss: 0.6341859102249146, training accuracy: 0.640625\n",
            "Epoch: 238, training loss: 0.6432837247848511, training accuracy: 0.65625\n",
            "Epoch: 239, training loss: 0.6153843402862549, training accuracy: 0.640625\n",
            "Epoch: 240, training loss: 0.6684255599975586, training accuracy: 0.609375\n",
            "Epoch: 241, training loss: 0.6599700450897217, training accuracy: 0.59375\n",
            "Epoch: 242, training loss: 0.6382055878639221, training accuracy: 0.671875\n",
            "Epoch: 243, training loss: 0.6682000160217285, training accuracy: 0.671875\n",
            "Epoch: 244, training loss: 0.6227024793624878, training accuracy: 0.6875\n",
            "Epoch: 245, training loss: 0.6782524585723877, training accuracy: 0.59375\n",
            "Epoch: 246, training loss: 0.6300852298736572, training accuracy: 0.671875\n",
            "Epoch: 247, training loss: 0.6363040208816528, training accuracy: 0.609375\n",
            "Epoch: 248, training loss: 0.6256054639816284, training accuracy: 0.671875\n",
            "Epoch: 249, training loss: 0.6915295124053955, training accuracy: 0.53125\n",
            "Epoch: 250, training loss: 0.6463614106178284, training accuracy: 0.65625\n",
            "Epoch: 251, training loss: 0.6342108249664307, training accuracy: 0.65625\n",
            "Epoch: 252, training loss: 0.6500589847564697, training accuracy: 0.625\n",
            "Epoch: 253, training loss: 0.6152821183204651, training accuracy: 0.6875\n",
            "Epoch: 254, training loss: 0.627820611000061, training accuracy: 0.6875\n",
            "Epoch: 255, training loss: 0.5986231565475464, training accuracy: 0.6875\n",
            "Epoch: 256, training loss: 0.6417261362075806, training accuracy: 0.609375\n",
            "Epoch: 257, training loss: 0.6559931635856628, training accuracy: 0.609375\n",
            "Epoch: 258, training loss: 0.6690449714660645, training accuracy: 0.546875\n",
            "Epoch: 259, training loss: 0.6710456609725952, training accuracy: 0.59375\n",
            "Epoch: 260, training loss: 0.554347038269043, training accuracy: 0.765625\n",
            "Epoch: 261, training loss: 0.5571264624595642, training accuracy: 0.703125\n",
            "Epoch: 262, training loss: 0.6230896711349487, training accuracy: 0.65625\n",
            "Epoch: 263, training loss: 0.6731371879577637, training accuracy: 0.609375\n",
            "Epoch: 264, training loss: 0.6165715456008911, training accuracy: 0.625\n",
            "Epoch: 265, training loss: 0.5901380777359009, training accuracy: 0.640625\n",
            "Epoch: 266, training loss: 0.661559522151947, training accuracy: 0.515625\n",
            "Epoch: 267, training loss: 0.6509835720062256, training accuracy: 0.609375\n",
            "Epoch: 268, training loss: 0.6155110597610474, training accuracy: 0.625\n",
            "Epoch: 269, training loss: 0.5726519227027893, training accuracy: 0.6875\n",
            "Epoch: 270, training loss: 0.6436590552330017, training accuracy: 0.671875\n",
            "Epoch: 271, training loss: 0.678820013999939, training accuracy: 0.6875\n",
            "Epoch: 272, training loss: 0.6691412925720215, training accuracy: 0.65625\n",
            "Epoch: 273, training loss: 0.7204223871231079, training accuracy: 0.578125\n",
            "Epoch: 274, training loss: 0.6289724707603455, training accuracy: 0.65625\n",
            "Epoch: 275, training loss: 0.659833550453186, training accuracy: 0.59375\n",
            "Epoch: 276, training loss: 0.6562504768371582, training accuracy: 0.578125\n",
            "Epoch: 277, training loss: 0.6439266204833984, training accuracy: 0.640625\n",
            "Epoch: 278, training loss: 0.5705617666244507, training accuracy: 0.75\n",
            "Epoch: 279, training loss: 0.6819604635238647, training accuracy: 0.640625\n",
            "Epoch: 280, training loss: 0.6928250789642334, training accuracy: 0.59375\n",
            "Epoch: 281, training loss: 0.6279072761535645, training accuracy: 0.640625\n",
            "Epoch: 282, training loss: 0.6720391511917114, training accuracy: 0.578125\n",
            "Epoch: 283, training loss: 0.6341351270675659, training accuracy: 0.65625\n",
            "Epoch: 284, training loss: 0.6579679846763611, training accuracy: 0.609375\n",
            "Epoch: 285, training loss: 0.617000937461853, training accuracy: 0.671875\n",
            "Epoch: 286, training loss: 0.6434684991836548, training accuracy: 0.625\n",
            "Epoch: 287, training loss: 0.6383464336395264, training accuracy: 0.671875\n",
            "Epoch: 288, training loss: 0.6627015471458435, training accuracy: 0.5625\n",
            "Epoch: 289, training loss: 0.70365309715271, training accuracy: 0.53125\n",
            "Epoch: 290, training loss: 0.6565146446228027, training accuracy: 0.578125\n",
            "Epoch: 291, training loss: 0.7038923501968384, training accuracy: 0.5625\n",
            "Epoch: 292, training loss: 0.6559334397315979, training accuracy: 0.671875\n",
            "Epoch: 293, training loss: 0.6731922626495361, training accuracy: 0.546875\n",
            "Epoch: 294, training loss: 0.7046989798545837, training accuracy: 0.5625\n",
            "Epoch: 295, training loss: 0.6574770212173462, training accuracy: 0.59375\n",
            "Epoch: 296, training loss: 0.6568421125411987, training accuracy: 0.59375\n",
            "Epoch: 297, training loss: 0.6442822813987732, training accuracy: 0.71875\n",
            "Epoch: 298, training loss: 0.6698884963989258, training accuracy: 0.65625\n",
            "Epoch: 299, training loss: 0.576676607131958, training accuracy: 0.6875\n",
            "Epoch: 300, training loss: 0.6629446744918823, training accuracy: 0.625\n",
            "Epoch: 301, training loss: 0.6537931561470032, training accuracy: 0.5\n",
            "Epoch: 302, training loss: 0.6738735437393188, training accuracy: 0.5625\n",
            "Epoch: 303, training loss: 0.5938587188720703, training accuracy: 0.6875\n",
            "Epoch: 304, training loss: 0.638613760471344, training accuracy: 0.578125\n",
            "Epoch: 305, training loss: 0.6360746622085571, training accuracy: 0.671875\n",
            "Epoch: 306, training loss: 0.660490870475769, training accuracy: 0.625\n",
            "Epoch: 307, training loss: 0.6027078628540039, training accuracy: 0.703125\n",
            "Epoch: 308, training loss: 0.7127575874328613, training accuracy: 0.625\n",
            "Epoch: 309, training loss: 0.6578713059425354, training accuracy: 0.5625\n",
            "Epoch: 310, training loss: 0.5799888372421265, training accuracy: 0.703125\n",
            "Epoch: 311, training loss: 0.6267457008361816, training accuracy: 0.59375\n",
            "Epoch: 312, training loss: 0.6592725515365601, training accuracy: 0.640625\n",
            "Epoch: 313, training loss: 0.6739587187767029, training accuracy: 0.609375\n",
            "Epoch: 314, training loss: 0.62955641746521, training accuracy: 0.640625\n",
            "Epoch: 315, training loss: 0.6931925415992737, training accuracy: 0.609375\n",
            "Epoch: 316, training loss: 0.5969892740249634, training accuracy: 0.703125\n",
            "Epoch: 317, training loss: 0.7495140433311462, training accuracy: 0.4375\n",
            "Epoch: 318, training loss: 0.6911376714706421, training accuracy: 0.578125\n",
            "Epoch: 319, training loss: 0.6143819689750671, training accuracy: 0.640625\n",
            "Epoch: 320, training loss: 0.648564338684082, training accuracy: 0.59375\n",
            "Epoch: 321, training loss: 0.6227855086326599, training accuracy: 0.65625\n",
            "Epoch: 322, training loss: 0.6393579244613647, training accuracy: 0.640625\n",
            "Epoch: 323, training loss: 0.6442130208015442, training accuracy: 0.671875\n",
            "Epoch: 324, training loss: 0.7076578736305237, training accuracy: 0.5625\n",
            "Epoch: 325, training loss: 0.6323150396347046, training accuracy: 0.65625\n",
            "Epoch: 326, training loss: 0.6523703336715698, training accuracy: 0.71875\n",
            "Epoch: 327, training loss: 0.6232362985610962, training accuracy: 0.6875\n",
            "Epoch: 328, training loss: 0.6587241291999817, training accuracy: 0.515625\n",
            "Epoch: 329, training loss: 0.6474353075027466, training accuracy: 0.65625\n",
            "Epoch: 330, training loss: 0.6057435870170593, training accuracy: 0.71875\n",
            "Epoch: 331, training loss: 0.6511763334274292, training accuracy: 0.59375\n",
            "Epoch: 332, training loss: 0.6200305223464966, training accuracy: 0.71875\n",
            "Epoch: 333, training loss: 0.6757216453552246, training accuracy: 0.546875\n",
            "Epoch: 334, training loss: 0.6244255304336548, training accuracy: 0.671875\n",
            "Epoch: 335, training loss: 0.6369620561599731, training accuracy: 0.671875\n",
            "Epoch: 336, training loss: 0.6676393151283264, training accuracy: 0.578125\n",
            "Epoch: 337, training loss: 0.688256561756134, training accuracy: 0.515625\n",
            "Epoch: 338, training loss: 0.6339893937110901, training accuracy: 0.65625\n",
            "Epoch: 339, training loss: 0.6394331455230713, training accuracy: 0.609375\n",
            "Epoch: 340, training loss: 0.6169615387916565, training accuracy: 0.671875\n",
            "Epoch: 341, training loss: 0.6560191512107849, training accuracy: 0.640625\n",
            "Epoch: 342, training loss: 0.700826108455658, training accuracy: 0.5\n",
            "Epoch: 343, training loss: 0.632885754108429, training accuracy: 0.671875\n",
            "Epoch: 344, training loss: 0.5719782114028931, training accuracy: 0.78125\n",
            "Epoch: 345, training loss: 0.6772964000701904, training accuracy: 0.625\n",
            "Epoch: 346, training loss: 0.6011093854904175, training accuracy: 0.625\n",
            "Epoch: 347, training loss: 0.671379566192627, training accuracy: 0.609375\n",
            "Epoch: 348, training loss: 0.6198612451553345, training accuracy: 0.625\n",
            "Epoch: 349, training loss: 0.6268743276596069, training accuracy: 0.671875\n",
            "Epoch: 350, training loss: 0.6690497398376465, training accuracy: 0.59375\n",
            "Epoch: 351, training loss: 0.610628068447113, training accuracy: 0.71875\n",
            "Epoch: 352, training loss: 0.6249133348464966, training accuracy: 0.703125\n",
            "Epoch: 353, training loss: 0.6644561290740967, training accuracy: 0.640625\n",
            "Epoch: 354, training loss: 0.619328498840332, training accuracy: 0.65625\n",
            "Epoch: 355, training loss: 0.6567421555519104, training accuracy: 0.625\n",
            "Epoch: 356, training loss: 0.6655714511871338, training accuracy: 0.59375\n",
            "Epoch: 357, training loss: 0.636039137840271, training accuracy: 0.578125\n",
            "Epoch: 358, training loss: 0.6484243869781494, training accuracy: 0.640625\n",
            "Epoch: 359, training loss: 0.6642510890960693, training accuracy: 0.578125\n",
            "Epoch: 360, training loss: 0.6748900413513184, training accuracy: 0.640625\n",
            "Epoch: 361, training loss: 0.6382368803024292, training accuracy: 0.5625\n",
            "Epoch: 362, training loss: 0.590563178062439, training accuracy: 0.6875\n",
            "Epoch: 363, training loss: 0.6541014909744263, training accuracy: 0.640625\n",
            "Epoch: 364, training loss: 0.5820969343185425, training accuracy: 0.75\n",
            "Epoch: 365, training loss: 0.669887900352478, training accuracy: 0.5625\n",
            "Epoch: 366, training loss: 0.6505608558654785, training accuracy: 0.59375\n",
            "Epoch: 367, training loss: 0.6873865127563477, training accuracy: 0.53125\n",
            "Epoch: 368, training loss: 0.6885334849357605, training accuracy: 0.609375\n",
            "Epoch: 369, training loss: 0.6568974256515503, training accuracy: 0.5625\n",
            "Epoch: 370, training loss: 0.5937761068344116, training accuracy: 0.640625\n",
            "Epoch: 371, training loss: 0.5863127708435059, training accuracy: 0.671875\n",
            "Epoch: 372, training loss: 0.617888331413269, training accuracy: 0.6875\n",
            "Epoch: 373, training loss: 0.5537270307540894, training accuracy: 0.765625\n",
            "Epoch: 374, training loss: 0.6846059560775757, training accuracy: 0.515625\n",
            "Epoch: 375, training loss: 0.6179482936859131, training accuracy: 0.703125\n",
            "Epoch: 376, training loss: 0.5968256592750549, training accuracy: 0.75\n",
            "Epoch: 377, training loss: 0.6422038674354553, training accuracy: 0.625\n",
            "Epoch: 378, training loss: 0.6704725027084351, training accuracy: 0.59375\n",
            "Epoch: 379, training loss: 0.6438713073730469, training accuracy: 0.609375\n",
            "Epoch: 380, training loss: 0.7161921262741089, training accuracy: 0.53125\n",
            "Epoch: 381, training loss: 0.6516516208648682, training accuracy: 0.703125\n",
            "Epoch: 382, training loss: 0.6921262145042419, training accuracy: 0.5625\n",
            "Epoch: 383, training loss: 0.6472945213317871, training accuracy: 0.59375\n",
            "Epoch: 384, training loss: 0.6639416217803955, training accuracy: 0.59375\n",
            "Epoch: 385, training loss: 0.7025293111801147, training accuracy: 0.578125\n",
            "Epoch: 386, training loss: 0.6457566618919373, training accuracy: 0.625\n",
            "Epoch: 387, training loss: 0.6312834024429321, training accuracy: 0.65625\n",
            "Epoch: 388, training loss: 0.6619486808776855, training accuracy: 0.578125\n",
            "Epoch: 389, training loss: 0.6580933332443237, training accuracy: 0.59375\n",
            "Epoch: 390, training loss: 0.6390700340270996, training accuracy: 0.671875\n",
            "Epoch: 391, training loss: 0.703590989112854, training accuracy: 0.515625\n",
            "Epoch: 392, training loss: 0.6389604806900024, training accuracy: 0.640625\n",
            "Epoch: 393, training loss: 0.6484652161598206, training accuracy: 0.59375\n",
            "Epoch: 394, training loss: 0.6143296957015991, training accuracy: 0.671875\n",
            "Epoch: 395, training loss: 0.680785059928894, training accuracy: 0.53125\n",
            "Epoch: 396, training loss: 0.6311451196670532, training accuracy: 0.625\n",
            "Epoch: 397, training loss: 0.6817027926445007, training accuracy: 0.578125\n",
            "Epoch: 398, training loss: 0.670378565788269, training accuracy: 0.5625\n",
            "Epoch: 399, training loss: 0.624365508556366, training accuracy: 0.6875\n",
            "Epoch: 400, training loss: 0.6522049903869629, training accuracy: 0.640625\n",
            "Epoch: 401, training loss: 0.6402379274368286, training accuracy: 0.640625\n",
            "Epoch: 402, training loss: 0.6841357946395874, training accuracy: 0.578125\n",
            "Epoch: 403, training loss: 0.6754271984100342, training accuracy: 0.546875\n",
            "Epoch: 404, training loss: 0.6439682245254517, training accuracy: 0.6875\n",
            "Epoch: 405, training loss: 0.6341738700866699, training accuracy: 0.640625\n",
            "Epoch: 406, training loss: 0.6257553100585938, training accuracy: 0.671875\n",
            "Epoch: 407, training loss: 0.6670653820037842, training accuracy: 0.59375\n",
            "Epoch: 408, training loss: 0.6689020991325378, training accuracy: 0.59375\n",
            "Epoch: 409, training loss: 0.6807118654251099, training accuracy: 0.515625\n",
            "Epoch: 410, training loss: 0.5735160112380981, training accuracy: 0.734375\n",
            "Epoch: 411, training loss: 0.6328369379043579, training accuracy: 0.703125\n",
            "Epoch: 412, training loss: 0.6205459237098694, training accuracy: 0.71875\n",
            "Epoch: 413, training loss: 0.669670820236206, training accuracy: 0.59375\n",
            "Epoch: 414, training loss: 0.617412269115448, training accuracy: 0.6875\n",
            "Epoch: 415, training loss: 0.6385282278060913, training accuracy: 0.640625\n",
            "Epoch: 416, training loss: 0.6539421081542969, training accuracy: 0.671875\n",
            "Epoch: 417, training loss: 0.6082847118377686, training accuracy: 0.671875\n",
            "Epoch: 418, training loss: 0.73615962266922, training accuracy: 0.421875\n",
            "Epoch: 419, training loss: 0.6829039454460144, training accuracy: 0.578125\n",
            "Epoch: 420, training loss: 0.713581919670105, training accuracy: 0.59375\n",
            "Epoch: 421, training loss: 0.6098380088806152, training accuracy: 0.65625\n",
            "Epoch: 422, training loss: 0.6538053750991821, training accuracy: 0.625\n",
            "Epoch: 423, training loss: 0.6527769565582275, training accuracy: 0.609375\n",
            "Epoch: 424, training loss: 0.6678382158279419, training accuracy: 0.59375\n",
            "Epoch: 425, training loss: 0.704373836517334, training accuracy: 0.546875\n",
            "Epoch: 426, training loss: 0.6128768920898438, training accuracy: 0.625\n",
            "Epoch: 427, training loss: 0.6560676097869873, training accuracy: 0.65625\n",
            "Epoch: 428, training loss: 0.7023722529411316, training accuracy: 0.5625\n",
            "Epoch: 429, training loss: 0.6385196447372437, training accuracy: 0.65625\n",
            "Epoch: 430, training loss: 0.6345642805099487, training accuracy: 0.703125\n",
            "Epoch: 431, training loss: 0.7118276953697205, training accuracy: 0.484375\n",
            "Epoch: 432, training loss: 0.6337502598762512, training accuracy: 0.65625\n",
            "Epoch: 433, training loss: 0.6638744473457336, training accuracy: 0.609375\n",
            "Epoch: 434, training loss: 0.6329751014709473, training accuracy: 0.671875\n",
            "Epoch: 435, training loss: 0.6864573359489441, training accuracy: 0.625\n",
            "Epoch: 436, training loss: 0.6542420387268066, training accuracy: 0.59375\n",
            "Epoch: 437, training loss: 0.6616407632827759, training accuracy: 0.640625\n",
            "Epoch: 438, training loss: 0.6769853830337524, training accuracy: 0.6875\n",
            "Epoch: 439, training loss: 0.6310619115829468, training accuracy: 0.625\n",
            "Epoch: 440, training loss: 0.614017128944397, training accuracy: 0.6875\n",
            "Epoch: 441, training loss: 0.5895483493804932, training accuracy: 0.71875\n",
            "Epoch: 442, training loss: 0.576712429523468, training accuracy: 0.71875\n",
            "Epoch: 443, training loss: 0.633275032043457, training accuracy: 0.703125\n",
            "Epoch: 444, training loss: 0.6018810868263245, training accuracy: 0.71875\n",
            "Epoch: 445, training loss: 0.6907007694244385, training accuracy: 0.5625\n",
            "Epoch: 446, training loss: 0.6081603765487671, training accuracy: 0.640625\n",
            "Epoch: 447, training loss: 0.6814389228820801, training accuracy: 0.640625\n",
            "Epoch: 448, training loss: 0.6916615962982178, training accuracy: 0.609375\n",
            "Epoch: 449, training loss: 0.6049082279205322, training accuracy: 0.65625\n",
            "Epoch: 450, training loss: 0.6188435554504395, training accuracy: 0.640625\n",
            "Epoch: 451, training loss: 0.6268450021743774, training accuracy: 0.703125\n",
            "Epoch: 452, training loss: 0.6399983167648315, training accuracy: 0.640625\n",
            "Epoch: 453, training loss: 0.5837322473526001, training accuracy: 0.71875\n",
            "Epoch: 454, training loss: 0.6890373229980469, training accuracy: 0.5\n",
            "Epoch: 455, training loss: 0.6933847665786743, training accuracy: 0.609375\n",
            "Epoch: 456, training loss: 0.5853911638259888, training accuracy: 0.6875\n",
            "Epoch: 457, training loss: 0.6576300859451294, training accuracy: 0.6875\n",
            "Epoch: 458, training loss: 0.6432617902755737, training accuracy: 0.6875\n",
            "Epoch: 459, training loss: 0.6082028746604919, training accuracy: 0.625\n",
            "Epoch: 460, training loss: 0.6364461183547974, training accuracy: 0.640625\n",
            "Epoch: 461, training loss: 0.6659225821495056, training accuracy: 0.625\n",
            "Epoch: 462, training loss: 0.5972509980201721, training accuracy: 0.6875\n",
            "Epoch: 463, training loss: 0.6300660967826843, training accuracy: 0.78125\n",
            "Epoch: 464, training loss: 0.6562069654464722, training accuracy: 0.6875\n",
            "Epoch: 465, training loss: 0.637333869934082, training accuracy: 0.640625\n",
            "Epoch: 466, training loss: 0.5857902765274048, training accuracy: 0.625\n",
            "Epoch: 467, training loss: 0.5935020446777344, training accuracy: 0.6875\n",
            "Epoch: 468, training loss: 0.5434948205947876, training accuracy: 0.765625\n",
            "Epoch: 469, training loss: 0.6486232280731201, training accuracy: 0.625\n",
            "Epoch: 470, training loss: 0.6909062266349792, training accuracy: 0.5625\n",
            "Epoch: 471, training loss: 0.6884398460388184, training accuracy: 0.5625\n",
            "Epoch: 472, training loss: 0.6247553825378418, training accuracy: 0.625\n",
            "Epoch: 473, training loss: 0.586108922958374, training accuracy: 0.671875\n",
            "Epoch: 474, training loss: 0.5971401333808899, training accuracy: 0.765625\n",
            "Epoch: 475, training loss: 0.6605049967765808, training accuracy: 0.59375\n",
            "Epoch: 476, training loss: 0.6928909420967102, training accuracy: 0.625\n",
            "Epoch: 477, training loss: 0.5965908765792847, training accuracy: 0.640625\n",
            "Epoch: 478, training loss: 0.6176174879074097, training accuracy: 0.65625\n",
            "Epoch: 479, training loss: 0.6182658672332764, training accuracy: 0.671875\n",
            "Epoch: 480, training loss: 0.6555535197257996, training accuracy: 0.625\n",
            "Epoch: 481, training loss: 0.6555260419845581, training accuracy: 0.65625\n",
            "Epoch: 482, training loss: 0.6821063756942749, training accuracy: 0.625\n",
            "Epoch: 483, training loss: 0.6140905618667603, training accuracy: 0.671875\n",
            "Epoch: 484, training loss: 0.6585477590560913, training accuracy: 0.640625\n",
            "Epoch: 485, training loss: 0.6639809012413025, training accuracy: 0.625\n",
            "Epoch: 486, training loss: 0.7154920101165771, training accuracy: 0.53125\n",
            "Epoch: 487, training loss: 0.6852689981460571, training accuracy: 0.546875\n",
            "Epoch: 488, training loss: 0.6736897826194763, training accuracy: 0.53125\n",
            "Epoch: 489, training loss: 0.6264151334762573, training accuracy: 0.609375\n",
            "Epoch: 490, training loss: 0.6568863391876221, training accuracy: 0.640625\n",
            "Epoch: 491, training loss: 0.6388781666755676, training accuracy: 0.640625\n",
            "Epoch: 492, training loss: 0.6580231785774231, training accuracy: 0.6875\n",
            "Epoch: 493, training loss: 0.6278601884841919, training accuracy: 0.75\n",
            "Epoch: 494, training loss: 0.6307046413421631, training accuracy: 0.625\n",
            "Epoch: 495, training loss: 0.6298040747642517, training accuracy: 0.671875\n",
            "Epoch: 496, training loss: 0.635709285736084, training accuracy: 0.625\n",
            "Epoch: 497, training loss: 0.6811418533325195, training accuracy: 0.5\n",
            "Epoch: 498, training loss: 0.5827905535697937, training accuracy: 0.703125\n",
            "Epoch: 499, training loss: 0.6565778851509094, training accuracy: 0.671875\n",
            "Epoch: 500, training loss: 0.6310976147651672, training accuracy: 0.625\n",
            "\u001b[31m Validation loss: 0.6456559300422668, validation accuracy: 0.6065965583173997 \u001b[0m\n",
            "Epoch: 501, training loss: 0.6026349067687988, training accuracy: 0.6875\n",
            "Epoch: 502, training loss: 0.6288577318191528, training accuracy: 0.625\n",
            "Epoch: 503, training loss: 0.6324865818023682, training accuracy: 0.71875\n",
            "Epoch: 504, training loss: 0.7077152132987976, training accuracy: 0.5625\n",
            "Epoch: 505, training loss: 0.6124254465103149, training accuracy: 0.703125\n",
            "Epoch: 506, training loss: 0.6566080451011658, training accuracy: 0.609375\n",
            "Epoch: 507, training loss: 0.6452845931053162, training accuracy: 0.640625\n",
            "Epoch: 508, training loss: 0.6620734930038452, training accuracy: 0.609375\n",
            "Epoch: 509, training loss: 0.6727365851402283, training accuracy: 0.59375\n",
            "Epoch: 510, training loss: 0.6827002763748169, training accuracy: 0.546875\n",
            "Epoch: 511, training loss: 0.6996673345565796, training accuracy: 0.546875\n",
            "Epoch: 512, training loss: 0.6646672487258911, training accuracy: 0.671875\n",
            "Epoch: 513, training loss: 0.708301305770874, training accuracy: 0.5625\n",
            "Epoch: 514, training loss: 0.6490902304649353, training accuracy: 0.609375\n",
            "Epoch: 515, training loss: 0.6328182816505432, training accuracy: 0.671875\n",
            "Epoch: 516, training loss: 0.6651564836502075, training accuracy: 0.625\n",
            "Epoch: 517, training loss: 0.6228348612785339, training accuracy: 0.59375\n",
            "Epoch: 518, training loss: 0.6777167320251465, training accuracy: 0.59375\n",
            "Epoch: 519, training loss: 0.7139132022857666, training accuracy: 0.5625\n",
            "Epoch: 520, training loss: 0.6885944604873657, training accuracy: 0.5625\n",
            "Epoch: 521, training loss: 0.5822746753692627, training accuracy: 0.65625\n",
            "Epoch: 522, training loss: 0.6289293766021729, training accuracy: 0.65625\n",
            "Epoch: 523, training loss: 0.6572780609130859, training accuracy: 0.640625\n",
            "Epoch: 524, training loss: 0.6892915368080139, training accuracy: 0.546875\n",
            "Epoch: 525, training loss: 0.658673882484436, training accuracy: 0.59375\n",
            "Epoch: 526, training loss: 0.6130163669586182, training accuracy: 0.640625\n",
            "Epoch: 527, training loss: 0.6157864332199097, training accuracy: 0.625\n",
            "Epoch: 528, training loss: 0.5819182395935059, training accuracy: 0.765625\n",
            "Epoch: 529, training loss: 0.6060144901275635, training accuracy: 0.65625\n",
            "Epoch: 530, training loss: 0.6484219431877136, training accuracy: 0.5625\n",
            "Epoch: 531, training loss: 0.6394018530845642, training accuracy: 0.609375\n",
            "Epoch: 532, training loss: 0.6127839088439941, training accuracy: 0.609375\n",
            "Epoch: 533, training loss: 0.6044284105300903, training accuracy: 0.65625\n",
            "Epoch: 534, training loss: 0.7184318900108337, training accuracy: 0.546875\n",
            "Epoch: 535, training loss: 0.6034125089645386, training accuracy: 0.640625\n",
            "Epoch: 536, training loss: 0.653188943862915, training accuracy: 0.578125\n",
            "Epoch: 537, training loss: 0.6197926998138428, training accuracy: 0.640625\n",
            "Epoch: 538, training loss: 0.5777193307876587, training accuracy: 0.671875\n",
            "Epoch: 539, training loss: 0.6428163051605225, training accuracy: 0.65625\n",
            "Epoch: 540, training loss: 0.6419488191604614, training accuracy: 0.640625\n",
            "Epoch: 541, training loss: 0.6054808497428894, training accuracy: 0.703125\n",
            "Epoch: 542, training loss: 0.6867302060127258, training accuracy: 0.578125\n",
            "Epoch: 543, training loss: 0.6391603946685791, training accuracy: 0.625\n",
            "Epoch: 544, training loss: 0.6488624811172485, training accuracy: 0.625\n",
            "Epoch: 545, training loss: 0.6409302949905396, training accuracy: 0.640625\n",
            "Epoch: 546, training loss: 0.6249894499778748, training accuracy: 0.609375\n",
            "Epoch: 547, training loss: 0.6755520105361938, training accuracy: 0.578125\n",
            "Epoch: 548, training loss: 0.6185513138771057, training accuracy: 0.609375\n",
            "Epoch: 549, training loss: 0.5863397121429443, training accuracy: 0.671875\n",
            "Epoch: 550, training loss: 0.6186520457267761, training accuracy: 0.640625\n",
            "Epoch: 551, training loss: 0.6403621435165405, training accuracy: 0.625\n",
            "Epoch: 552, training loss: 0.6282621026039124, training accuracy: 0.640625\n",
            "Epoch: 553, training loss: 0.6346035003662109, training accuracy: 0.6875\n",
            "Epoch: 554, training loss: 0.6001063585281372, training accuracy: 0.671875\n",
            "Epoch: 555, training loss: 0.6589359641075134, training accuracy: 0.625\n",
            "Epoch: 556, training loss: 0.6696401834487915, training accuracy: 0.5625\n",
            "Epoch: 557, training loss: 0.6293838620185852, training accuracy: 0.65625\n",
            "Epoch: 558, training loss: 0.5943131446838379, training accuracy: 0.734375\n",
            "Epoch: 559, training loss: 0.5778104066848755, training accuracy: 0.578125\n",
            "Epoch: 560, training loss: 0.6593274474143982, training accuracy: 0.59375\n",
            "Epoch: 561, training loss: 0.6398237347602844, training accuracy: 0.65625\n",
            "Epoch: 562, training loss: 0.6098397970199585, training accuracy: 0.671875\n",
            "Epoch: 563, training loss: 0.6427017450332642, training accuracy: 0.59375\n",
            "Epoch: 564, training loss: 0.6851470470428467, training accuracy: 0.578125\n",
            "Epoch: 565, training loss: 0.6947617530822754, training accuracy: 0.5625\n",
            "Epoch: 566, training loss: 0.7029322385787964, training accuracy: 0.625\n",
            "Epoch: 567, training loss: 0.7157498002052307, training accuracy: 0.5\n",
            "Epoch: 568, training loss: 0.5897864103317261, training accuracy: 0.625\n",
            "Epoch: 569, training loss: 0.661601185798645, training accuracy: 0.625\n",
            "Epoch: 570, training loss: 0.6921671032905579, training accuracy: 0.546875\n",
            "Epoch: 571, training loss: 0.6679559946060181, training accuracy: 0.609375\n",
            "Epoch: 572, training loss: 0.6145930886268616, training accuracy: 0.640625\n",
            "Epoch: 573, training loss: 0.6585918664932251, training accuracy: 0.578125\n",
            "Epoch: 574, training loss: 0.6897568106651306, training accuracy: 0.5625\n",
            "Epoch: 575, training loss: 0.7336305379867554, training accuracy: 0.453125\n",
            "Epoch: 576, training loss: 0.5967919826507568, training accuracy: 0.71875\n",
            "Epoch: 577, training loss: 0.6798433661460876, training accuracy: 0.515625\n",
            "Epoch: 578, training loss: 0.6413776278495789, training accuracy: 0.625\n",
            "Epoch: 579, training loss: 0.668045163154602, training accuracy: 0.609375\n",
            "Epoch: 580, training loss: 0.6674609184265137, training accuracy: 0.5625\n",
            "Epoch: 581, training loss: 0.6899092197418213, training accuracy: 0.5625\n",
            "Epoch: 582, training loss: 0.5964674949645996, training accuracy: 0.71875\n",
            "Epoch: 583, training loss: 0.5894148945808411, training accuracy: 0.6875\n",
            "Epoch: 584, training loss: 0.6312360167503357, training accuracy: 0.59375\n",
            "Epoch: 585, training loss: 0.6560646295547485, training accuracy: 0.59375\n",
            "Epoch: 586, training loss: 0.6048551797866821, training accuracy: 0.640625\n",
            "Epoch: 587, training loss: 0.6411389112472534, training accuracy: 0.640625\n",
            "Epoch: 588, training loss: 0.6522003412246704, training accuracy: 0.671875\n",
            "Epoch: 589, training loss: 0.6107563972473145, training accuracy: 0.671875\n",
            "Epoch: 590, training loss: 0.6683756113052368, training accuracy: 0.578125\n",
            "Epoch: 591, training loss: 0.6746349334716797, training accuracy: 0.578125\n",
            "Epoch: 592, training loss: 0.643112063407898, training accuracy: 0.578125\n",
            "Epoch: 593, training loss: 0.6699298620223999, training accuracy: 0.546875\n",
            "Epoch: 594, training loss: 0.6237838268280029, training accuracy: 0.6875\n",
            "Epoch: 595, training loss: 0.6310434937477112, training accuracy: 0.609375\n",
            "Epoch: 596, training loss: 0.6900609731674194, training accuracy: 0.625\n",
            "Epoch: 597, training loss: 0.7025600671768188, training accuracy: 0.578125\n",
            "Epoch: 598, training loss: 0.6469202041625977, training accuracy: 0.625\n",
            "Epoch: 599, training loss: 0.6591343283653259, training accuracy: 0.609375\n",
            "Epoch: 600, training loss: 0.5558770895004272, training accuracy: 0.765625\n",
            "Epoch: 601, training loss: 0.6415965557098389, training accuracy: 0.59375\n",
            "Epoch: 602, training loss: 0.603049635887146, training accuracy: 0.703125\n",
            "Epoch: 603, training loss: 0.6767638921737671, training accuracy: 0.609375\n",
            "Epoch: 604, training loss: 0.6178983449935913, training accuracy: 0.609375\n",
            "Epoch: 605, training loss: 0.5828319191932678, training accuracy: 0.609375\n",
            "Epoch: 606, training loss: 0.5934700965881348, training accuracy: 0.671875\n",
            "Epoch: 607, training loss: 0.6696352958679199, training accuracy: 0.65625\n",
            "Epoch: 608, training loss: 0.580264151096344, training accuracy: 0.671875\n",
            "Epoch: 609, training loss: 0.6038026809692383, training accuracy: 0.65625\n",
            "Epoch: 610, training loss: 0.6427024602890015, training accuracy: 0.59375\n",
            "Epoch: 611, training loss: 0.6612061262130737, training accuracy: 0.53125\n",
            "Epoch: 612, training loss: 0.7042797803878784, training accuracy: 0.546875\n",
            "Epoch: 613, training loss: 0.580693781375885, training accuracy: 0.6875\n",
            "Epoch: 614, training loss: 0.5881191492080688, training accuracy: 0.734375\n",
            "Epoch: 615, training loss: 0.644399106502533, training accuracy: 0.625\n",
            "Epoch: 616, training loss: 0.6295286417007446, training accuracy: 0.703125\n",
            "Epoch: 617, training loss: 0.6510207653045654, training accuracy: 0.671875\n",
            "Epoch: 618, training loss: 0.6144230365753174, training accuracy: 0.65625\n",
            "Epoch: 619, training loss: 0.7028104066848755, training accuracy: 0.640625\n",
            "Epoch: 620, training loss: 0.6840728521347046, training accuracy: 0.515625\n",
            "Epoch: 621, training loss: 0.7102618217468262, training accuracy: 0.5625\n",
            "Epoch: 622, training loss: 0.6374479532241821, training accuracy: 0.625\n",
            "Epoch: 623, training loss: 0.653704047203064, training accuracy: 0.5625\n",
            "Epoch: 624, training loss: 0.6409094929695129, training accuracy: 0.640625\n",
            "Epoch: 625, training loss: 0.6578223705291748, training accuracy: 0.59375\n",
            "Epoch: 626, training loss: 0.6313764452934265, training accuracy: 0.703125\n",
            "Epoch: 627, training loss: 0.6149872541427612, training accuracy: 0.65625\n",
            "Epoch: 628, training loss: 0.6223503351211548, training accuracy: 0.65625\n",
            "Epoch: 629, training loss: 0.6854974031448364, training accuracy: 0.5625\n",
            "Epoch: 630, training loss: 0.6155965328216553, training accuracy: 0.640625\n",
            "Epoch: 631, training loss: 0.6228445768356323, training accuracy: 0.5625\n",
            "Epoch: 632, training loss: 0.6258884072303772, training accuracy: 0.671875\n",
            "Epoch: 633, training loss: 0.6260453462600708, training accuracy: 0.625\n",
            "Epoch: 634, training loss: 0.6527508497238159, training accuracy: 0.65625\n",
            "Epoch: 635, training loss: 0.6396111249923706, training accuracy: 0.640625\n",
            "Epoch: 636, training loss: 0.6065562963485718, training accuracy: 0.703125\n",
            "Epoch: 637, training loss: 0.6062771081924438, training accuracy: 0.65625\n",
            "Epoch: 638, training loss: 0.611000120639801, training accuracy: 0.6875\n",
            "Epoch: 639, training loss: 0.6249250769615173, training accuracy: 0.734375\n",
            "Epoch: 640, training loss: 0.5748098492622375, training accuracy: 0.671875\n",
            "Epoch: 641, training loss: 0.62477707862854, training accuracy: 0.65625\n",
            "Epoch: 642, training loss: 0.5986790060997009, training accuracy: 0.65625\n",
            "Epoch: 643, training loss: 0.6959313750267029, training accuracy: 0.5625\n",
            "Epoch: 644, training loss: 0.6974056959152222, training accuracy: 0.59375\n",
            "Epoch: 645, training loss: 0.6016817092895508, training accuracy: 0.71875\n",
            "Epoch: 646, training loss: 0.6961455941200256, training accuracy: 0.59375\n",
            "Epoch: 647, training loss: 0.6360015869140625, training accuracy: 0.640625\n",
            "Epoch: 648, training loss: 0.6041674613952637, training accuracy: 0.6875\n",
            "Epoch: 649, training loss: 0.6798642873764038, training accuracy: 0.5625\n",
            "Epoch: 650, training loss: 0.6861876249313354, training accuracy: 0.5625\n",
            "Epoch: 651, training loss: 0.6244195103645325, training accuracy: 0.640625\n",
            "Epoch: 652, training loss: 0.6287162899971008, training accuracy: 0.671875\n",
            "Epoch: 653, training loss: 0.641557514667511, training accuracy: 0.59375\n",
            "Epoch: 654, training loss: 0.6544677019119263, training accuracy: 0.65625\n",
            "Epoch: 655, training loss: 0.6241366863250732, training accuracy: 0.65625\n",
            "Epoch: 656, training loss: 0.6670010089874268, training accuracy: 0.625\n",
            "Epoch: 657, training loss: 0.6057474613189697, training accuracy: 0.703125\n",
            "Epoch: 658, training loss: 0.6465121507644653, training accuracy: 0.625\n",
            "Epoch: 659, training loss: 0.6536483764648438, training accuracy: 0.609375\n",
            "Epoch: 660, training loss: 0.6199771761894226, training accuracy: 0.6875\n",
            "Epoch: 661, training loss: 0.6868034601211548, training accuracy: 0.625\n",
            "Epoch: 662, training loss: 0.6561221480369568, training accuracy: 0.671875\n",
            "Epoch: 663, training loss: 0.6852084994316101, training accuracy: 0.59375\n",
            "Epoch: 664, training loss: 0.6871312856674194, training accuracy: 0.65625\n",
            "Epoch: 665, training loss: 0.6160278916358948, training accuracy: 0.640625\n",
            "Epoch: 666, training loss: 0.6258448362350464, training accuracy: 0.75\n",
            "Epoch: 667, training loss: 0.5892090201377869, training accuracy: 0.71875\n",
            "Epoch: 668, training loss: 0.6377094984054565, training accuracy: 0.6875\n",
            "Epoch: 669, training loss: 0.637984037399292, training accuracy: 0.671875\n",
            "Epoch: 670, training loss: 0.5803510546684265, training accuracy: 0.671875\n",
            "Epoch: 671, training loss: 0.6728261709213257, training accuracy: 0.703125\n",
            "Epoch: 672, training loss: 0.6483839750289917, training accuracy: 0.671875\n",
            "Epoch: 673, training loss: 0.634655237197876, training accuracy: 0.578125\n",
            "Epoch: 674, training loss: 0.6365749835968018, training accuracy: 0.6875\n",
            "Epoch: 675, training loss: 0.6748106479644775, training accuracy: 0.703125\n",
            "Epoch: 676, training loss: 0.6455050706863403, training accuracy: 0.671875\n",
            "Epoch: 677, training loss: 0.5727381706237793, training accuracy: 0.734375\n",
            "Epoch: 678, training loss: 0.5760997533798218, training accuracy: 0.765625\n",
            "Epoch: 679, training loss: 0.6340408325195312, training accuracy: 0.578125\n",
            "Epoch: 680, training loss: 0.6912322044372559, training accuracy: 0.578125\n",
            "Epoch: 681, training loss: 0.6271224021911621, training accuracy: 0.640625\n",
            "Epoch: 682, training loss: 0.6140121221542358, training accuracy: 0.671875\n",
            "Epoch: 683, training loss: 0.5212984085083008, training accuracy: 0.75\n",
            "Epoch: 684, training loss: 0.663658082485199, training accuracy: 0.5625\n",
            "Epoch: 685, training loss: 0.6416752338409424, training accuracy: 0.65625\n",
            "Epoch: 686, training loss: 0.6284899711608887, training accuracy: 0.6875\n",
            "Epoch: 687, training loss: 0.7021600604057312, training accuracy: 0.578125\n",
            "Epoch: 688, training loss: 0.6307058334350586, training accuracy: 0.65625\n",
            "Epoch: 689, training loss: 0.5735578536987305, training accuracy: 0.703125\n",
            "Epoch: 690, training loss: 0.6416493654251099, training accuracy: 0.6875\n",
            "Epoch: 691, training loss: 0.5933086276054382, training accuracy: 0.703125\n",
            "Epoch: 692, training loss: 0.5980793833732605, training accuracy: 0.6875\n",
            "Epoch: 693, training loss: 0.6035929322242737, training accuracy: 0.671875\n",
            "Epoch: 694, training loss: 0.6434338092803955, training accuracy: 0.671875\n",
            "Epoch: 695, training loss: 0.7044911980628967, training accuracy: 0.5625\n",
            "Epoch: 696, training loss: 0.6397725343704224, training accuracy: 0.734375\n",
            "Epoch: 697, training loss: 0.6292755603790283, training accuracy: 0.609375\n",
            "Epoch: 698, training loss: 0.6423801183700562, training accuracy: 0.640625\n",
            "Epoch: 699, training loss: 0.6677931547164917, training accuracy: 0.578125\n",
            "Epoch: 700, training loss: 0.6667194366455078, training accuracy: 0.578125\n",
            "Epoch: 701, training loss: 0.6825501322746277, training accuracy: 0.609375\n",
            "Epoch: 702, training loss: 0.6358875632286072, training accuracy: 0.671875\n",
            "Epoch: 703, training loss: 0.6116012930870056, training accuracy: 0.6875\n",
            "Epoch: 704, training loss: 0.5542744398117065, training accuracy: 0.765625\n",
            "Epoch: 705, training loss: 0.6224116683006287, training accuracy: 0.65625\n",
            "Epoch: 706, training loss: 0.6361500024795532, training accuracy: 0.65625\n",
            "Epoch: 707, training loss: 0.6798447370529175, training accuracy: 0.609375\n",
            "Epoch: 708, training loss: 0.6685875654220581, training accuracy: 0.5625\n",
            "Epoch: 709, training loss: 0.6620306968688965, training accuracy: 0.59375\n",
            "Epoch: 710, training loss: 0.6453757882118225, training accuracy: 0.703125\n",
            "Epoch: 711, training loss: 0.6793349981307983, training accuracy: 0.546875\n",
            "Epoch: 712, training loss: 0.6058678030967712, training accuracy: 0.671875\n",
            "Epoch: 713, training loss: 0.6764273643493652, training accuracy: 0.640625\n",
            "Epoch: 714, training loss: 0.6649084091186523, training accuracy: 0.59375\n",
            "Epoch: 715, training loss: 0.5839134454727173, training accuracy: 0.6875\n",
            "Epoch: 716, training loss: 0.6271182298660278, training accuracy: 0.671875\n",
            "Epoch: 717, training loss: 0.6198748350143433, training accuracy: 0.65625\n",
            "Epoch: 718, training loss: 0.6441628932952881, training accuracy: 0.671875\n",
            "Epoch: 719, training loss: 0.6699610948562622, training accuracy: 0.5\n",
            "Epoch: 720, training loss: 0.6355228424072266, training accuracy: 0.546875\n",
            "Epoch: 721, training loss: 0.6004738211631775, training accuracy: 0.640625\n",
            "Epoch: 722, training loss: 0.632297694683075, training accuracy: 0.65625\n",
            "Epoch: 723, training loss: 0.6322462558746338, training accuracy: 0.625\n",
            "Epoch: 724, training loss: 0.6730082035064697, training accuracy: 0.515625\n",
            "Epoch: 725, training loss: 0.6227135062217712, training accuracy: 0.6875\n",
            "Epoch: 726, training loss: 0.6960306167602539, training accuracy: 0.609375\n",
            "Epoch: 727, training loss: 0.5766435861587524, training accuracy: 0.65625\n",
            "Epoch: 728, training loss: 0.6211795210838318, training accuracy: 0.6875\n",
            "Epoch: 729, training loss: 0.6075292229652405, training accuracy: 0.671875\n",
            "Epoch: 730, training loss: 0.6887568831443787, training accuracy: 0.5625\n",
            "Epoch: 731, training loss: 0.631159782409668, training accuracy: 0.71875\n",
            "Epoch: 732, training loss: 0.5857172012329102, training accuracy: 0.6875\n",
            "Epoch: 733, training loss: 0.6198492050170898, training accuracy: 0.609375\n",
            "Epoch: 734, training loss: 0.6763349771499634, training accuracy: 0.578125\n",
            "Epoch: 735, training loss: 0.59293133020401, training accuracy: 0.625\n",
            "Epoch: 736, training loss: 0.6728644371032715, training accuracy: 0.65625\n",
            "Epoch: 737, training loss: 0.6141701936721802, training accuracy: 0.609375\n",
            "Epoch: 738, training loss: 0.5934692621231079, training accuracy: 0.734375\n",
            "Epoch: 739, training loss: 0.6312568783760071, training accuracy: 0.609375\n",
            "Epoch: 740, training loss: 0.6197553277015686, training accuracy: 0.671875\n",
            "Epoch: 741, training loss: 0.6903871893882751, training accuracy: 0.546875\n",
            "Epoch: 742, training loss: 0.6123566031455994, training accuracy: 0.671875\n",
            "Epoch: 743, training loss: 0.6992570757865906, training accuracy: 0.5625\n",
            "Epoch: 744, training loss: 0.5830259323120117, training accuracy: 0.6875\n",
            "Epoch: 745, training loss: 0.6571400761604309, training accuracy: 0.5625\n",
            "Epoch: 746, training loss: 0.6218794584274292, training accuracy: 0.65625\n",
            "Epoch: 747, training loss: 0.600101113319397, training accuracy: 0.75\n",
            "Epoch: 748, training loss: 0.6642202734947205, training accuracy: 0.59375\n",
            "Epoch: 749, training loss: 0.6483006477355957, training accuracy: 0.59375\n",
            "Epoch: 750, training loss: 0.7416449785232544, training accuracy: 0.484375\n",
            "Epoch: 751, training loss: 0.5847852230072021, training accuracy: 0.71875\n",
            "Epoch: 752, training loss: 0.6186670064926147, training accuracy: 0.6875\n",
            "Epoch: 753, training loss: 0.5396448969841003, training accuracy: 0.703125\n",
            "Epoch: 754, training loss: 0.6263673901557922, training accuracy: 0.71875\n",
            "Epoch: 755, training loss: 0.6339362859725952, training accuracy: 0.6875\n",
            "Epoch: 756, training loss: 0.6414872407913208, training accuracy: 0.5625\n",
            "Epoch: 757, training loss: 0.6301250457763672, training accuracy: 0.6875\n",
            "Epoch: 758, training loss: 0.6076316833496094, training accuracy: 0.609375\n",
            "Epoch: 759, training loss: 0.6540364027023315, training accuracy: 0.59375\n",
            "Epoch: 760, training loss: 0.6560375690460205, training accuracy: 0.59375\n",
            "Epoch: 761, training loss: 0.5363847017288208, training accuracy: 0.734375\n",
            "Epoch: 762, training loss: 0.6053924560546875, training accuracy: 0.671875\n",
            "Epoch: 763, training loss: 0.6799939870834351, training accuracy: 0.515625\n",
            "Epoch: 764, training loss: 0.6010366678237915, training accuracy: 0.671875\n",
            "Epoch: 765, training loss: 0.6622835993766785, training accuracy: 0.671875\n",
            "Epoch: 766, training loss: 0.6587820649147034, training accuracy: 0.578125\n",
            "Epoch: 767, training loss: 0.6054972410202026, training accuracy: 0.71875\n",
            "Epoch: 768, training loss: 0.6405622959136963, training accuracy: 0.609375\n",
            "Epoch: 769, training loss: 0.637466549873352, training accuracy: 0.6875\n",
            "Epoch: 770, training loss: 0.6578596234321594, training accuracy: 0.59375\n",
            "Epoch: 771, training loss: 0.7090593576431274, training accuracy: 0.578125\n",
            "Epoch: 772, training loss: 0.6427075266838074, training accuracy: 0.609375\n",
            "Epoch: 773, training loss: 0.6609246134757996, training accuracy: 0.640625\n",
            "Epoch: 774, training loss: 0.628122866153717, training accuracy: 0.625\n",
            "Epoch: 775, training loss: 0.6614373922348022, training accuracy: 0.609375\n",
            "Epoch: 776, training loss: 0.7051477432250977, training accuracy: 0.546875\n",
            "Epoch: 777, training loss: 0.6657344102859497, training accuracy: 0.609375\n",
            "Epoch: 778, training loss: 0.5730112791061401, training accuracy: 0.703125\n",
            "Epoch: 779, training loss: 0.6189217567443848, training accuracy: 0.703125\n",
            "Epoch: 780, training loss: 0.6460697650909424, training accuracy: 0.671875\n",
            "Epoch: 781, training loss: 0.6381528377532959, training accuracy: 0.609375\n",
            "Epoch: 782, training loss: 0.6073960065841675, training accuracy: 0.6875\n",
            "Epoch: 783, training loss: 0.6170225143432617, training accuracy: 0.703125\n",
            "Epoch: 784, training loss: 0.6783854961395264, training accuracy: 0.609375\n",
            "Epoch: 785, training loss: 0.6254607439041138, training accuracy: 0.671875\n",
            "Epoch: 786, training loss: 0.649518609046936, training accuracy: 0.640625\n",
            "Epoch: 787, training loss: 0.6466389298439026, training accuracy: 0.625\n",
            "Epoch: 788, training loss: 0.6043732166290283, training accuracy: 0.625\n",
            "Epoch: 789, training loss: 0.5916790962219238, training accuracy: 0.703125\n",
            "Epoch: 790, training loss: 0.6397919654846191, training accuracy: 0.5625\n",
            "Epoch: 791, training loss: 0.5609105825424194, training accuracy: 0.75\n",
            "Epoch: 792, training loss: 0.6128227710723877, training accuracy: 0.609375\n",
            "Epoch: 793, training loss: 0.5653309226036072, training accuracy: 0.78125\n",
            "Epoch: 794, training loss: 0.581872820854187, training accuracy: 0.625\n",
            "Epoch: 795, training loss: 0.7657533884048462, training accuracy: 0.46875\n",
            "Epoch: 796, training loss: 0.6761194467544556, training accuracy: 0.546875\n",
            "Epoch: 797, training loss: 0.6311610341072083, training accuracy: 0.59375\n",
            "Epoch: 798, training loss: 0.6070090532302856, training accuracy: 0.703125\n",
            "Epoch: 799, training loss: 0.8006690740585327, training accuracy: 0.484375\n",
            "Epoch: 800, training loss: 0.5906243324279785, training accuracy: 0.640625\n",
            "Epoch: 801, training loss: 0.5436314344406128, training accuracy: 0.765625\n",
            "Epoch: 802, training loss: 0.6341784000396729, training accuracy: 0.734375\n",
            "Epoch: 803, training loss: 0.6168786287307739, training accuracy: 0.71875\n",
            "Epoch: 804, training loss: 0.6300344467163086, training accuracy: 0.609375\n",
            "Epoch: 805, training loss: 0.6432812213897705, training accuracy: 0.671875\n",
            "Epoch: 806, training loss: 0.62045818567276, training accuracy: 0.65625\n",
            "Epoch: 807, training loss: 0.7390599250793457, training accuracy: 0.5625\n",
            "Epoch: 808, training loss: 0.6271337866783142, training accuracy: 0.546875\n",
            "Epoch: 809, training loss: 0.606600284576416, training accuracy: 0.703125\n",
            "Epoch: 810, training loss: 0.6010669469833374, training accuracy: 0.703125\n",
            "Epoch: 811, training loss: 0.5953030586242676, training accuracy: 0.734375\n",
            "Epoch: 812, training loss: 0.6403717994689941, training accuracy: 0.59375\n",
            "Epoch: 813, training loss: 0.6346555948257446, training accuracy: 0.625\n",
            "Epoch: 814, training loss: 0.615734338760376, training accuracy: 0.625\n",
            "Epoch: 815, training loss: 0.593633770942688, training accuracy: 0.71875\n",
            "Epoch: 816, training loss: 0.6481234431266785, training accuracy: 0.640625\n",
            "Epoch: 817, training loss: 0.6232060194015503, training accuracy: 0.65625\n",
            "Epoch: 818, training loss: 0.6433930397033691, training accuracy: 0.609375\n",
            "Epoch: 819, training loss: 0.6501991748809814, training accuracy: 0.59375\n",
            "Epoch: 820, training loss: 0.5859382748603821, training accuracy: 0.734375\n",
            "Epoch: 821, training loss: 0.6131828427314758, training accuracy: 0.671875\n",
            "Epoch: 822, training loss: 0.6839810609817505, training accuracy: 0.609375\n",
            "Epoch: 823, training loss: 0.5873599052429199, training accuracy: 0.703125\n",
            "Epoch: 824, training loss: 0.5818173885345459, training accuracy: 0.6875\n",
            "Epoch: 825, training loss: 0.6547780632972717, training accuracy: 0.5625\n",
            "Epoch: 826, training loss: 0.5695112347602844, training accuracy: 0.734375\n",
            "Epoch: 827, training loss: 0.591005802154541, training accuracy: 0.703125\n",
            "Epoch: 828, training loss: 0.6557894349098206, training accuracy: 0.734375\n",
            "Epoch: 829, training loss: 0.6653305292129517, training accuracy: 0.65625\n",
            "Epoch: 830, training loss: 0.6485362648963928, training accuracy: 0.671875\n",
            "Epoch: 831, training loss: 0.5807547569274902, training accuracy: 0.6875\n",
            "Epoch: 832, training loss: 0.6718395948410034, training accuracy: 0.609375\n",
            "Epoch: 833, training loss: 0.5827798843383789, training accuracy: 0.734375\n",
            "Epoch: 834, training loss: 0.5936441421508789, training accuracy: 0.703125\n",
            "Epoch: 835, training loss: 0.5951883792877197, training accuracy: 0.6875\n",
            "Epoch: 836, training loss: 0.6289458870887756, training accuracy: 0.640625\n",
            "Epoch: 837, training loss: 0.6770763397216797, training accuracy: 0.625\n",
            "Epoch: 838, training loss: 0.5897878408432007, training accuracy: 0.65625\n",
            "Epoch: 839, training loss: 0.6349636316299438, training accuracy: 0.6875\n",
            "Epoch: 840, training loss: 0.5921341180801392, training accuracy: 0.640625\n",
            "Epoch: 841, training loss: 0.6105683445930481, training accuracy: 0.65625\n",
            "Epoch: 842, training loss: 0.6311405301094055, training accuracy: 0.671875\n",
            "Epoch: 843, training loss: 0.5720353126525879, training accuracy: 0.75\n",
            "Epoch: 844, training loss: 0.6449266672134399, training accuracy: 0.640625\n",
            "Epoch: 845, training loss: 0.5955907106399536, training accuracy: 0.640625\n",
            "Epoch: 846, training loss: 0.5748347640037537, training accuracy: 0.734375\n",
            "Epoch: 847, training loss: 0.5960688591003418, training accuracy: 0.671875\n",
            "Epoch: 848, training loss: 0.6392232179641724, training accuracy: 0.6875\n",
            "Epoch: 849, training loss: 0.632219672203064, training accuracy: 0.65625\n",
            "Epoch: 850, training loss: 0.5044415593147278, training accuracy: 0.796875\n",
            "Epoch: 851, training loss: 0.7541147470474243, training accuracy: 0.5625\n",
            "Epoch: 852, training loss: 0.6549211740493774, training accuracy: 0.5625\n",
            "Epoch: 853, training loss: 0.7651721239089966, training accuracy: 0.5\n",
            "Epoch: 854, training loss: 0.6866980195045471, training accuracy: 0.609375\n",
            "Epoch: 855, training loss: 0.6283243894577026, training accuracy: 0.5625\n",
            "Epoch: 856, training loss: 0.6472595930099487, training accuracy: 0.609375\n",
            "Epoch: 857, training loss: 0.609275221824646, training accuracy: 0.703125\n",
            "Epoch: 858, training loss: 0.6487833261489868, training accuracy: 0.53125\n",
            "Epoch: 859, training loss: 0.6602518558502197, training accuracy: 0.59375\n",
            "Epoch: 860, training loss: 0.5877262353897095, training accuracy: 0.703125\n",
            "Epoch: 861, training loss: 0.609289288520813, training accuracy: 0.703125\n",
            "Epoch: 862, training loss: 0.6561115980148315, training accuracy: 0.640625\n",
            "Epoch: 863, training loss: 0.6197484135627747, training accuracy: 0.71875\n",
            "Epoch: 864, training loss: 0.6283973455429077, training accuracy: 0.59375\n",
            "Epoch: 865, training loss: 0.6654031276702881, training accuracy: 0.640625\n",
            "Epoch: 866, training loss: 0.6117407083511353, training accuracy: 0.625\n",
            "Epoch: 867, training loss: 0.6487947702407837, training accuracy: 0.65625\n",
            "Epoch: 868, training loss: 0.6061605215072632, training accuracy: 0.6875\n",
            "Epoch: 869, training loss: 0.661154568195343, training accuracy: 0.578125\n",
            "Epoch: 870, training loss: 0.6100407838821411, training accuracy: 0.6875\n",
            "Epoch: 871, training loss: 0.608482837677002, training accuracy: 0.65625\n",
            "Epoch: 872, training loss: 0.6077921390533447, training accuracy: 0.65625\n",
            "Epoch: 873, training loss: 0.6618679761886597, training accuracy: 0.640625\n",
            "Epoch: 874, training loss: 0.7128884792327881, training accuracy: 0.5625\n",
            "Epoch: 875, training loss: 0.5751062631607056, training accuracy: 0.703125\n",
            "Epoch: 876, training loss: 0.6365033388137817, training accuracy: 0.609375\n",
            "Epoch: 877, training loss: 0.5798640251159668, training accuracy: 0.65625\n",
            "Epoch: 878, training loss: 0.6545332670211792, training accuracy: 0.5625\n",
            "Epoch: 879, training loss: 0.6199253797531128, training accuracy: 0.671875\n",
            "Epoch: 880, training loss: 0.6690541505813599, training accuracy: 0.59375\n",
            "Epoch: 881, training loss: 0.6930859088897705, training accuracy: 0.59375\n",
            "Epoch: 882, training loss: 0.6533446907997131, training accuracy: 0.65625\n",
            "Epoch: 883, training loss: 0.6262280941009521, training accuracy: 0.640625\n",
            "Epoch: 884, training loss: 0.607437014579773, training accuracy: 0.65625\n",
            "Epoch: 885, training loss: 0.5793564319610596, training accuracy: 0.734375\n",
            "Epoch: 886, training loss: 0.6415072679519653, training accuracy: 0.71875\n",
            "Epoch: 887, training loss: 0.6821151971817017, training accuracy: 0.609375\n",
            "Epoch: 888, training loss: 0.6743887662887573, training accuracy: 0.5625\n",
            "Epoch: 889, training loss: 0.6633120775222778, training accuracy: 0.625\n",
            "Epoch: 890, training loss: 0.5817515850067139, training accuracy: 0.671875\n",
            "Epoch: 891, training loss: 0.5719912648200989, training accuracy: 0.671875\n",
            "Epoch: 892, training loss: 0.6484713554382324, training accuracy: 0.53125\n",
            "Epoch: 893, training loss: 0.6925791501998901, training accuracy: 0.59375\n",
            "Epoch: 894, training loss: 0.6245124936103821, training accuracy: 0.59375\n",
            "Epoch: 895, training loss: 0.6717129945755005, training accuracy: 0.640625\n",
            "Epoch: 896, training loss: 0.6582472324371338, training accuracy: 0.640625\n",
            "Epoch: 897, training loss: 0.6277488470077515, training accuracy: 0.6875\n",
            "Epoch: 898, training loss: 0.7443842887878418, training accuracy: 0.578125\n",
            "Epoch: 899, training loss: 0.5777202844619751, training accuracy: 0.71875\n",
            "Epoch: 900, training loss: 0.6680442094802856, training accuracy: 0.625\n",
            "Epoch: 901, training loss: 0.5922924280166626, training accuracy: 0.65625\n",
            "Epoch: 902, training loss: 0.6223086714744568, training accuracy: 0.640625\n",
            "Epoch: 903, training loss: 0.6094201803207397, training accuracy: 0.6875\n",
            "Epoch: 904, training loss: 0.6512004137039185, training accuracy: 0.6875\n",
            "Epoch: 905, training loss: 0.5881199836730957, training accuracy: 0.6875\n",
            "Epoch: 906, training loss: 0.6058295369148254, training accuracy: 0.640625\n",
            "Epoch: 907, training loss: 0.5953234434127808, training accuracy: 0.671875\n",
            "Epoch: 908, training loss: 0.6921344995498657, training accuracy: 0.640625\n",
            "Epoch: 909, training loss: 0.6837852001190186, training accuracy: 0.546875\n",
            "Epoch: 910, training loss: 0.5896289348602295, training accuracy: 0.6875\n",
            "Epoch: 911, training loss: 0.5590231418609619, training accuracy: 0.65625\n",
            "Epoch: 912, training loss: 0.6134055852890015, training accuracy: 0.625\n",
            "Epoch: 913, training loss: 0.6292550563812256, training accuracy: 0.625\n",
            "Epoch: 914, training loss: 0.6428180932998657, training accuracy: 0.625\n",
            "Epoch: 915, training loss: 0.6507851481437683, training accuracy: 0.609375\n",
            "Epoch: 916, training loss: 0.5947455167770386, training accuracy: 0.65625\n",
            "Epoch: 917, training loss: 0.7189295291900635, training accuracy: 0.625\n",
            "Epoch: 918, training loss: 0.5442107915878296, training accuracy: 0.734375\n",
            "Epoch: 919, training loss: 0.6138670444488525, training accuracy: 0.640625\n",
            "Epoch: 920, training loss: 0.6799119114875793, training accuracy: 0.625\n",
            "Epoch: 921, training loss: 0.6073622107505798, training accuracy: 0.734375\n",
            "Epoch: 922, training loss: 0.6855776309967041, training accuracy: 0.578125\n",
            "Epoch: 923, training loss: 0.6550575494766235, training accuracy: 0.640625\n",
            "Epoch: 924, training loss: 0.7170418500900269, training accuracy: 0.5625\n",
            "Epoch: 925, training loss: 0.6153868436813354, training accuracy: 0.65625\n",
            "Epoch: 926, training loss: 0.6987833976745605, training accuracy: 0.5625\n",
            "Epoch: 927, training loss: 0.6651800870895386, training accuracy: 0.65625\n",
            "Epoch: 928, training loss: 0.613109827041626, training accuracy: 0.640625\n",
            "Epoch: 929, training loss: 0.6756765842437744, training accuracy: 0.59375\n",
            "Epoch: 930, training loss: 0.6138703227043152, training accuracy: 0.65625\n",
            "Epoch: 931, training loss: 0.5748899579048157, training accuracy: 0.6875\n",
            "Epoch: 932, training loss: 0.5852469801902771, training accuracy: 0.703125\n",
            "Epoch: 933, training loss: 0.6123912334442139, training accuracy: 0.671875\n",
            "Epoch: 934, training loss: 0.5800826549530029, training accuracy: 0.703125\n",
            "Epoch: 935, training loss: 0.6412253379821777, training accuracy: 0.6875\n",
            "Epoch: 936, training loss: 0.6355497241020203, training accuracy: 0.515625\n",
            "Epoch: 937, training loss: 0.6175771951675415, training accuracy: 0.640625\n",
            "Epoch: 938, training loss: 0.7029271125793457, training accuracy: 0.515625\n",
            "Epoch: 939, training loss: 0.6619844436645508, training accuracy: 0.59375\n",
            "Epoch: 940, training loss: 0.6416608095169067, training accuracy: 0.640625\n",
            "Epoch: 941, training loss: 0.6129087805747986, training accuracy: 0.703125\n",
            "Epoch: 942, training loss: 0.6301566362380981, training accuracy: 0.671875\n",
            "Epoch: 943, training loss: 0.6565790772438049, training accuracy: 0.625\n",
            "Epoch: 944, training loss: 0.6025935411453247, training accuracy: 0.671875\n",
            "Epoch: 945, training loss: 0.6747027635574341, training accuracy: 0.578125\n",
            "Epoch: 946, training loss: 0.5806643962860107, training accuracy: 0.703125\n",
            "Epoch: 947, training loss: 0.6600052714347839, training accuracy: 0.6875\n",
            "Epoch: 948, training loss: 0.6153929233551025, training accuracy: 0.640625\n",
            "Epoch: 949, training loss: 0.6218186616897583, training accuracy: 0.640625\n",
            "Epoch: 950, training loss: 0.61099773645401, training accuracy: 0.6875\n",
            "Epoch: 951, training loss: 0.5814775824546814, training accuracy: 0.78125\n",
            "Epoch: 952, training loss: 0.637030839920044, training accuracy: 0.671875\n",
            "Epoch: 953, training loss: 0.6295929551124573, training accuracy: 0.640625\n",
            "Epoch: 954, training loss: 0.6254528760910034, training accuracy: 0.5625\n",
            "Epoch: 955, training loss: 0.6631898880004883, training accuracy: 0.546875\n",
            "Epoch: 956, training loss: 0.6280648112297058, training accuracy: 0.640625\n",
            "Epoch: 957, training loss: 0.6746542453765869, training accuracy: 0.5625\n",
            "Epoch: 958, training loss: 0.626603364944458, training accuracy: 0.671875\n",
            "Epoch: 959, training loss: 0.6486234068870544, training accuracy: 0.671875\n",
            "Epoch: 960, training loss: 0.6384908556938171, training accuracy: 0.671875\n",
            "Epoch: 961, training loss: 0.6525977253913879, training accuracy: 0.6875\n",
            "Epoch: 962, training loss: 0.6077598929405212, training accuracy: 0.671875\n",
            "Epoch: 963, training loss: 0.580197811126709, training accuracy: 0.765625\n",
            "Epoch: 964, training loss: 0.6057378053665161, training accuracy: 0.625\n",
            "Epoch: 965, training loss: 0.6323480010032654, training accuracy: 0.625\n",
            "Epoch: 966, training loss: 0.6547930836677551, training accuracy: 0.671875\n",
            "Epoch: 967, training loss: 0.7074857354164124, training accuracy: 0.484375\n",
            "Epoch: 968, training loss: 0.6151222586631775, training accuracy: 0.65625\n",
            "Epoch: 969, training loss: 0.7357844114303589, training accuracy: 0.59375\n",
            "Epoch: 970, training loss: 0.576502799987793, training accuracy: 0.734375\n",
            "Epoch: 971, training loss: 0.6421236991882324, training accuracy: 0.625\n",
            "Epoch: 972, training loss: 0.6544414758682251, training accuracy: 0.609375\n",
            "Epoch: 973, training loss: 0.5427800416946411, training accuracy: 0.71875\n",
            "Epoch: 974, training loss: 0.5929896831512451, training accuracy: 0.671875\n",
            "Epoch: 975, training loss: 0.6325199604034424, training accuracy: 0.625\n",
            "Epoch: 976, training loss: 0.6491695642471313, training accuracy: 0.609375\n",
            "Epoch: 977, training loss: 0.6782916188240051, training accuracy: 0.578125\n",
            "Epoch: 978, training loss: 0.589290976524353, training accuracy: 0.671875\n",
            "Epoch: 979, training loss: 0.6562119126319885, training accuracy: 0.671875\n",
            "Epoch: 980, training loss: 0.6072632670402527, training accuracy: 0.65625\n",
            "Epoch: 981, training loss: 0.6464394330978394, training accuracy: 0.59375\n",
            "Epoch: 982, training loss: 0.5993567705154419, training accuracy: 0.640625\n",
            "Epoch: 983, training loss: 0.7217316627502441, training accuracy: 0.5625\n",
            "Epoch: 984, training loss: 0.6541217565536499, training accuracy: 0.625\n",
            "Epoch: 985, training loss: 0.5838942527770996, training accuracy: 0.71875\n",
            "Epoch: 986, training loss: 0.6321501135826111, training accuracy: 0.671875\n",
            "Epoch: 987, training loss: 0.6158875226974487, training accuracy: 0.65625\n",
            "Epoch: 988, training loss: 0.6215710043907166, training accuracy: 0.6875\n",
            "Epoch: 989, training loss: 0.6129974126815796, training accuracy: 0.75\n",
            "Epoch: 990, training loss: 0.6088224649429321, training accuracy: 0.671875\n",
            "Epoch: 991, training loss: 0.7226451635360718, training accuracy: 0.546875\n",
            "Epoch: 992, training loss: 0.558377206325531, training accuracy: 0.65625\n",
            "Epoch: 993, training loss: 0.687307596206665, training accuracy: 0.625\n",
            "Epoch: 994, training loss: 0.5957085490226746, training accuracy: 0.625\n",
            "Epoch: 995, training loss: 0.6613626480102539, training accuracy: 0.59375\n",
            "Epoch: 996, training loss: 0.6658352017402649, training accuracy: 0.5625\n",
            "Epoch: 997, training loss: 0.6333732604980469, training accuracy: 0.71875\n",
            "Epoch: 998, training loss: 0.6390995383262634, training accuracy: 0.59375\n",
            "Epoch: 999, training loss: 0.5632526278495789, training accuracy: 0.765625\n",
            "Epoch: 1000, training loss: 0.6020665168762207, training accuracy: 0.734375\n",
            "\u001b[31m Validation loss: 0.6316234469413757, validation accuracy: 0.6175908221797323 \u001b[0m\n",
            "Epoch: 1001, training loss: 0.6220121383666992, training accuracy: 0.65625\n",
            "Epoch: 1002, training loss: 0.6257482767105103, training accuracy: 0.609375\n",
            "Epoch: 1003, training loss: 0.6497683525085449, training accuracy: 0.671875\n",
            "Epoch: 1004, training loss: 0.578445315361023, training accuracy: 0.703125\n",
            "Epoch: 1005, training loss: 0.5974911451339722, training accuracy: 0.796875\n",
            "Epoch: 1006, training loss: 0.584561288356781, training accuracy: 0.640625\n",
            "Epoch: 1007, training loss: 0.59995436668396, training accuracy: 0.734375\n",
            "Epoch: 1008, training loss: 0.588055431842804, training accuracy: 0.671875\n",
            "Epoch: 1009, training loss: 0.6944184303283691, training accuracy: 0.59375\n",
            "Epoch: 1010, training loss: 0.5885039567947388, training accuracy: 0.59375\n",
            "Epoch: 1011, training loss: 0.6387456655502319, training accuracy: 0.640625\n",
            "Epoch: 1012, training loss: 0.6024001836776733, training accuracy: 0.765625\n",
            "Epoch: 1013, training loss: 0.7417035698890686, training accuracy: 0.609375\n",
            "Epoch: 1014, training loss: 0.6407824754714966, training accuracy: 0.65625\n",
            "Epoch: 1015, training loss: 0.683021605014801, training accuracy: 0.59375\n",
            "Epoch: 1016, training loss: 0.6969380378723145, training accuracy: 0.546875\n",
            "Epoch: 1017, training loss: 0.6438432335853577, training accuracy: 0.578125\n",
            "Epoch: 1018, training loss: 0.7055044174194336, training accuracy: 0.59375\n",
            "Epoch: 1019, training loss: 0.6713728308677673, training accuracy: 0.65625\n",
            "Epoch: 1020, training loss: 0.6238653659820557, training accuracy: 0.640625\n",
            "Epoch: 1021, training loss: 0.6241838932037354, training accuracy: 0.671875\n",
            "Epoch: 1022, training loss: 0.5996829271316528, training accuracy: 0.75\n",
            "Epoch: 1023, training loss: 0.6617692708969116, training accuracy: 0.625\n",
            "Epoch: 1024, training loss: 0.5602214336395264, training accuracy: 0.71875\n",
            "Epoch: 1025, training loss: 0.6603965759277344, training accuracy: 0.5625\n",
            "Epoch: 1026, training loss: 0.5876827239990234, training accuracy: 0.6875\n",
            "Epoch: 1027, training loss: 0.6538949012756348, training accuracy: 0.578125\n",
            "Epoch: 1028, training loss: 0.6131504774093628, training accuracy: 0.625\n",
            "Epoch: 1029, training loss: 0.6253229379653931, training accuracy: 0.640625\n",
            "Epoch: 1030, training loss: 0.7072776556015015, training accuracy: 0.5\n",
            "Epoch: 1031, training loss: 0.6803154349327087, training accuracy: 0.59375\n",
            "Epoch: 1032, training loss: 0.678908109664917, training accuracy: 0.5\n",
            "Epoch: 1033, training loss: 0.6541118621826172, training accuracy: 0.59375\n",
            "Epoch: 1034, training loss: 0.6373659372329712, training accuracy: 0.609375\n",
            "Epoch: 1035, training loss: 0.6034262180328369, training accuracy: 0.703125\n",
            "Epoch: 1036, training loss: 0.5858306884765625, training accuracy: 0.71875\n",
            "Epoch: 1037, training loss: 0.7041727304458618, training accuracy: 0.609375\n",
            "Epoch: 1038, training loss: 0.6138721704483032, training accuracy: 0.6875\n",
            "Epoch: 1039, training loss: 0.6343410015106201, training accuracy: 0.703125\n",
            "Epoch: 1040, training loss: 0.6009660959243774, training accuracy: 0.65625\n",
            "Epoch: 1041, training loss: 0.6909381151199341, training accuracy: 0.515625\n",
            "Epoch: 1042, training loss: 0.5424163341522217, training accuracy: 0.796875\n",
            "Epoch: 1043, training loss: 0.7040207386016846, training accuracy: 0.65625\n",
            "Epoch: 1044, training loss: 0.6248843669891357, training accuracy: 0.71875\n",
            "Epoch: 1045, training loss: 0.6719743013381958, training accuracy: 0.640625\n",
            "Epoch: 1046, training loss: 0.6933209896087646, training accuracy: 0.5\n",
            "Epoch: 1047, training loss: 0.5834081172943115, training accuracy: 0.703125\n",
            "Epoch: 1048, training loss: 0.5879459381103516, training accuracy: 0.71875\n",
            "Epoch: 1049, training loss: 0.6202220320701599, training accuracy: 0.640625\n",
            "Epoch: 1050, training loss: 0.6256725788116455, training accuracy: 0.625\n",
            "Epoch: 1051, training loss: 0.6142783761024475, training accuracy: 0.71875\n",
            "Epoch: 1052, training loss: 0.583753228187561, training accuracy: 0.65625\n",
            "Epoch: 1053, training loss: 0.6990385055541992, training accuracy: 0.578125\n",
            "Epoch: 1054, training loss: 0.6466474533081055, training accuracy: 0.671875\n",
            "Epoch: 1055, training loss: 0.6620173454284668, training accuracy: 0.59375\n",
            "Epoch: 1056, training loss: 0.69558185338974, training accuracy: 0.5625\n",
            "Epoch: 1057, training loss: 0.5990540981292725, training accuracy: 0.65625\n",
            "Epoch: 1058, training loss: 0.6399180293083191, training accuracy: 0.65625\n",
            "Epoch: 1059, training loss: 0.6924543380737305, training accuracy: 0.5625\n",
            "Epoch: 1060, training loss: 0.6134589910507202, training accuracy: 0.65625\n",
            "Epoch: 1061, training loss: 0.6720018982887268, training accuracy: 0.609375\n",
            "Epoch: 1062, training loss: 0.5878419876098633, training accuracy: 0.671875\n",
            "Epoch: 1063, training loss: 0.6452149748802185, training accuracy: 0.6875\n",
            "Epoch: 1064, training loss: 0.6529150009155273, training accuracy: 0.640625\n",
            "Epoch: 1065, training loss: 0.6289008855819702, training accuracy: 0.671875\n",
            "Epoch: 1066, training loss: 0.6457388401031494, training accuracy: 0.734375\n",
            "Epoch: 1067, training loss: 0.6186405420303345, training accuracy: 0.75\n",
            "Epoch: 1068, training loss: 0.6042658090591431, training accuracy: 0.6875\n",
            "Epoch: 1069, training loss: 0.6186079978942871, training accuracy: 0.6875\n",
            "Epoch: 1070, training loss: 0.6109498739242554, training accuracy: 0.671875\n",
            "Epoch: 1071, training loss: 0.6669398546218872, training accuracy: 0.59375\n",
            "Epoch: 1072, training loss: 0.640809178352356, training accuracy: 0.5625\n",
            "Epoch: 1073, training loss: 0.6180949211120605, training accuracy: 0.65625\n",
            "Epoch: 1074, training loss: 0.5783017873764038, training accuracy: 0.65625\n",
            "Epoch: 1075, training loss: 0.6164304614067078, training accuracy: 0.625\n",
            "Epoch: 1076, training loss: 0.5785720348358154, training accuracy: 0.8125\n",
            "Epoch: 1077, training loss: 0.6195112466812134, training accuracy: 0.625\n",
            "Epoch: 1078, training loss: 0.6189419031143188, training accuracy: 0.65625\n",
            "Epoch: 1079, training loss: 0.5542961359024048, training accuracy: 0.671875\n",
            "Epoch: 1080, training loss: 0.5614563822746277, training accuracy: 0.734375\n",
            "Epoch: 1081, training loss: 0.6050236225128174, training accuracy: 0.765625\n",
            "Epoch: 1082, training loss: 0.6721720695495605, training accuracy: 0.609375\n",
            "Epoch: 1083, training loss: 0.6834733486175537, training accuracy: 0.578125\n",
            "Epoch: 1084, training loss: 0.6520016193389893, training accuracy: 0.640625\n",
            "Epoch: 1085, training loss: 0.660049557685852, training accuracy: 0.6875\n",
            "Epoch: 1086, training loss: 0.5826689004898071, training accuracy: 0.734375\n",
            "Epoch: 1087, training loss: 0.6739234924316406, training accuracy: 0.578125\n",
            "Epoch: 1088, training loss: 0.6008651852607727, training accuracy: 0.65625\n",
            "Epoch: 1089, training loss: 0.5613021850585938, training accuracy: 0.671875\n",
            "Epoch: 1090, training loss: 0.5695275068283081, training accuracy: 0.703125\n",
            "Epoch: 1091, training loss: 0.6937251687049866, training accuracy: 0.59375\n",
            "Epoch: 1092, training loss: 0.6242226958274841, training accuracy: 0.640625\n",
            "Epoch: 1093, training loss: 0.7116274237632751, training accuracy: 0.53125\n",
            "Epoch: 1094, training loss: 0.5263924598693848, training accuracy: 0.78125\n",
            "Epoch: 1095, training loss: 0.6256768107414246, training accuracy: 0.640625\n",
            "Epoch: 1096, training loss: 0.5448817610740662, training accuracy: 0.671875\n",
            "Epoch: 1097, training loss: 0.5751535296440125, training accuracy: 0.65625\n",
            "Epoch: 1098, training loss: 0.5797505378723145, training accuracy: 0.6875\n",
            "Epoch: 1099, training loss: 0.6152936220169067, training accuracy: 0.734375\n",
            "Epoch: 1100, training loss: 0.6228570938110352, training accuracy: 0.65625\n",
            "Epoch: 1101, training loss: 0.6318677663803101, training accuracy: 0.671875\n",
            "Epoch: 1102, training loss: 0.6054021120071411, training accuracy: 0.703125\n",
            "Epoch: 1103, training loss: 0.649909257888794, training accuracy: 0.671875\n",
            "Epoch: 1104, training loss: 0.670386552810669, training accuracy: 0.546875\n",
            "Epoch: 1105, training loss: 0.6878718137741089, training accuracy: 0.5625\n",
            "Epoch: 1106, training loss: 0.6339071989059448, training accuracy: 0.65625\n",
            "Epoch: 1107, training loss: 0.577509880065918, training accuracy: 0.65625\n",
            "Epoch: 1108, training loss: 0.5870829820632935, training accuracy: 0.671875\n",
            "Epoch: 1109, training loss: 0.6374130249023438, training accuracy: 0.65625\n",
            "Epoch: 1110, training loss: 0.6138311624526978, training accuracy: 0.625\n",
            "Epoch: 1111, training loss: 0.6251908540725708, training accuracy: 0.625\n",
            "Epoch: 1112, training loss: 0.637816309928894, training accuracy: 0.609375\n",
            "Epoch: 1113, training loss: 0.6666603088378906, training accuracy: 0.609375\n",
            "Epoch: 1114, training loss: 0.6115059852600098, training accuracy: 0.625\n",
            "Epoch: 1115, training loss: 0.6316130757331848, training accuracy: 0.671875\n",
            "Epoch: 1116, training loss: 0.6336221098899841, training accuracy: 0.640625\n",
            "Epoch: 1117, training loss: 0.6463958024978638, training accuracy: 0.640625\n",
            "Epoch: 1118, training loss: 0.7133991718292236, training accuracy: 0.578125\n",
            "Epoch: 1119, training loss: 0.6427273750305176, training accuracy: 0.65625\n",
            "Epoch: 1120, training loss: 0.612013041973114, training accuracy: 0.6875\n",
            "Epoch: 1121, training loss: 0.6601059436798096, training accuracy: 0.609375\n",
            "Epoch: 1122, training loss: 0.5931282639503479, training accuracy: 0.71875\n",
            "Epoch: 1123, training loss: 0.67781662940979, training accuracy: 0.5\n",
            "Epoch: 1124, training loss: 0.6115685701370239, training accuracy: 0.6875\n",
            "Epoch: 1125, training loss: 0.6461769938468933, training accuracy: 0.59375\n",
            "Epoch: 1126, training loss: 0.6188330054283142, training accuracy: 0.734375\n",
            "Epoch: 1127, training loss: 0.5868560075759888, training accuracy: 0.671875\n",
            "Epoch: 1128, training loss: 0.6313923001289368, training accuracy: 0.640625\n",
            "Epoch: 1129, training loss: 0.586740255355835, training accuracy: 0.703125\n",
            "Epoch: 1130, training loss: 0.6101741194725037, training accuracy: 0.671875\n",
            "Epoch: 1131, training loss: 0.6007767915725708, training accuracy: 0.65625\n",
            "Epoch: 1132, training loss: 0.6466053128242493, training accuracy: 0.625\n",
            "Epoch: 1133, training loss: 0.5811842679977417, training accuracy: 0.6875\n",
            "Epoch: 1134, training loss: 0.6655647158622742, training accuracy: 0.609375\n",
            "Epoch: 1135, training loss: 0.5790389776229858, training accuracy: 0.703125\n",
            "Epoch: 1136, training loss: 0.6360828876495361, training accuracy: 0.671875\n",
            "Epoch: 1137, training loss: 0.6027940511703491, training accuracy: 0.671875\n",
            "Epoch: 1138, training loss: 0.6618629693984985, training accuracy: 0.609375\n",
            "Epoch: 1139, training loss: 0.6556829810142517, training accuracy: 0.703125\n",
            "Epoch: 1140, training loss: 0.607280433177948, training accuracy: 0.671875\n",
            "Epoch: 1141, training loss: 0.6322737336158752, training accuracy: 0.625\n",
            "Epoch: 1142, training loss: 0.6010728478431702, training accuracy: 0.671875\n",
            "Epoch: 1143, training loss: 0.7301386594772339, training accuracy: 0.5625\n",
            "Epoch: 1144, training loss: 0.6278281211853027, training accuracy: 0.65625\n",
            "Epoch: 1145, training loss: 0.7184153199195862, training accuracy: 0.546875\n",
            "Epoch: 1146, training loss: 0.7002014517784119, training accuracy: 0.578125\n",
            "Epoch: 1147, training loss: 0.5908101201057434, training accuracy: 0.75\n",
            "Epoch: 1148, training loss: 0.6182754039764404, training accuracy: 0.71875\n",
            "Epoch: 1149, training loss: 0.6885995268821716, training accuracy: 0.5625\n",
            "Epoch: 1150, training loss: 0.6703529357910156, training accuracy: 0.671875\n",
            "Epoch: 1151, training loss: 0.6224275827407837, training accuracy: 0.640625\n",
            "Epoch: 1152, training loss: 0.5979586839675903, training accuracy: 0.6875\n",
            "Epoch: 1153, training loss: 0.6143679618835449, training accuracy: 0.703125\n",
            "Epoch: 1154, training loss: 0.5707694292068481, training accuracy: 0.765625\n",
            "Epoch: 1155, training loss: 0.6396228075027466, training accuracy: 0.671875\n",
            "Epoch: 1156, training loss: 0.6170528531074524, training accuracy: 0.6875\n",
            "Epoch: 1157, training loss: 0.6373916864395142, training accuracy: 0.65625\n",
            "Epoch: 1158, training loss: 0.6262826919555664, training accuracy: 0.625\n",
            "Epoch: 1159, training loss: 0.6569114327430725, training accuracy: 0.671875\n",
            "Epoch: 1160, training loss: 0.5891833305358887, training accuracy: 0.6875\n",
            "Epoch: 1161, training loss: 0.6035144329071045, training accuracy: 0.75\n",
            "Epoch: 1162, training loss: 0.612025260925293, training accuracy: 0.609375\n",
            "Epoch: 1163, training loss: 0.6143139004707336, training accuracy: 0.671875\n",
            "Epoch: 1164, training loss: 0.641386866569519, training accuracy: 0.65625\n",
            "Epoch: 1165, training loss: 0.651680588722229, training accuracy: 0.609375\n",
            "Epoch: 1166, training loss: 0.5327999591827393, training accuracy: 0.765625\n",
            "Epoch: 1167, training loss: 0.6846643686294556, training accuracy: 0.59375\n",
            "Epoch: 1168, training loss: 0.5539910793304443, training accuracy: 0.703125\n",
            "Epoch: 1169, training loss: 0.6749411821365356, training accuracy: 0.578125\n",
            "Epoch: 1170, training loss: 0.5557384490966797, training accuracy: 0.640625\n",
            "Epoch: 1171, training loss: 0.5811324715614319, training accuracy: 0.65625\n",
            "Epoch: 1172, training loss: 0.6292248368263245, training accuracy: 0.625\n",
            "Epoch: 1173, training loss: 0.5482230186462402, training accuracy: 0.765625\n",
            "Epoch: 1174, training loss: 0.6356415152549744, training accuracy: 0.640625\n",
            "Epoch: 1175, training loss: 0.5848503708839417, training accuracy: 0.703125\n",
            "Epoch: 1176, training loss: 0.6185829639434814, training accuracy: 0.671875\n",
            "Epoch: 1177, training loss: 0.6461197137832642, training accuracy: 0.578125\n",
            "Epoch: 1178, training loss: 0.6028622388839722, training accuracy: 0.6875\n",
            "Epoch: 1179, training loss: 0.7037768363952637, training accuracy: 0.5625\n",
            "Epoch: 1180, training loss: 0.6470211744308472, training accuracy: 0.609375\n",
            "Epoch: 1181, training loss: 0.5968722701072693, training accuracy: 0.671875\n",
            "Epoch: 1182, training loss: 0.6206430196762085, training accuracy: 0.671875\n",
            "Epoch: 1183, training loss: 0.6299924254417419, training accuracy: 0.59375\n",
            "Epoch: 1184, training loss: 0.7044273614883423, training accuracy: 0.53125\n",
            "Epoch: 1185, training loss: 0.5871486663818359, training accuracy: 0.6875\n",
            "Epoch: 1186, training loss: 0.6972381472587585, training accuracy: 0.578125\n",
            "Epoch: 1187, training loss: 0.584290623664856, training accuracy: 0.609375\n",
            "Epoch: 1188, training loss: 0.6912565231323242, training accuracy: 0.609375\n",
            "Epoch: 1189, training loss: 0.6548337340354919, training accuracy: 0.53125\n",
            "Epoch: 1190, training loss: 0.7177801132202148, training accuracy: 0.5625\n",
            "Epoch: 1191, training loss: 0.6073378324508667, training accuracy: 0.703125\n",
            "Epoch: 1192, training loss: 0.5611028671264648, training accuracy: 0.6875\n",
            "Epoch: 1193, training loss: 0.6538277268409729, training accuracy: 0.578125\n",
            "Epoch: 1194, training loss: 0.5973206758499146, training accuracy: 0.671875\n",
            "Epoch: 1195, training loss: 0.6521804928779602, training accuracy: 0.578125\n",
            "Epoch: 1196, training loss: 0.6852563619613647, training accuracy: 0.5625\n",
            "Epoch: 1197, training loss: 0.61473149061203, training accuracy: 0.640625\n",
            "Epoch: 1198, training loss: 0.6350976228713989, training accuracy: 0.75\n",
            "Epoch: 1199, training loss: 0.6249850392341614, training accuracy: 0.65625\n",
            "Epoch: 1200, training loss: 0.6174155473709106, training accuracy: 0.640625\n",
            "Epoch: 1201, training loss: 0.6774234771728516, training accuracy: 0.578125\n",
            "Epoch: 1202, training loss: 0.6656780242919922, training accuracy: 0.609375\n",
            "Epoch: 1203, training loss: 0.6943804025650024, training accuracy: 0.609375\n",
            "Epoch: 1204, training loss: 0.6518909931182861, training accuracy: 0.6875\n",
            "Epoch: 1205, training loss: 0.6191297173500061, training accuracy: 0.625\n",
            "Epoch: 1206, training loss: 0.6826501488685608, training accuracy: 0.578125\n",
            "Epoch: 1207, training loss: 0.6437696218490601, training accuracy: 0.609375\n",
            "Epoch: 1208, training loss: 0.6472972631454468, training accuracy: 0.625\n",
            "Epoch: 1209, training loss: 0.6159173250198364, training accuracy: 0.6875\n",
            "Epoch: 1210, training loss: 0.6913943290710449, training accuracy: 0.578125\n",
            "Epoch: 1211, training loss: 0.6381741166114807, training accuracy: 0.75\n",
            "Epoch: 1212, training loss: 0.6986188888549805, training accuracy: 0.515625\n",
            "Epoch: 1213, training loss: 0.6275031566619873, training accuracy: 0.6875\n",
            "Epoch: 1214, training loss: 0.7177166938781738, training accuracy: 0.515625\n",
            "Epoch: 1215, training loss: 0.6822395324707031, training accuracy: 0.59375\n",
            "Epoch: 1216, training loss: 0.7046171426773071, training accuracy: 0.5625\n",
            "Epoch: 1217, training loss: 0.5759739875793457, training accuracy: 0.734375\n",
            "Epoch: 1218, training loss: 0.6182673573493958, training accuracy: 0.640625\n",
            "Epoch: 1219, training loss: 0.6125283241271973, training accuracy: 0.6875\n",
            "Epoch: 1220, training loss: 0.7265206575393677, training accuracy: 0.578125\n",
            "Epoch: 1221, training loss: 0.6245208382606506, training accuracy: 0.640625\n",
            "Epoch: 1222, training loss: 0.618005633354187, training accuracy: 0.6875\n",
            "Epoch: 1223, training loss: 0.6320092678070068, training accuracy: 0.703125\n",
            "Epoch: 1224, training loss: 0.6408348679542542, training accuracy: 0.6875\n",
            "Epoch: 1225, training loss: 0.6269862651824951, training accuracy: 0.65625\n",
            "Epoch: 1226, training loss: 0.6612536907196045, training accuracy: 0.640625\n",
            "Epoch: 1227, training loss: 0.6392725706100464, training accuracy: 0.703125\n",
            "Epoch: 1228, training loss: 0.5939568281173706, training accuracy: 0.671875\n",
            "Epoch: 1229, training loss: 0.6246411800384521, training accuracy: 0.625\n",
            "Epoch: 1230, training loss: 0.6204721927642822, training accuracy: 0.625\n",
            "Epoch: 1231, training loss: 0.68531334400177, training accuracy: 0.625\n",
            "Epoch: 1232, training loss: 0.6455819606781006, training accuracy: 0.625\n",
            "Epoch: 1233, training loss: 0.6959899663925171, training accuracy: 0.578125\n",
            "Epoch: 1234, training loss: 0.5515528917312622, training accuracy: 0.6875\n",
            "Epoch: 1235, training loss: 0.635534405708313, training accuracy: 0.640625\n",
            "Epoch: 1236, training loss: 0.5721331834793091, training accuracy: 0.828125\n",
            "Epoch: 1237, training loss: 0.6003375053405762, training accuracy: 0.703125\n",
            "Epoch: 1238, training loss: 0.6464624404907227, training accuracy: 0.671875\n",
            "Epoch: 1239, training loss: 0.6319209933280945, training accuracy: 0.640625\n",
            "Epoch: 1240, training loss: 0.5618299841880798, training accuracy: 0.6875\n",
            "Epoch: 1241, training loss: 0.6391241550445557, training accuracy: 0.5625\n",
            "Epoch: 1242, training loss: 0.5563294887542725, training accuracy: 0.75\n",
            "Epoch: 1243, training loss: 0.571199893951416, training accuracy: 0.65625\n",
            "Epoch: 1244, training loss: 0.6431446671485901, training accuracy: 0.703125\n",
            "Epoch: 1245, training loss: 0.6035523414611816, training accuracy: 0.734375\n",
            "Epoch: 1246, training loss: 0.5406960248947144, training accuracy: 0.71875\n",
            "Epoch: 1247, training loss: 0.6551252603530884, training accuracy: 0.640625\n",
            "Epoch: 1248, training loss: 0.6144900321960449, training accuracy: 0.6875\n",
            "Epoch: 1249, training loss: 0.621066153049469, training accuracy: 0.640625\n",
            "Epoch: 1250, training loss: 0.6909893751144409, training accuracy: 0.609375\n",
            "Epoch: 1251, training loss: 0.7739531993865967, training accuracy: 0.5625\n",
            "Epoch: 1252, training loss: 0.5938977599143982, training accuracy: 0.640625\n",
            "Epoch: 1253, training loss: 0.6081684231758118, training accuracy: 0.671875\n",
            "Epoch: 1254, training loss: 0.6969309449195862, training accuracy: 0.546875\n",
            "Epoch: 1255, training loss: 0.6491121053695679, training accuracy: 0.6875\n",
            "Epoch: 1256, training loss: 0.6495192646980286, training accuracy: 0.640625\n",
            "Epoch: 1257, training loss: 0.6889584064483643, training accuracy: 0.5\n",
            "Epoch: 1258, training loss: 0.6122007369995117, training accuracy: 0.671875\n",
            "Epoch: 1259, training loss: 0.6961689591407776, training accuracy: 0.546875\n",
            "Epoch: 1260, training loss: 0.5746436715126038, training accuracy: 0.75\n",
            "Epoch: 1261, training loss: 0.6453803181648254, training accuracy: 0.671875\n",
            "Epoch: 1262, training loss: 0.6190909147262573, training accuracy: 0.625\n",
            "Epoch: 1263, training loss: 0.642292857170105, training accuracy: 0.65625\n",
            "Epoch: 1264, training loss: 0.6209399700164795, training accuracy: 0.609375\n",
            "Epoch: 1265, training loss: 0.5781408548355103, training accuracy: 0.671875\n",
            "Epoch: 1266, training loss: 0.597882866859436, training accuracy: 0.65625\n",
            "Epoch: 1267, training loss: 0.5869521498680115, training accuracy: 0.6875\n",
            "Epoch: 1268, training loss: 0.6604292988777161, training accuracy: 0.609375\n",
            "Epoch: 1269, training loss: 0.6299629211425781, training accuracy: 0.625\n",
            "Epoch: 1270, training loss: 0.6250961422920227, training accuracy: 0.625\n",
            "Epoch: 1271, training loss: 0.6439695358276367, training accuracy: 0.671875\n",
            "Epoch: 1272, training loss: 0.5767879486083984, training accuracy: 0.6875\n",
            "Epoch: 1273, training loss: 0.62150639295578, training accuracy: 0.71875\n",
            "Epoch: 1274, training loss: 0.6223915815353394, training accuracy: 0.609375\n",
            "Epoch: 1275, training loss: 0.665233850479126, training accuracy: 0.625\n",
            "Epoch: 1276, training loss: 0.6607022285461426, training accuracy: 0.609375\n",
            "Epoch: 1277, training loss: 0.592393159866333, training accuracy: 0.671875\n",
            "Epoch: 1278, training loss: 0.6420187950134277, training accuracy: 0.609375\n",
            "Epoch: 1279, training loss: 0.6074680089950562, training accuracy: 0.703125\n",
            "Epoch: 1280, training loss: 0.6325520277023315, training accuracy: 0.671875\n",
            "Epoch: 1281, training loss: 0.5980345010757446, training accuracy: 0.671875\n",
            "Epoch: 1282, training loss: 0.5902309417724609, training accuracy: 0.6875\n",
            "Epoch: 1283, training loss: 0.7395296096801758, training accuracy: 0.578125\n",
            "Epoch: 1284, training loss: 0.6933864951133728, training accuracy: 0.578125\n",
            "Epoch: 1285, training loss: 0.6542705297470093, training accuracy: 0.640625\n",
            "Epoch: 1286, training loss: 0.6245532631874084, training accuracy: 0.640625\n",
            "Epoch: 1287, training loss: 0.5725600123405457, training accuracy: 0.75\n",
            "Epoch: 1288, training loss: 0.6703777313232422, training accuracy: 0.578125\n",
            "Epoch: 1289, training loss: 0.6049362421035767, training accuracy: 0.65625\n",
            "Epoch: 1290, training loss: 0.649329662322998, training accuracy: 0.625\n",
            "Epoch: 1291, training loss: 0.5966237187385559, training accuracy: 0.671875\n",
            "Epoch: 1292, training loss: 0.6789087057113647, training accuracy: 0.546875\n",
            "Epoch: 1293, training loss: 0.6025071144104004, training accuracy: 0.71875\n",
            "Epoch: 1294, training loss: 0.594149649143219, training accuracy: 0.671875\n",
            "Epoch: 1295, training loss: 0.6582738161087036, training accuracy: 0.671875\n",
            "Epoch: 1296, training loss: 0.6747866868972778, training accuracy: 0.625\n",
            "Epoch: 1297, training loss: 0.7120489478111267, training accuracy: 0.59375\n",
            "Epoch: 1298, training loss: 0.6142464876174927, training accuracy: 0.625\n",
            "Epoch: 1299, training loss: 0.5802206993103027, training accuracy: 0.65625\n",
            "Epoch: 1300, training loss: 0.6043218374252319, training accuracy: 0.640625\n",
            "Epoch: 1301, training loss: 0.6833183765411377, training accuracy: 0.59375\n",
            "Epoch: 1302, training loss: 0.5777025818824768, training accuracy: 0.671875\n",
            "Epoch: 1303, training loss: 0.6050242781639099, training accuracy: 0.671875\n",
            "Epoch: 1304, training loss: 0.5872442722320557, training accuracy: 0.71875\n",
            "Epoch: 1305, training loss: 0.587882936000824, training accuracy: 0.6875\n",
            "Epoch: 1306, training loss: 0.700668454170227, training accuracy: 0.5625\n",
            "Epoch: 1307, training loss: 0.6092780232429504, training accuracy: 0.609375\n",
            "Epoch: 1308, training loss: 0.7223434448242188, training accuracy: 0.515625\n",
            "Epoch: 1309, training loss: 0.712095320224762, training accuracy: 0.546875\n",
            "Epoch: 1310, training loss: 0.665960431098938, training accuracy: 0.59375\n",
            "Epoch: 1311, training loss: 0.6206661462783813, training accuracy: 0.65625\n",
            "Epoch: 1312, training loss: 0.6638781428337097, training accuracy: 0.65625\n",
            "Epoch: 1313, training loss: 0.6129598617553711, training accuracy: 0.71875\n",
            "Epoch: 1314, training loss: 0.6309669017791748, training accuracy: 0.65625\n",
            "Epoch: 1315, training loss: 0.5455056428909302, training accuracy: 0.796875\n",
            "Epoch: 1316, training loss: 0.6403924226760864, training accuracy: 0.578125\n",
            "Epoch: 1317, training loss: 0.647984504699707, training accuracy: 0.625\n",
            "Epoch: 1318, training loss: 0.6150164008140564, training accuracy: 0.65625\n",
            "Epoch: 1319, training loss: 0.6596705913543701, training accuracy: 0.640625\n",
            "Epoch: 1320, training loss: 0.6642047166824341, training accuracy: 0.640625\n",
            "Epoch: 1321, training loss: 0.6442687511444092, training accuracy: 0.65625\n",
            "Epoch: 1322, training loss: 0.6489846706390381, training accuracy: 0.609375\n",
            "Epoch: 1323, training loss: 0.6081377267837524, training accuracy: 0.640625\n",
            "Epoch: 1324, training loss: 0.610175609588623, training accuracy: 0.71875\n",
            "Epoch: 1325, training loss: 0.5856056213378906, training accuracy: 0.625\n",
            "Epoch: 1326, training loss: 0.6204841136932373, training accuracy: 0.625\n",
            "Epoch: 1327, training loss: 0.6500548124313354, training accuracy: 0.578125\n",
            "Epoch: 1328, training loss: 0.6437520980834961, training accuracy: 0.59375\n",
            "Epoch: 1329, training loss: 0.6657183170318604, training accuracy: 0.640625\n",
            "Epoch: 1330, training loss: 0.6561961770057678, training accuracy: 0.578125\n",
            "Epoch: 1331, training loss: 0.6520557999610901, training accuracy: 0.640625\n",
            "Epoch: 1332, training loss: 0.5549343824386597, training accuracy: 0.65625\n",
            "Epoch: 1333, training loss: 0.6518274545669556, training accuracy: 0.546875\n",
            "Epoch: 1334, training loss: 0.6716898679733276, training accuracy: 0.625\n",
            "Epoch: 1335, training loss: 0.6050012111663818, training accuracy: 0.65625\n",
            "Epoch: 1336, training loss: 0.5647023916244507, training accuracy: 0.78125\n",
            "Epoch: 1337, training loss: 0.723479151725769, training accuracy: 0.53125\n",
            "Epoch: 1338, training loss: 0.5950982570648193, training accuracy: 0.65625\n",
            "Epoch: 1339, training loss: 0.718421220779419, training accuracy: 0.53125\n",
            "Epoch: 1340, training loss: 0.6685229539871216, training accuracy: 0.5625\n",
            "Epoch: 1341, training loss: 0.6454664468765259, training accuracy: 0.640625\n",
            "Epoch: 1342, training loss: 0.7014619708061218, training accuracy: 0.578125\n",
            "Epoch: 1343, training loss: 0.6067643761634827, training accuracy: 0.640625\n",
            "Epoch: 1344, training loss: 0.6121151447296143, training accuracy: 0.671875\n",
            "Epoch: 1345, training loss: 0.6835508942604065, training accuracy: 0.578125\n",
            "Epoch: 1346, training loss: 0.6787022352218628, training accuracy: 0.640625\n",
            "Epoch: 1347, training loss: 0.6466348767280579, training accuracy: 0.59375\n",
            "Epoch: 1348, training loss: 0.6108874082565308, training accuracy: 0.640625\n",
            "Epoch: 1349, training loss: 0.6337787508964539, training accuracy: 0.6875\n",
            "Epoch: 1350, training loss: 0.6072555780410767, training accuracy: 0.65625\n",
            "Epoch: 1351, training loss: 0.5906498432159424, training accuracy: 0.75\n",
            "Epoch: 1352, training loss: 0.6001458168029785, training accuracy: 0.703125\n",
            "Epoch: 1353, training loss: 0.6652894020080566, training accuracy: 0.640625\n",
            "Epoch: 1354, training loss: 0.6300374269485474, training accuracy: 0.65625\n",
            "Epoch: 1355, training loss: 0.602038562297821, training accuracy: 0.640625\n",
            "Epoch: 1356, training loss: 0.5653418302536011, training accuracy: 0.796875\n",
            "Epoch: 1357, training loss: 0.6446424722671509, training accuracy: 0.6875\n",
            "Epoch: 1358, training loss: 0.6429229974746704, training accuracy: 0.671875\n",
            "Epoch: 1359, training loss: 0.6475081443786621, training accuracy: 0.703125\n",
            "Epoch: 1360, training loss: 0.6410411596298218, training accuracy: 0.609375\n",
            "Epoch: 1361, training loss: 0.6033496856689453, training accuracy: 0.6875\n",
            "Epoch: 1362, training loss: 0.6241149306297302, training accuracy: 0.6875\n",
            "Epoch: 1363, training loss: 0.6064847111701965, training accuracy: 0.671875\n",
            "Epoch: 1364, training loss: 0.5962527394294739, training accuracy: 0.65625\n",
            "Epoch: 1365, training loss: 0.6780446171760559, training accuracy: 0.5625\n",
            "Epoch: 1366, training loss: 0.5845180749893188, training accuracy: 0.6875\n",
            "Epoch: 1367, training loss: 0.5908029675483704, training accuracy: 0.71875\n",
            "Epoch: 1368, training loss: 0.6499829888343811, training accuracy: 0.640625\n",
            "Epoch: 1369, training loss: 0.5595127940177917, training accuracy: 0.6875\n",
            "Epoch: 1370, training loss: 0.6652724742889404, training accuracy: 0.671875\n",
            "Epoch: 1371, training loss: 0.6454936265945435, training accuracy: 0.671875\n",
            "Epoch: 1372, training loss: 0.5714570879936218, training accuracy: 0.6875\n",
            "Epoch: 1373, training loss: 0.543053925037384, training accuracy: 0.734375\n",
            "Epoch: 1374, training loss: 0.6258919835090637, training accuracy: 0.640625\n",
            "Epoch: 1375, training loss: 0.5767674446105957, training accuracy: 0.6875\n",
            "Epoch: 1376, training loss: 0.6029438376426697, training accuracy: 0.734375\n",
            "Epoch: 1377, training loss: 0.7343891859054565, training accuracy: 0.546875\n",
            "Epoch: 1378, training loss: 0.6757918000221252, training accuracy: 0.59375\n",
            "Epoch: 1379, training loss: 0.6530351042747498, training accuracy: 0.609375\n",
            "Epoch: 1380, training loss: 0.60578453540802, training accuracy: 0.609375\n",
            "Epoch: 1381, training loss: 0.6021240949630737, training accuracy: 0.671875\n",
            "Epoch: 1382, training loss: 0.6162921190261841, training accuracy: 0.609375\n",
            "Epoch: 1383, training loss: 0.6753718852996826, training accuracy: 0.5625\n",
            "Epoch: 1384, training loss: 0.6933650970458984, training accuracy: 0.546875\n",
            "Epoch: 1385, training loss: 0.6568408608436584, training accuracy: 0.65625\n",
            "Epoch: 1386, training loss: 0.6323692798614502, training accuracy: 0.671875\n",
            "Epoch: 1387, training loss: 0.6017800569534302, training accuracy: 0.6875\n",
            "Epoch: 1388, training loss: 0.5864726305007935, training accuracy: 0.640625\n",
            "Epoch: 1389, training loss: 0.6410528421401978, training accuracy: 0.5625\n",
            "Epoch: 1390, training loss: 0.6399319171905518, training accuracy: 0.625\n",
            "Epoch: 1391, training loss: 0.6306033134460449, training accuracy: 0.734375\n",
            "Epoch: 1392, training loss: 0.5888794660568237, training accuracy: 0.703125\n",
            "Epoch: 1393, training loss: 0.5946297645568848, training accuracy: 0.734375\n",
            "Epoch: 1394, training loss: 0.6605836153030396, training accuracy: 0.53125\n",
            "Epoch: 1395, training loss: 0.6135175228118896, training accuracy: 0.65625\n",
            "Epoch: 1396, training loss: 0.6119944453239441, training accuracy: 0.625\n",
            "Epoch: 1397, training loss: 0.5859662294387817, training accuracy: 0.703125\n",
            "Epoch: 1398, training loss: 0.6712818741798401, training accuracy: 0.640625\n",
            "Epoch: 1399, training loss: 0.7045652866363525, training accuracy: 0.625\n",
            "Epoch: 1400, training loss: 0.5794576406478882, training accuracy: 0.703125\n",
            "Epoch: 1401, training loss: 0.5800485610961914, training accuracy: 0.703125\n",
            "Epoch: 1402, training loss: 0.6410719156265259, training accuracy: 0.65625\n",
            "Epoch: 1403, training loss: 0.6188462972640991, training accuracy: 0.640625\n",
            "Epoch: 1404, training loss: 0.6594977378845215, training accuracy: 0.625\n",
            "Epoch: 1405, training loss: 0.6118331551551819, training accuracy: 0.6875\n",
            "Epoch: 1406, training loss: 0.5957881808280945, training accuracy: 0.734375\n",
            "Epoch: 1407, training loss: 0.6470337510108948, training accuracy: 0.640625\n",
            "Epoch: 1408, training loss: 0.6213022470474243, training accuracy: 0.6875\n",
            "Epoch: 1409, training loss: 0.6337027549743652, training accuracy: 0.640625\n",
            "Epoch: 1410, training loss: 0.6829025149345398, training accuracy: 0.640625\n",
            "Epoch: 1411, training loss: 0.6073076725006104, training accuracy: 0.65625\n",
            "Epoch: 1412, training loss: 0.6687155365943909, training accuracy: 0.5625\n",
            "Epoch: 1413, training loss: 0.6164550185203552, training accuracy: 0.75\n",
            "Epoch: 1414, training loss: 0.7028732299804688, training accuracy: 0.640625\n",
            "Epoch: 1415, training loss: 0.5864076018333435, training accuracy: 0.734375\n",
            "Epoch: 1416, training loss: 0.580333948135376, training accuracy: 0.6875\n",
            "Epoch: 1417, training loss: 0.5947466492652893, training accuracy: 0.703125\n",
            "Epoch: 1418, training loss: 0.6242846250534058, training accuracy: 0.53125\n",
            "Epoch: 1419, training loss: 0.6634547710418701, training accuracy: 0.546875\n",
            "Epoch: 1420, training loss: 0.599990725517273, training accuracy: 0.71875\n",
            "Epoch: 1421, training loss: 0.6374810934066772, training accuracy: 0.65625\n",
            "Epoch: 1422, training loss: 0.5996958613395691, training accuracy: 0.65625\n",
            "Epoch: 1423, training loss: 0.622683048248291, training accuracy: 0.65625\n",
            "Epoch: 1424, training loss: 0.6632453203201294, training accuracy: 0.65625\n",
            "Epoch: 1425, training loss: 0.6804875135421753, training accuracy: 0.59375\n",
            "Epoch: 1426, training loss: 0.6787258386611938, training accuracy: 0.59375\n",
            "Epoch: 1427, training loss: 0.6051280498504639, training accuracy: 0.671875\n",
            "Epoch: 1428, training loss: 0.6833208799362183, training accuracy: 0.578125\n",
            "Epoch: 1429, training loss: 0.7111848592758179, training accuracy: 0.5625\n",
            "Epoch: 1430, training loss: 0.6008262038230896, training accuracy: 0.65625\n",
            "Epoch: 1431, training loss: 0.5875331163406372, training accuracy: 0.609375\n",
            "Epoch: 1432, training loss: 0.6832891702651978, training accuracy: 0.59375\n",
            "Epoch: 1433, training loss: 0.6636214256286621, training accuracy: 0.65625\n",
            "Epoch: 1434, training loss: 0.6397380828857422, training accuracy: 0.59375\n",
            "Epoch: 1435, training loss: 0.6057162880897522, training accuracy: 0.625\n",
            "Epoch: 1436, training loss: 0.690773606300354, training accuracy: 0.609375\n",
            "Epoch: 1437, training loss: 0.6832535266876221, training accuracy: 0.546875\n",
            "Epoch: 1438, training loss: 0.6765364408493042, training accuracy: 0.5625\n",
            "Epoch: 1439, training loss: 0.5971454381942749, training accuracy: 0.765625\n",
            "Epoch: 1440, training loss: 0.6186440587043762, training accuracy: 0.734375\n",
            "Epoch: 1441, training loss: 0.6647731065750122, training accuracy: 0.578125\n",
            "Epoch: 1442, training loss: 0.6248996257781982, training accuracy: 0.65625\n",
            "Epoch: 1443, training loss: 0.6717060804367065, training accuracy: 0.578125\n",
            "Epoch: 1444, training loss: 0.6324993371963501, training accuracy: 0.671875\n",
            "Epoch: 1445, training loss: 0.6388169527053833, training accuracy: 0.53125\n",
            "Epoch: 1446, training loss: 0.5661776065826416, training accuracy: 0.75\n",
            "Epoch: 1447, training loss: 0.6132593154907227, training accuracy: 0.703125\n",
            "Epoch: 1448, training loss: 0.577335000038147, training accuracy: 0.734375\n",
            "Epoch: 1449, training loss: 0.6241495609283447, training accuracy: 0.65625\n",
            "Epoch: 1450, training loss: 0.6456305384635925, training accuracy: 0.6875\n",
            "Epoch: 1451, training loss: 0.6004420518875122, training accuracy: 0.671875\n",
            "Epoch: 1452, training loss: 0.6221314668655396, training accuracy: 0.625\n",
            "Epoch: 1453, training loss: 0.5975873470306396, training accuracy: 0.65625\n",
            "Epoch: 1454, training loss: 0.5923849940299988, training accuracy: 0.6875\n",
            "Epoch: 1455, training loss: 0.6817545890808105, training accuracy: 0.59375\n",
            "Epoch: 1456, training loss: 0.6889631748199463, training accuracy: 0.578125\n",
            "Epoch: 1457, training loss: 0.5850117206573486, training accuracy: 0.65625\n",
            "Epoch: 1458, training loss: 0.7805609703063965, training accuracy: 0.484375\n",
            "Epoch: 1459, training loss: 0.5818749666213989, training accuracy: 0.640625\n",
            "Epoch: 1460, training loss: 0.6223461031913757, training accuracy: 0.671875\n",
            "Epoch: 1461, training loss: 0.6182682514190674, training accuracy: 0.640625\n",
            "Epoch: 1462, training loss: 0.6404390335083008, training accuracy: 0.6875\n",
            "Epoch: 1463, training loss: 0.6354693174362183, training accuracy: 0.625\n",
            "Epoch: 1464, training loss: 0.6534609198570251, training accuracy: 0.609375\n",
            "Epoch: 1465, training loss: 0.5898639559745789, training accuracy: 0.75\n",
            "Epoch: 1466, training loss: 0.6653174161911011, training accuracy: 0.65625\n",
            "Epoch: 1467, training loss: 0.6386554837226868, training accuracy: 0.65625\n",
            "Epoch: 1468, training loss: 0.6569349765777588, training accuracy: 0.578125\n",
            "Epoch: 1469, training loss: 0.6328774690628052, training accuracy: 0.671875\n",
            "Epoch: 1470, training loss: 0.5646384954452515, training accuracy: 0.703125\n",
            "Epoch: 1471, training loss: 0.6478911638259888, training accuracy: 0.6875\n",
            "Epoch: 1472, training loss: 0.6540616750717163, training accuracy: 0.71875\n",
            "Epoch: 1473, training loss: 0.6047472953796387, training accuracy: 0.609375\n",
            "Epoch: 1474, training loss: 0.6469897031784058, training accuracy: 0.671875\n",
            "Epoch: 1475, training loss: 0.6501508355140686, training accuracy: 0.6875\n",
            "Epoch: 1476, training loss: 0.6941618919372559, training accuracy: 0.53125\n",
            "Epoch: 1477, training loss: 0.5897824168205261, training accuracy: 0.65625\n",
            "Epoch: 1478, training loss: 0.6574165225028992, training accuracy: 0.625\n",
            "Epoch: 1479, training loss: 0.6285824775695801, training accuracy: 0.609375\n",
            "Epoch: 1480, training loss: 0.5786111950874329, training accuracy: 0.6875\n",
            "Epoch: 1481, training loss: 0.6033892631530762, training accuracy: 0.734375\n",
            "Epoch: 1482, training loss: 0.5944506525993347, training accuracy: 0.6875\n",
            "Epoch: 1483, training loss: 0.6847903728485107, training accuracy: 0.609375\n",
            "Epoch: 1484, training loss: 0.5620954036712646, training accuracy: 0.75\n",
            "Epoch: 1485, training loss: 0.6289048194885254, training accuracy: 0.671875\n",
            "Epoch: 1486, training loss: 0.638450026512146, training accuracy: 0.625\n",
            "Epoch: 1487, training loss: 0.6044275760650635, training accuracy: 0.625\n",
            "Epoch: 1488, training loss: 0.6716957092285156, training accuracy: 0.59375\n",
            "Epoch: 1489, training loss: 0.6787556409835815, training accuracy: 0.640625\n",
            "Epoch: 1490, training loss: 0.6391505599021912, training accuracy: 0.703125\n",
            "Epoch: 1491, training loss: 0.6446084976196289, training accuracy: 0.59375\n",
            "Epoch: 1492, training loss: 0.631524384021759, training accuracy: 0.671875\n",
            "Epoch: 1493, training loss: 0.6247680187225342, training accuracy: 0.625\n",
            "Epoch: 1494, training loss: 0.6759413480758667, training accuracy: 0.53125\n",
            "Epoch: 1495, training loss: 0.6774954795837402, training accuracy: 0.671875\n",
            "Epoch: 1496, training loss: 0.6248128414154053, training accuracy: 0.765625\n",
            "Epoch: 1497, training loss: 0.5724376440048218, training accuracy: 0.65625\n",
            "Epoch: 1498, training loss: 0.5933440923690796, training accuracy: 0.703125\n",
            "Epoch: 1499, training loss: 0.6786258816719055, training accuracy: 0.640625\n",
            "Epoch: 1500, training loss: 0.6469376683235168, training accuracy: 0.625\n",
            "\u001b[31m Validation loss: 0.6490233540534973, validation accuracy: 0.6080305927342257 \u001b[0m\n",
            "Epoch: 1501, training loss: 0.7432219386100769, training accuracy: 0.546875\n",
            "Epoch: 1502, training loss: 0.5766686201095581, training accuracy: 0.671875\n",
            "Epoch: 1503, training loss: 0.647697389125824, training accuracy: 0.65625\n",
            "Epoch: 1504, training loss: 0.6638122797012329, training accuracy: 0.578125\n",
            "Epoch: 1505, training loss: 0.6968637108802795, training accuracy: 0.546875\n",
            "Epoch: 1506, training loss: 0.5877013206481934, training accuracy: 0.703125\n",
            "Epoch: 1507, training loss: 0.6514263153076172, training accuracy: 0.59375\n",
            "Epoch: 1508, training loss: 0.6440014839172363, training accuracy: 0.640625\n",
            "Epoch: 1509, training loss: 0.6780436038970947, training accuracy: 0.515625\n",
            "Epoch: 1510, training loss: 0.6214326620101929, training accuracy: 0.65625\n",
            "Epoch: 1511, training loss: 0.6436588764190674, training accuracy: 0.65625\n",
            "Epoch: 1512, training loss: 0.6078963279724121, training accuracy: 0.75\n",
            "Epoch: 1513, training loss: 0.5698744058609009, training accuracy: 0.703125\n",
            "Epoch: 1514, training loss: 0.6055158972740173, training accuracy: 0.671875\n",
            "Epoch: 1515, training loss: 0.6050287485122681, training accuracy: 0.65625\n",
            "Epoch: 1516, training loss: 0.5776222348213196, training accuracy: 0.734375\n",
            "Epoch: 1517, training loss: 0.6248942613601685, training accuracy: 0.65625\n",
            "Epoch: 1518, training loss: 0.6306009292602539, training accuracy: 0.65625\n",
            "Epoch: 1519, training loss: 0.6684141159057617, training accuracy: 0.609375\n",
            "Epoch: 1520, training loss: 0.5977211594581604, training accuracy: 0.75\n",
            "Epoch: 1521, training loss: 0.6962628364562988, training accuracy: 0.546875\n",
            "Epoch: 1522, training loss: 0.5906279683113098, training accuracy: 0.71875\n",
            "Epoch: 1523, training loss: 0.6219364404678345, training accuracy: 0.625\n",
            "Epoch: 1524, training loss: 0.5990098714828491, training accuracy: 0.625\n",
            "Epoch: 1525, training loss: 0.668608546257019, training accuracy: 0.671875\n",
            "Epoch: 1526, training loss: 0.7049951553344727, training accuracy: 0.578125\n",
            "Epoch: 1527, training loss: 0.5703227519989014, training accuracy: 0.6875\n",
            "Epoch: 1528, training loss: 0.628224790096283, training accuracy: 0.671875\n",
            "Epoch: 1529, training loss: 0.5956047773361206, training accuracy: 0.6875\n",
            "Epoch: 1530, training loss: 0.6217741370201111, training accuracy: 0.625\n",
            "Epoch: 1531, training loss: 0.6359713077545166, training accuracy: 0.65625\n",
            "Epoch: 1532, training loss: 0.629370391368866, training accuracy: 0.765625\n",
            "Epoch: 1533, training loss: 0.6178581714630127, training accuracy: 0.671875\n",
            "Epoch: 1534, training loss: 0.6306402683258057, training accuracy: 0.6875\n",
            "Epoch: 1535, training loss: 0.6256188154220581, training accuracy: 0.640625\n",
            "Epoch: 1536, training loss: 0.5678437948226929, training accuracy: 0.765625\n",
            "Epoch: 1537, training loss: 0.6101536750793457, training accuracy: 0.6875\n",
            "Epoch: 1538, training loss: 0.6029338836669922, training accuracy: 0.671875\n",
            "Epoch: 1539, training loss: 0.6319412589073181, training accuracy: 0.5625\n",
            "Epoch: 1540, training loss: 0.6795642375946045, training accuracy: 0.609375\n",
            "Epoch: 1541, training loss: 0.6645073294639587, training accuracy: 0.625\n",
            "Epoch: 1542, training loss: 0.5933490991592407, training accuracy: 0.671875\n",
            "Epoch: 1543, training loss: 0.6354435086250305, training accuracy: 0.640625\n",
            "Epoch: 1544, training loss: 0.5927197337150574, training accuracy: 0.65625\n",
            "Epoch: 1545, training loss: 0.6186502575874329, training accuracy: 0.625\n",
            "Epoch: 1546, training loss: 0.5529624223709106, training accuracy: 0.609375\n",
            "Epoch: 1547, training loss: 0.6754559874534607, training accuracy: 0.625\n",
            "Epoch: 1548, training loss: 0.646037220954895, training accuracy: 0.625\n",
            "Epoch: 1549, training loss: 0.6003904342651367, training accuracy: 0.6875\n",
            "Epoch: 1550, training loss: 0.7039949297904968, training accuracy: 0.640625\n",
            "Epoch: 1551, training loss: 0.5261234045028687, training accuracy: 0.78125\n",
            "Epoch: 1552, training loss: 0.5785800814628601, training accuracy: 0.65625\n",
            "Epoch: 1553, training loss: 0.6593260765075684, training accuracy: 0.625\n",
            "Epoch: 1554, training loss: 0.5616661310195923, training accuracy: 0.703125\n",
            "Epoch: 1555, training loss: 0.6351969242095947, training accuracy: 0.671875\n",
            "Epoch: 1556, training loss: 0.6378934383392334, training accuracy: 0.671875\n",
            "Epoch: 1557, training loss: 0.5478496551513672, training accuracy: 0.75\n",
            "Epoch: 1558, training loss: 0.5418318510055542, training accuracy: 0.703125\n",
            "Epoch: 1559, training loss: 0.6514835357666016, training accuracy: 0.625\n",
            "Epoch: 1560, training loss: 0.6384670734405518, training accuracy: 0.65625\n",
            "Epoch: 1561, training loss: 0.6836605072021484, training accuracy: 0.640625\n",
            "Epoch: 1562, training loss: 0.6076959371566772, training accuracy: 0.71875\n",
            "Epoch: 1563, training loss: 0.5977445840835571, training accuracy: 0.6875\n",
            "Epoch: 1564, training loss: 0.603965699672699, training accuracy: 0.640625\n",
            "Epoch: 1565, training loss: 0.6273179650306702, training accuracy: 0.609375\n",
            "Epoch: 1566, training loss: 0.5791569948196411, training accuracy: 0.59375\n",
            "Epoch: 1567, training loss: 0.6720854640007019, training accuracy: 0.578125\n",
            "Epoch: 1568, training loss: 0.6386787295341492, training accuracy: 0.609375\n",
            "Epoch: 1569, training loss: 0.7452182173728943, training accuracy: 0.484375\n",
            "Epoch: 1570, training loss: 0.5545158386230469, training accuracy: 0.765625\n",
            "Epoch: 1571, training loss: 0.5651053786277771, training accuracy: 0.75\n",
            "Epoch: 1572, training loss: 0.6510390639305115, training accuracy: 0.65625\n",
            "Epoch: 1573, training loss: 0.701388955116272, training accuracy: 0.59375\n",
            "Epoch: 1574, training loss: 0.6133534908294678, training accuracy: 0.703125\n",
            "Epoch: 1575, training loss: 0.543255090713501, training accuracy: 0.734375\n",
            "Epoch: 1576, training loss: 0.6070553660392761, training accuracy: 0.71875\n",
            "Epoch: 1577, training loss: 0.6749458909034729, training accuracy: 0.65625\n",
            "Epoch: 1578, training loss: 0.649658203125, training accuracy: 0.703125\n",
            "Epoch: 1579, training loss: 0.6571645736694336, training accuracy: 0.578125\n",
            "Epoch: 1580, training loss: 0.6667943596839905, training accuracy: 0.6875\n",
            "Epoch: 1581, training loss: 0.7474825382232666, training accuracy: 0.484375\n",
            "Epoch: 1582, training loss: 0.6284222602844238, training accuracy: 0.625\n",
            "Epoch: 1583, training loss: 0.6632993221282959, training accuracy: 0.609375\n",
            "Epoch: 1584, training loss: 0.620523989200592, training accuracy: 0.65625\n",
            "Epoch: 1585, training loss: 0.6410139799118042, training accuracy: 0.5625\n",
            "Epoch: 1586, training loss: 0.5812747478485107, training accuracy: 0.75\n",
            "Epoch: 1587, training loss: 0.7791670560836792, training accuracy: 0.46875\n",
            "Epoch: 1588, training loss: 0.6498687267303467, training accuracy: 0.65625\n",
            "Epoch: 1589, training loss: 0.6621766686439514, training accuracy: 0.546875\n",
            "Epoch: 1590, training loss: 0.622297465801239, training accuracy: 0.640625\n",
            "Epoch: 1591, training loss: 0.5717089176177979, training accuracy: 0.765625\n",
            "Epoch: 1592, training loss: 0.6556529998779297, training accuracy: 0.671875\n",
            "Epoch: 1593, training loss: 0.6147249937057495, training accuracy: 0.609375\n",
            "Epoch: 1594, training loss: 0.6401952505111694, training accuracy: 0.59375\n",
            "Epoch: 1595, training loss: 0.6246675252914429, training accuracy: 0.671875\n",
            "Epoch: 1596, training loss: 0.6537476778030396, training accuracy: 0.5625\n",
            "Epoch: 1597, training loss: 0.6793786287307739, training accuracy: 0.609375\n",
            "Epoch: 1598, training loss: 0.6842300891876221, training accuracy: 0.59375\n",
            "Epoch: 1599, training loss: 0.6718548536300659, training accuracy: 0.546875\n",
            "Epoch: 1600, training loss: 0.6298347115516663, training accuracy: 0.625\n",
            "Epoch: 1601, training loss: 0.6253105401992798, training accuracy: 0.640625\n",
            "Epoch: 1602, training loss: 0.5863754749298096, training accuracy: 0.671875\n",
            "Epoch: 1603, training loss: 0.6573452949523926, training accuracy: 0.640625\n",
            "Epoch: 1604, training loss: 0.602932870388031, training accuracy: 0.640625\n",
            "Epoch: 1605, training loss: 0.5957285165786743, training accuracy: 0.671875\n",
            "Epoch: 1606, training loss: 0.6475083827972412, training accuracy: 0.59375\n",
            "Epoch: 1607, training loss: 0.6225762963294983, training accuracy: 0.625\n",
            "Epoch: 1608, training loss: 0.6174389123916626, training accuracy: 0.6875\n",
            "Epoch: 1609, training loss: 0.6394805312156677, training accuracy: 0.65625\n",
            "Epoch: 1610, training loss: 0.6744571328163147, training accuracy: 0.59375\n",
            "Epoch: 1611, training loss: 0.6966034173965454, training accuracy: 0.578125\n",
            "Epoch: 1612, training loss: 0.6075093746185303, training accuracy: 0.65625\n",
            "Epoch: 1613, training loss: 0.6204949021339417, training accuracy: 0.6875\n",
            "Epoch: 1614, training loss: 0.5696868300437927, training accuracy: 0.765625\n",
            "Epoch: 1615, training loss: 0.6080325841903687, training accuracy: 0.671875\n",
            "Epoch: 1616, training loss: 0.6432459950447083, training accuracy: 0.640625\n",
            "Epoch: 1617, training loss: 0.6771974563598633, training accuracy: 0.578125\n",
            "Epoch: 1618, training loss: 0.601594090461731, training accuracy: 0.625\n",
            "Epoch: 1619, training loss: 0.6279710531234741, training accuracy: 0.640625\n",
            "Epoch: 1620, training loss: 0.6272417902946472, training accuracy: 0.53125\n",
            "Epoch: 1621, training loss: 0.6481531858444214, training accuracy: 0.671875\n",
            "Epoch: 1622, training loss: 0.671148419380188, training accuracy: 0.5\n",
            "Epoch: 1623, training loss: 0.6017088890075684, training accuracy: 0.6875\n",
            "Epoch: 1624, training loss: 0.6674892902374268, training accuracy: 0.5625\n",
            "Epoch: 1625, training loss: 0.6628376245498657, training accuracy: 0.609375\n",
            "Epoch: 1626, training loss: 0.5996440052986145, training accuracy: 0.6875\n",
            "Epoch: 1627, training loss: 0.6509336233139038, training accuracy: 0.609375\n",
            "Epoch: 1628, training loss: 0.596919059753418, training accuracy: 0.71875\n",
            "Epoch: 1629, training loss: 0.6346422433853149, training accuracy: 0.671875\n",
            "Epoch: 1630, training loss: 0.6186994910240173, training accuracy: 0.71875\n",
            "Epoch: 1631, training loss: 0.624802827835083, training accuracy: 0.6875\n",
            "Epoch: 1632, training loss: 0.6779167652130127, training accuracy: 0.640625\n",
            "Epoch: 1633, training loss: 0.6350204944610596, training accuracy: 0.6875\n",
            "Epoch: 1634, training loss: 0.6188040971755981, training accuracy: 0.65625\n",
            "Epoch: 1635, training loss: 0.6561716794967651, training accuracy: 0.578125\n",
            "Epoch: 1636, training loss: 0.6669280529022217, training accuracy: 0.5625\n",
            "Epoch: 1637, training loss: 0.7119717001914978, training accuracy: 0.53125\n",
            "Epoch: 1638, training loss: 0.6073572635650635, training accuracy: 0.625\n",
            "Epoch: 1639, training loss: 0.6808897256851196, training accuracy: 0.65625\n",
            "Epoch: 1640, training loss: 0.6238133907318115, training accuracy: 0.625\n",
            "Epoch: 1641, training loss: 0.610531747341156, training accuracy: 0.765625\n",
            "Epoch: 1642, training loss: 0.551530122756958, training accuracy: 0.703125\n",
            "Epoch: 1643, training loss: 0.6101806163787842, training accuracy: 0.6875\n",
            "Epoch: 1644, training loss: 0.6470147371292114, training accuracy: 0.578125\n",
            "Epoch: 1645, training loss: 0.5850403308868408, training accuracy: 0.6875\n",
            "Epoch: 1646, training loss: 0.6554969549179077, training accuracy: 0.546875\n",
            "Epoch: 1647, training loss: 0.624764084815979, training accuracy: 0.640625\n",
            "Epoch: 1648, training loss: 0.7109301090240479, training accuracy: 0.53125\n",
            "Epoch: 1649, training loss: 0.6206076741218567, training accuracy: 0.640625\n",
            "Epoch: 1650, training loss: 0.6729870438575745, training accuracy: 0.640625\n",
            "Epoch: 1651, training loss: 0.6080071330070496, training accuracy: 0.671875\n",
            "Epoch: 1652, training loss: 0.5699281692504883, training accuracy: 0.71875\n",
            "Epoch: 1653, training loss: 0.6876339316368103, training accuracy: 0.546875\n",
            "Epoch: 1654, training loss: 0.636591911315918, training accuracy: 0.65625\n",
            "Epoch: 1655, training loss: 0.6546624898910522, training accuracy: 0.671875\n",
            "Epoch: 1656, training loss: 0.6306227445602417, training accuracy: 0.6875\n",
            "Epoch: 1657, training loss: 0.6559598445892334, training accuracy: 0.609375\n",
            "Epoch: 1658, training loss: 0.6179983615875244, training accuracy: 0.65625\n",
            "Epoch: 1659, training loss: 0.719512939453125, training accuracy: 0.578125\n",
            "Epoch: 1660, training loss: 0.6522951126098633, training accuracy: 0.59375\n",
            "Epoch: 1661, training loss: 0.6700793504714966, training accuracy: 0.546875\n",
            "Epoch: 1662, training loss: 0.608254611492157, training accuracy: 0.6875\n",
            "Epoch: 1663, training loss: 0.5480519533157349, training accuracy: 0.78125\n",
            "Epoch: 1664, training loss: 0.5727810859680176, training accuracy: 0.75\n",
            "Epoch: 1665, training loss: 0.6776265501976013, training accuracy: 0.59375\n",
            "Epoch: 1666, training loss: 0.6108148097991943, training accuracy: 0.640625\n",
            "Epoch: 1667, training loss: 0.6084572076797485, training accuracy: 0.71875\n",
            "Epoch: 1668, training loss: 0.7329163551330566, training accuracy: 0.453125\n",
            "Epoch: 1669, training loss: 0.6667815446853638, training accuracy: 0.609375\n",
            "Epoch: 1670, training loss: 0.6378275752067566, training accuracy: 0.640625\n",
            "Epoch: 1671, training loss: 0.6738308668136597, training accuracy: 0.6875\n",
            "Epoch: 1672, training loss: 0.6157633066177368, training accuracy: 0.671875\n",
            "Epoch: 1673, training loss: 0.6727405190467834, training accuracy: 0.578125\n",
            "Epoch: 1674, training loss: 0.6560744643211365, training accuracy: 0.640625\n",
            "Epoch: 1675, training loss: 0.6546809673309326, training accuracy: 0.59375\n",
            "Epoch: 1676, training loss: 0.6188416481018066, training accuracy: 0.578125\n",
            "Epoch: 1677, training loss: 0.6452265381813049, training accuracy: 0.578125\n",
            "Epoch: 1678, training loss: 0.663532555103302, training accuracy: 0.65625\n",
            "Epoch: 1679, training loss: 0.609478235244751, training accuracy: 0.703125\n",
            "Epoch: 1680, training loss: 0.6091248989105225, training accuracy: 0.65625\n",
            "Epoch: 1681, training loss: 0.6329564452171326, training accuracy: 0.640625\n",
            "Epoch: 1682, training loss: 0.6320436000823975, training accuracy: 0.640625\n",
            "Epoch: 1683, training loss: 0.6509978175163269, training accuracy: 0.625\n",
            "Epoch: 1684, training loss: 0.5861225128173828, training accuracy: 0.734375\n",
            "Epoch: 1685, training loss: 0.6179114580154419, training accuracy: 0.71875\n",
            "Epoch: 1686, training loss: 0.6670138239860535, training accuracy: 0.59375\n",
            "Epoch: 1687, training loss: 0.5797750949859619, training accuracy: 0.734375\n",
            "Epoch: 1688, training loss: 0.6068172454833984, training accuracy: 0.71875\n",
            "Epoch: 1689, training loss: 0.6357741355895996, training accuracy: 0.65625\n",
            "Epoch: 1690, training loss: 0.593527615070343, training accuracy: 0.6875\n",
            "Epoch: 1691, training loss: 0.6199878454208374, training accuracy: 0.609375\n",
            "Epoch: 1692, training loss: 0.6048001050949097, training accuracy: 0.671875\n",
            "Epoch: 1693, training loss: 0.5950034260749817, training accuracy: 0.65625\n",
            "Epoch: 1694, training loss: 0.6790780425071716, training accuracy: 0.53125\n",
            "Epoch: 1695, training loss: 0.593553900718689, training accuracy: 0.703125\n",
            "Epoch: 1696, training loss: 0.6112958192825317, training accuracy: 0.59375\n",
            "Epoch: 1697, training loss: 0.6650223135948181, training accuracy: 0.65625\n",
            "Epoch: 1698, training loss: 0.6774721145629883, training accuracy: 0.625\n",
            "Epoch: 1699, training loss: 0.6759365797042847, training accuracy: 0.53125\n",
            "Epoch: 1700, training loss: 0.6398556232452393, training accuracy: 0.640625\n",
            "Epoch: 1701, training loss: 0.576288104057312, training accuracy: 0.765625\n",
            "Epoch: 1702, training loss: 0.6021213531494141, training accuracy: 0.765625\n",
            "Epoch: 1703, training loss: 0.5622459650039673, training accuracy: 0.78125\n",
            "Epoch: 1704, training loss: 0.5905474424362183, training accuracy: 0.71875\n",
            "Epoch: 1705, training loss: 0.6496323347091675, training accuracy: 0.625\n",
            "Epoch: 1706, training loss: 0.5834987759590149, training accuracy: 0.640625\n",
            "Epoch: 1707, training loss: 0.5820662975311279, training accuracy: 0.75\n",
            "Epoch: 1708, training loss: 0.6347113847732544, training accuracy: 0.625\n",
            "Epoch: 1709, training loss: 0.6097131967544556, training accuracy: 0.671875\n",
            "Epoch: 1710, training loss: 0.6428179144859314, training accuracy: 0.65625\n",
            "Epoch: 1711, training loss: 0.716702401638031, training accuracy: 0.578125\n",
            "Epoch: 1712, training loss: 0.5855902433395386, training accuracy: 0.6875\n",
            "Epoch: 1713, training loss: 0.6132651567459106, training accuracy: 0.734375\n",
            "Epoch: 1714, training loss: 0.6649563312530518, training accuracy: 0.65625\n",
            "Epoch: 1715, training loss: 0.575657308101654, training accuracy: 0.65625\n",
            "Epoch: 1716, training loss: 0.6442307233810425, training accuracy: 0.734375\n",
            "Epoch: 1717, training loss: 0.6629116535186768, training accuracy: 0.625\n",
            "Epoch: 1718, training loss: 0.6120713949203491, training accuracy: 0.75\n",
            "Epoch: 1719, training loss: 0.6424698829650879, training accuracy: 0.671875\n",
            "Epoch: 1720, training loss: 0.7106505632400513, training accuracy: 0.59375\n",
            "Epoch: 1721, training loss: 0.5914572477340698, training accuracy: 0.6875\n",
            "Epoch: 1722, training loss: 0.6720079183578491, training accuracy: 0.65625\n",
            "Epoch: 1723, training loss: 0.6683655381202698, training accuracy: 0.59375\n",
            "Epoch: 1724, training loss: 0.6982258558273315, training accuracy: 0.53125\n",
            "Epoch: 1725, training loss: 0.61123126745224, training accuracy: 0.703125\n",
            "Epoch: 1726, training loss: 0.5966891646385193, training accuracy: 0.671875\n",
            "Epoch: 1727, training loss: 0.6471582651138306, training accuracy: 0.65625\n",
            "Epoch: 1728, training loss: 0.6371515989303589, training accuracy: 0.640625\n",
            "Epoch: 1729, training loss: 0.6150750517845154, training accuracy: 0.609375\n",
            "Epoch: 1730, training loss: 0.6593493223190308, training accuracy: 0.578125\n",
            "Epoch: 1731, training loss: 0.6681265234947205, training accuracy: 0.578125\n",
            "Epoch: 1732, training loss: 0.6097290515899658, training accuracy: 0.65625\n",
            "Epoch: 1733, training loss: 0.6470583081245422, training accuracy: 0.625\n",
            "Epoch: 1734, training loss: 0.6283135414123535, training accuracy: 0.625\n",
            "Epoch: 1735, training loss: 0.590850830078125, training accuracy: 0.65625\n",
            "Epoch: 1736, training loss: 0.601893424987793, training accuracy: 0.703125\n",
            "Epoch: 1737, training loss: 0.5945039987564087, training accuracy: 0.59375\n",
            "Epoch: 1738, training loss: 0.6758212447166443, training accuracy: 0.5625\n",
            "Epoch: 1739, training loss: 0.6514520645141602, training accuracy: 0.5625\n",
            "Epoch: 1740, training loss: 0.6283987164497375, training accuracy: 0.640625\n",
            "Epoch: 1741, training loss: 0.6377204060554504, training accuracy: 0.5625\n",
            "Epoch: 1742, training loss: 0.6237165927886963, training accuracy: 0.6875\n",
            "Epoch: 1743, training loss: 0.6293913125991821, training accuracy: 0.65625\n",
            "Epoch: 1744, training loss: 0.6589866876602173, training accuracy: 0.59375\n",
            "Epoch: 1745, training loss: 0.6728259325027466, training accuracy: 0.578125\n",
            "Epoch: 1746, training loss: 0.6387656331062317, training accuracy: 0.625\n",
            "Epoch: 1747, training loss: 0.5952316522598267, training accuracy: 0.703125\n",
            "Epoch: 1748, training loss: 0.6186168193817139, training accuracy: 0.625\n",
            "Epoch: 1749, training loss: 0.6595470905303955, training accuracy: 0.65625\n",
            "Epoch: 1750, training loss: 0.5288586616516113, training accuracy: 0.765625\n",
            "Epoch: 1751, training loss: 0.6730006337165833, training accuracy: 0.578125\n",
            "Epoch: 1752, training loss: 0.6146242618560791, training accuracy: 0.625\n",
            "Epoch: 1753, training loss: 0.6107308864593506, training accuracy: 0.703125\n",
            "Epoch: 1754, training loss: 0.6015514135360718, training accuracy: 0.65625\n",
            "Epoch: 1755, training loss: 0.6042560935020447, training accuracy: 0.640625\n",
            "Epoch: 1756, training loss: 0.6230424642562866, training accuracy: 0.703125\n",
            "Epoch: 1757, training loss: 0.6541735529899597, training accuracy: 0.609375\n",
            "Epoch: 1758, training loss: 0.6875993609428406, training accuracy: 0.59375\n",
            "Epoch: 1759, training loss: 0.5293611288070679, training accuracy: 0.734375\n",
            "Epoch: 1760, training loss: 0.6267400979995728, training accuracy: 0.640625\n",
            "Epoch: 1761, training loss: 0.6250909566879272, training accuracy: 0.640625\n",
            "Epoch: 1762, training loss: 0.6995691061019897, training accuracy: 0.59375\n",
            "Epoch: 1763, training loss: 0.5978585481643677, training accuracy: 0.609375\n",
            "Epoch: 1764, training loss: 0.660871684551239, training accuracy: 0.609375\n",
            "Epoch: 1765, training loss: 0.6618061065673828, training accuracy: 0.5625\n",
            "Epoch: 1766, training loss: 0.6826655864715576, training accuracy: 0.578125\n",
            "Epoch: 1767, training loss: 0.6379735469818115, training accuracy: 0.578125\n",
            "Epoch: 1768, training loss: 0.6222337484359741, training accuracy: 0.625\n",
            "Epoch: 1769, training loss: 0.6180222630500793, training accuracy: 0.609375\n",
            "Epoch: 1770, training loss: 0.647362232208252, training accuracy: 0.640625\n",
            "Epoch: 1771, training loss: 0.6292566657066345, training accuracy: 0.6875\n",
            "Epoch: 1772, training loss: 0.6286482214927673, training accuracy: 0.65625\n",
            "Epoch: 1773, training loss: 0.6567037105560303, training accuracy: 0.609375\n",
            "Epoch: 1774, training loss: 0.6356929540634155, training accuracy: 0.65625\n",
            "Epoch: 1775, training loss: 0.584099292755127, training accuracy: 0.6875\n",
            "Epoch: 1776, training loss: 0.6313210725784302, training accuracy: 0.65625\n",
            "Epoch: 1777, training loss: 0.5796481370925903, training accuracy: 0.6875\n",
            "Epoch: 1778, training loss: 0.6143530011177063, training accuracy: 0.71875\n",
            "Epoch: 1779, training loss: 0.6297064423561096, training accuracy: 0.671875\n",
            "Epoch: 1780, training loss: 0.7372239828109741, training accuracy: 0.515625\n",
            "Epoch: 1781, training loss: 0.6385179758071899, training accuracy: 0.625\n",
            "Epoch: 1782, training loss: 0.6347885131835938, training accuracy: 0.5625\n",
            "Epoch: 1783, training loss: 0.6528118848800659, training accuracy: 0.640625\n",
            "Epoch: 1784, training loss: 0.634878933429718, training accuracy: 0.671875\n",
            "Epoch: 1785, training loss: 0.6562749147415161, training accuracy: 0.640625\n",
            "Epoch: 1786, training loss: 0.6131782531738281, training accuracy: 0.6875\n",
            "Epoch: 1787, training loss: 0.643526554107666, training accuracy: 0.6875\n",
            "Epoch: 1788, training loss: 0.6351221799850464, training accuracy: 0.625\n",
            "Epoch: 1789, training loss: 0.5313297510147095, training accuracy: 0.828125\n",
            "Epoch: 1790, training loss: 0.6418272852897644, training accuracy: 0.65625\n",
            "Epoch: 1791, training loss: 0.5780245661735535, training accuracy: 0.703125\n",
            "Epoch: 1792, training loss: 0.6481032371520996, training accuracy: 0.578125\n",
            "Epoch: 1793, training loss: 0.60394287109375, training accuracy: 0.6875\n",
            "Epoch: 1794, training loss: 0.6896342039108276, training accuracy: 0.59375\n",
            "Epoch: 1795, training loss: 0.6053912043571472, training accuracy: 0.703125\n",
            "Epoch: 1796, training loss: 0.6555893421173096, training accuracy: 0.625\n",
            "Epoch: 1797, training loss: 0.6117979884147644, training accuracy: 0.65625\n",
            "Epoch: 1798, training loss: 0.655643105506897, training accuracy: 0.578125\n",
            "Epoch: 1799, training loss: 0.5790712833404541, training accuracy: 0.6875\n",
            "Epoch: 1800, training loss: 0.6244801878929138, training accuracy: 0.59375\n",
            "Epoch: 1801, training loss: 0.6245267987251282, training accuracy: 0.609375\n",
            "Epoch: 1802, training loss: 0.7499834299087524, training accuracy: 0.546875\n",
            "Epoch: 1803, training loss: 0.6445889472961426, training accuracy: 0.640625\n",
            "Epoch: 1804, training loss: 0.5886354446411133, training accuracy: 0.671875\n",
            "Epoch: 1805, training loss: 0.6310809850692749, training accuracy: 0.625\n",
            "Epoch: 1806, training loss: 0.6156328916549683, training accuracy: 0.578125\n",
            "Epoch: 1807, training loss: 0.6507633924484253, training accuracy: 0.59375\n",
            "Epoch: 1808, training loss: 0.6675295233726501, training accuracy: 0.546875\n",
            "Epoch: 1809, training loss: 0.6448326110839844, training accuracy: 0.609375\n",
            "Epoch: 1810, training loss: 0.6141234636306763, training accuracy: 0.65625\n",
            "Epoch: 1811, training loss: 0.666906476020813, training accuracy: 0.515625\n",
            "Epoch: 1812, training loss: 0.6199520230293274, training accuracy: 0.65625\n",
            "Epoch: 1813, training loss: 0.6334449648857117, training accuracy: 0.671875\n",
            "Epoch: 1814, training loss: 0.6387011408805847, training accuracy: 0.59375\n",
            "Epoch: 1815, training loss: 0.6503787040710449, training accuracy: 0.578125\n",
            "Epoch: 1816, training loss: 0.590570867061615, training accuracy: 0.71875\n",
            "Epoch: 1817, training loss: 0.6063939332962036, training accuracy: 0.671875\n",
            "Epoch: 1818, training loss: 0.6213594675064087, training accuracy: 0.625\n",
            "Epoch: 1819, training loss: 0.6506400108337402, training accuracy: 0.5625\n",
            "Epoch: 1820, training loss: 0.6668189764022827, training accuracy: 0.578125\n",
            "Epoch: 1821, training loss: 0.6607959270477295, training accuracy: 0.59375\n",
            "Epoch: 1822, training loss: 0.5969561338424683, training accuracy: 0.6875\n",
            "Epoch: 1823, training loss: 0.5941756367683411, training accuracy: 0.71875\n",
            "Epoch: 1824, training loss: 0.6267931461334229, training accuracy: 0.578125\n",
            "Epoch: 1825, training loss: 0.6108872890472412, training accuracy: 0.703125\n",
            "Epoch: 1826, training loss: 0.6622915267944336, training accuracy: 0.71875\n",
            "Epoch: 1827, training loss: 0.538632333278656, training accuracy: 0.796875\n",
            "Epoch: 1828, training loss: 0.6076892018318176, training accuracy: 0.6875\n",
            "Epoch: 1829, training loss: 0.641752302646637, training accuracy: 0.703125\n",
            "Epoch: 1830, training loss: 0.5724363327026367, training accuracy: 0.734375\n",
            "Epoch: 1831, training loss: 0.5948503613471985, training accuracy: 0.78125\n",
            "Epoch: 1832, training loss: 0.612578272819519, training accuracy: 0.65625\n",
            "Epoch: 1833, training loss: 0.5851709842681885, training accuracy: 0.703125\n",
            "Epoch: 1834, training loss: 0.5565657019615173, training accuracy: 0.734375\n",
            "Epoch: 1835, training loss: 0.6672499775886536, training accuracy: 0.609375\n",
            "Epoch: 1836, training loss: 0.6834518313407898, training accuracy: 0.671875\n",
            "Epoch: 1837, training loss: 0.645293116569519, training accuracy: 0.609375\n",
            "Epoch: 1838, training loss: 0.6918034553527832, training accuracy: 0.546875\n",
            "Epoch: 1839, training loss: 0.559486985206604, training accuracy: 0.671875\n",
            "Epoch: 1840, training loss: 0.6517441272735596, training accuracy: 0.671875\n",
            "Epoch: 1841, training loss: 0.6075126528739929, training accuracy: 0.6875\n",
            "Epoch: 1842, training loss: 0.6168836355209351, training accuracy: 0.6875\n",
            "Epoch: 1843, training loss: 0.66890549659729, training accuracy: 0.609375\n",
            "Epoch: 1844, training loss: 0.6261407136917114, training accuracy: 0.65625\n",
            "Epoch: 1845, training loss: 0.5956871509552002, training accuracy: 0.671875\n",
            "Epoch: 1846, training loss: 0.6224939227104187, training accuracy: 0.609375\n",
            "Epoch: 1847, training loss: 0.5506792664527893, training accuracy: 0.765625\n",
            "Epoch: 1848, training loss: 0.6441439390182495, training accuracy: 0.71875\n",
            "Epoch: 1849, training loss: 0.6268332600593567, training accuracy: 0.640625\n",
            "Epoch: 1850, training loss: 0.5806070566177368, training accuracy: 0.6875\n",
            "Epoch: 1851, training loss: 0.5871212482452393, training accuracy: 0.625\n",
            "Epoch: 1852, training loss: 0.5976118445396423, training accuracy: 0.734375\n",
            "Epoch: 1853, training loss: 0.6318787932395935, training accuracy: 0.609375\n",
            "Epoch: 1854, training loss: 0.6279671788215637, training accuracy: 0.703125\n",
            "Epoch: 1855, training loss: 0.6490024328231812, training accuracy: 0.671875\n",
            "Epoch: 1856, training loss: 0.6290011405944824, training accuracy: 0.65625\n",
            "Epoch: 1857, training loss: 0.5750926733016968, training accuracy: 0.75\n",
            "Epoch: 1858, training loss: 0.5540453195571899, training accuracy: 0.734375\n",
            "Epoch: 1859, training loss: 0.6813956499099731, training accuracy: 0.578125\n",
            "Epoch: 1860, training loss: 0.6809075474739075, training accuracy: 0.5625\n",
            "Epoch: 1861, training loss: 0.688851535320282, training accuracy: 0.625\n",
            "Epoch: 1862, training loss: 0.633455216884613, training accuracy: 0.59375\n",
            "Epoch: 1863, training loss: 0.612125039100647, training accuracy: 0.625\n",
            "Epoch: 1864, training loss: 0.6618415117263794, training accuracy: 0.59375\n",
            "Epoch: 1865, training loss: 0.6036316156387329, training accuracy: 0.671875\n",
            "Epoch: 1866, training loss: 0.5531442165374756, training accuracy: 0.75\n",
            "Epoch: 1867, training loss: 0.631500780582428, training accuracy: 0.65625\n",
            "Epoch: 1868, training loss: 0.6814073324203491, training accuracy: 0.578125\n",
            "Epoch: 1869, training loss: 0.6108681559562683, training accuracy: 0.6875\n",
            "Epoch: 1870, training loss: 0.6822406053543091, training accuracy: 0.59375\n",
            "Epoch: 1871, training loss: 0.6704961061477661, training accuracy: 0.59375\n",
            "Epoch: 1872, training loss: 0.6248968839645386, training accuracy: 0.71875\n",
            "Epoch: 1873, training loss: 0.6653884053230286, training accuracy: 0.640625\n",
            "Epoch: 1874, training loss: 0.6329135894775391, training accuracy: 0.671875\n",
            "Epoch: 1875, training loss: 0.6517205834388733, training accuracy: 0.71875\n",
            "Epoch: 1876, training loss: 0.6274992823600769, training accuracy: 0.640625\n",
            "Epoch: 1877, training loss: 0.6595027446746826, training accuracy: 0.640625\n",
            "Epoch: 1878, training loss: 0.5762219429016113, training accuracy: 0.765625\n",
            "Epoch: 1879, training loss: 0.526455283164978, training accuracy: 0.75\n",
            "Epoch: 1880, training loss: 0.6608965992927551, training accuracy: 0.609375\n",
            "Epoch: 1881, training loss: 0.6480077505111694, training accuracy: 0.65625\n",
            "Epoch: 1882, training loss: 0.6345580816268921, training accuracy: 0.609375\n",
            "Epoch: 1883, training loss: 0.6811054944992065, training accuracy: 0.625\n",
            "Epoch: 1884, training loss: 0.6279934644699097, training accuracy: 0.65625\n",
            "Epoch: 1885, training loss: 0.5922962427139282, training accuracy: 0.734375\n",
            "Epoch: 1886, training loss: 0.6313025951385498, training accuracy: 0.625\n",
            "Epoch: 1887, training loss: 0.6473919153213501, training accuracy: 0.625\n",
            "Epoch: 1888, training loss: 0.6490699052810669, training accuracy: 0.625\n",
            "Epoch: 1889, training loss: 0.6625537872314453, training accuracy: 0.609375\n",
            "Epoch: 1890, training loss: 0.6297230124473572, training accuracy: 0.625\n",
            "Epoch: 1891, training loss: 0.6181927919387817, training accuracy: 0.609375\n",
            "Epoch: 1892, training loss: 0.6393416523933411, training accuracy: 0.59375\n",
            "Epoch: 1893, training loss: 0.6781624555587769, training accuracy: 0.640625\n",
            "Epoch: 1894, training loss: 0.6608079671859741, training accuracy: 0.5625\n",
            "Epoch: 1895, training loss: 0.6048924922943115, training accuracy: 0.6875\n",
            "Epoch: 1896, training loss: 0.6325525045394897, training accuracy: 0.625\n",
            "Epoch: 1897, training loss: 0.6270166039466858, training accuracy: 0.5625\n",
            "Epoch: 1898, training loss: 0.5773670673370361, training accuracy: 0.65625\n",
            "Epoch: 1899, training loss: 0.568537175655365, training accuracy: 0.78125\n",
            "Epoch: 1900, training loss: 0.6843712329864502, training accuracy: 0.5625\n",
            "Epoch: 1901, training loss: 0.5608404874801636, training accuracy: 0.734375\n",
            "Epoch: 1902, training loss: 0.6210813522338867, training accuracy: 0.671875\n",
            "Epoch: 1903, training loss: 0.5930867195129395, training accuracy: 0.65625\n",
            "Epoch: 1904, training loss: 0.6386615037918091, training accuracy: 0.59375\n",
            "Epoch: 1905, training loss: 0.5982484221458435, training accuracy: 0.609375\n",
            "Epoch: 1906, training loss: 0.6352214813232422, training accuracy: 0.625\n",
            "Epoch: 1907, training loss: 0.5986506938934326, training accuracy: 0.65625\n",
            "Epoch: 1908, training loss: 0.5974592566490173, training accuracy: 0.671875\n",
            "Epoch: 1909, training loss: 0.677699089050293, training accuracy: 0.5625\n",
            "Epoch: 1910, training loss: 0.5789899826049805, training accuracy: 0.703125\n",
            "Epoch: 1911, training loss: 0.6377643346786499, training accuracy: 0.625\n",
            "Epoch: 1912, training loss: 0.5657694339752197, training accuracy: 0.71875\n",
            "Epoch: 1913, training loss: 0.6511160135269165, training accuracy: 0.609375\n",
            "Epoch: 1914, training loss: 0.5369695425033569, training accuracy: 0.71875\n",
            "Epoch: 1915, training loss: 0.5950478315353394, training accuracy: 0.671875\n",
            "Epoch: 1916, training loss: 0.6062874794006348, training accuracy: 0.640625\n",
            "Epoch: 1917, training loss: 0.5752108097076416, training accuracy: 0.703125\n",
            "Epoch: 1918, training loss: 0.5817970037460327, training accuracy: 0.703125\n",
            "Epoch: 1919, training loss: 0.6272200345993042, training accuracy: 0.625\n",
            "Epoch: 1920, training loss: 0.5526807308197021, training accuracy: 0.734375\n",
            "Epoch: 1921, training loss: 0.5759649276733398, training accuracy: 0.671875\n",
            "Epoch: 1922, training loss: 0.6431792974472046, training accuracy: 0.671875\n",
            "Epoch: 1923, training loss: 0.637791633605957, training accuracy: 0.65625\n",
            "Epoch: 1924, training loss: 0.605048418045044, training accuracy: 0.671875\n",
            "Epoch: 1925, training loss: 0.5930754542350769, training accuracy: 0.71875\n",
            "Epoch: 1926, training loss: 0.6508451104164124, training accuracy: 0.59375\n",
            "Epoch: 1927, training loss: 0.764142632484436, training accuracy: 0.53125\n",
            "Epoch: 1928, training loss: 0.644797682762146, training accuracy: 0.6875\n",
            "Epoch: 1929, training loss: 0.6556172370910645, training accuracy: 0.625\n",
            "Epoch: 1930, training loss: 0.784332275390625, training accuracy: 0.53125\n",
            "Epoch: 1931, training loss: 0.6077677011489868, training accuracy: 0.609375\n",
            "Epoch: 1932, training loss: 0.638961672782898, training accuracy: 0.625\n",
            "Epoch: 1933, training loss: 0.6378556489944458, training accuracy: 0.65625\n",
            "Epoch: 1934, training loss: 0.6378173232078552, training accuracy: 0.640625\n",
            "Epoch: 1935, training loss: 0.5816640257835388, training accuracy: 0.703125\n",
            "Epoch: 1936, training loss: 0.6251084804534912, training accuracy: 0.6875\n",
            "Epoch: 1937, training loss: 0.6644290685653687, training accuracy: 0.640625\n",
            "Epoch: 1938, training loss: 0.6248152256011963, training accuracy: 0.578125\n",
            "Epoch: 1939, training loss: 0.6461102366447449, training accuracy: 0.640625\n",
            "Epoch: 1940, training loss: 0.6803661584854126, training accuracy: 0.546875\n",
            "Epoch: 1941, training loss: 0.6041222810745239, training accuracy: 0.625\n",
            "Epoch: 1942, training loss: 0.632672131061554, training accuracy: 0.625\n",
            "Epoch: 1943, training loss: 0.608477771282196, training accuracy: 0.703125\n",
            "Epoch: 1944, training loss: 0.608730673789978, training accuracy: 0.75\n",
            "Epoch: 1945, training loss: 0.6229244470596313, training accuracy: 0.71875\n",
            "Epoch: 1946, training loss: 0.6378481984138489, training accuracy: 0.65625\n",
            "Epoch: 1947, training loss: 0.6146056652069092, training accuracy: 0.65625\n",
            "Epoch: 1948, training loss: 0.718990683555603, training accuracy: 0.53125\n",
            "Epoch: 1949, training loss: 0.6293286085128784, training accuracy: 0.59375\n",
            "Epoch: 1950, training loss: 0.6258354187011719, training accuracy: 0.640625\n",
            "Epoch: 1951, training loss: 0.5697118639945984, training accuracy: 0.765625\n",
            "Epoch: 1952, training loss: 0.5873849987983704, training accuracy: 0.671875\n",
            "Epoch: 1953, training loss: 0.5799323320388794, training accuracy: 0.625\n",
            "Epoch: 1954, training loss: 0.5683789849281311, training accuracy: 0.65625\n",
            "Epoch: 1955, training loss: 0.6708992123603821, training accuracy: 0.65625\n",
            "Epoch: 1956, training loss: 0.6303088665008545, training accuracy: 0.625\n",
            "Epoch: 1957, training loss: 0.6664983034133911, training accuracy: 0.578125\n",
            "Epoch: 1958, training loss: 0.6679428815841675, training accuracy: 0.640625\n",
            "Epoch: 1959, training loss: 0.6174060702323914, training accuracy: 0.609375\n",
            "Epoch: 1960, training loss: 0.6634738445281982, training accuracy: 0.609375\n",
            "Epoch: 1961, training loss: 0.6180004477500916, training accuracy: 0.65625\n",
            "Epoch: 1962, training loss: 0.5847527384757996, training accuracy: 0.671875\n",
            "Epoch: 1963, training loss: 0.6950789093971252, training accuracy: 0.671875\n",
            "Epoch: 1964, training loss: 0.5669794082641602, training accuracy: 0.640625\n",
            "Epoch: 1965, training loss: 0.6468198299407959, training accuracy: 0.609375\n",
            "Epoch: 1966, training loss: 0.6281466484069824, training accuracy: 0.65625\n",
            "Epoch: 1967, training loss: 0.6417453289031982, training accuracy: 0.703125\n",
            "Epoch: 1968, training loss: 0.5905274748802185, training accuracy: 0.78125\n",
            "Epoch: 1969, training loss: 0.6184502840042114, training accuracy: 0.6875\n",
            "Epoch: 1970, training loss: 0.6189542412757874, training accuracy: 0.625\n",
            "Epoch: 1971, training loss: 0.6395877599716187, training accuracy: 0.625\n",
            "Epoch: 1972, training loss: 0.5957005023956299, training accuracy: 0.65625\n",
            "Epoch: 1973, training loss: 0.5629389882087708, training accuracy: 0.703125\n",
            "Epoch: 1974, training loss: 0.5759400129318237, training accuracy: 0.671875\n",
            "Epoch: 1975, training loss: 0.5715493559837341, training accuracy: 0.765625\n",
            "Epoch: 1976, training loss: 0.7848647236824036, training accuracy: 0.5\n",
            "Epoch: 1977, training loss: 0.5844590067863464, training accuracy: 0.609375\n",
            "Epoch: 1978, training loss: 0.5973988175392151, training accuracy: 0.703125\n",
            "Epoch: 1979, training loss: 0.598228931427002, training accuracy: 0.6875\n",
            "Epoch: 1980, training loss: 0.6147106885910034, training accuracy: 0.71875\n",
            "Epoch: 1981, training loss: 0.7151793837547302, training accuracy: 0.59375\n",
            "Epoch: 1982, training loss: 0.561199963092804, training accuracy: 0.65625\n",
            "Epoch: 1983, training loss: 0.6313649415969849, training accuracy: 0.65625\n",
            "Epoch: 1984, training loss: 0.6064167022705078, training accuracy: 0.6875\n",
            "Epoch: 1985, training loss: 0.6422720551490784, training accuracy: 0.71875\n",
            "Epoch: 1986, training loss: 0.5688615441322327, training accuracy: 0.671875\n",
            "Epoch: 1987, training loss: 0.5791048407554626, training accuracy: 0.671875\n",
            "Epoch: 1988, training loss: 0.6084210872650146, training accuracy: 0.640625\n",
            "Epoch: 1989, training loss: 0.7024879455566406, training accuracy: 0.578125\n",
            "Epoch: 1990, training loss: 0.6506307721138, training accuracy: 0.59375\n",
            "Epoch: 1991, training loss: 0.6587588787078857, training accuracy: 0.59375\n",
            "Epoch: 1992, training loss: 0.5883129835128784, training accuracy: 0.703125\n",
            "Epoch: 1993, training loss: 0.5772525072097778, training accuracy: 0.671875\n",
            "Epoch: 1994, training loss: 0.6592227220535278, training accuracy: 0.546875\n",
            "Epoch: 1995, training loss: 0.6145247220993042, training accuracy: 0.703125\n",
            "Epoch: 1996, training loss: 0.5615957379341125, training accuracy: 0.703125\n",
            "Epoch: 1997, training loss: 0.5618215203285217, training accuracy: 0.71875\n",
            "Epoch: 1998, training loss: 0.6849246621131897, training accuracy: 0.578125\n",
            "Epoch: 1999, training loss: 0.6040733456611633, training accuracy: 0.6875\n",
            "Epoch: 2000, training loss: 0.618904709815979, training accuracy: 0.640625\n",
            "\u001b[31m Validation loss: 0.6148104667663574, validation accuracy: 0.6434034416826003 \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "52a1bf7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "52a1bf7e",
        "outputId": "9fe5387d-93fe-4f1e-ba2f-71911dd7c523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7a51d67690>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Graphing losses\n",
        "plt.plot(np.array([x.cpu().detach().numpy() for x in train_losses]), 'r')\n",
        "plt.plot(np.array([x.cpu().detach().numpy() for x in val_losses]), 'b')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphing accuracies\n",
        "plt.plot(np.array([x for x in train_accuracies]), 'r')\n",
        "plt.plot(np.array([x for x in val_accuracies]), 'b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "gF8RzpowfGRo",
        "outputId": "8f7faccc-d46b-4d95-e567-bc74d3b5653d"
      },
      "id": "gF8RzpowfGRo",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7a550a6a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL PERFORMANCE - TRAINING PART\n",
        "plot_model_performance_single_horizon(model, train_x, train_y, num_samples=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "3jBqmTq_TUlT",
        "outputId": "5203115d-b180-4a94-818f-a219e1f9a8fe"
      },
      "id": "3jBqmTq_TUlT",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL'S PERFORMANCE - VALIDATION PART\n",
        "plot_model_performance_single_horizon(model, val_x, val_y, num_samples=len(val_x))"
      ],
      "metadata": {
        "id": "HwOpx_9ZhOH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "25fa04e8-517a-47fa-a04a-341f4dfa78cd"
      },
      "id": "HwOpx_9ZhOH3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy:\", classifier_accuracy(model, val_x, val_y, num_samples=len(val_x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CAIgzYSd-z0",
        "outputId": "3e7a331e-da57-451b-8f0a-743e0d012f73"
      },
      "id": "0CAIgzYSd-z0",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.6496175908221797\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "net-sasha.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}